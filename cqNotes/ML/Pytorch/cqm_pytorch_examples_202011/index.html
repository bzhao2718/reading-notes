<!DOCTYPE html><html class="hide-aside" lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>cqm_pytorch_examples_202011 | ReadingNotes</title><meta name="keywords" content="pytorch"><meta name="author"><meta name="copyright"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Pytorch Impl Examples [TOC] Common How To Word Embeddings This is the code shared in this part. Mostly shared variables. 1234567891011121314151617181920212223242526import torchfrom torch import nnimpo">
<meta property="og:type" content="article">
<meta property="og:title" content="cqm_pytorch_examples_202011">
<meta property="og:url" content="https://bzhao2718.github.io/reading-notes/cqNotes/ML/Pytorch/cqm_pytorch_examples_202011/index.html">
<meta property="og:site_name" content="ReadingNotes">
<meta property="og:description" content="Pytorch Impl Examples [TOC] Common How To Word Embeddings This is the code shared in this part. Mostly shared variables. 1234567891011121314151617181920212223242526import torchfrom torch import nnimpo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.pixabay.com/photo/2015/06/08/15/11/typewriter-801921_1280.jpg">
<meta property="article:published_time" content="2020-12-31T16:00:00.000Z">
<meta property="article:modified_time" content="2021-11-25T20:50:35.641Z">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.pixabay.com/photo/2015/06/08/15/11/typewriter-801921_1280.jpg"><link rel="shortcut icon" href="/reading-notes/img/favicon.png"><link rel="canonical" href="https://bzhao2718.github.io/reading-notes/cqNotes/ML/Pytorch/cqm_pytorch_examples_202011/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/reading-notes/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/reading-notes/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'cqm_pytorch_examples_202011',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-26 04:50:35'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.min.css" media="defer" onload="this.media='all'"><script async src="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/carousel-touch.min.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/archives/"><div class="headline">Articles</div><div class="length-num">90</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/tags/"><div class="headline">Tags</div><div class="length-num">20</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/reading-notes/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/reading-notes/">ReadingNotes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/reading-notes/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">cqm_pytorch_examples_202011</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2020-12-31T16:00:00.000Z" title="Created 2021-01-01 00:00:00">2021-01-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-11-25T20:50:35.641Z" title="Updated 2021-11-26 04:50:35">2021-11-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/reading-notes/categories/MLFramework/">MLFramework</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/reading-notes/categories/MLFramework/Pytorch/">Pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>63min</span></span></div></div></div><article class="post-content" id="article-container"><h1 id="pytorch-impl-examples">Pytorch Impl Examples</h1>
<p>[TOC]</p>
<h2 id="common-how-to">Common How To</h2>
<h4 id="word-embeddings">Word Embeddings</h4>
<p>This is the code shared in this part. Mostly shared variables.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">sen1 = <span class="string">"this is the first sentence"</span></span><br><span class="line">sen2 = <span class="string">"the second sentence"</span></span><br><span class="line">sen3 = <span class="string">"sentence three"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sorted_words</span>(<span class="params">sen_list</span>):</span></span><br><span class="line">    words = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sen_list:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sen.split():</span><br><span class="line">            words.add(word)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sorted</span>(words)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_special_chars</span>(<span class="params">token2idx, idx2token</span>):</span></span><br><span class="line">    UNK = <span class="string">"UNK"</span></span><br><span class="line">    BOS = <span class="string">"BOS"</span></span><br><span class="line">    EOS = <span class="string">"EOS"</span></span><br><span class="line">    token2idx[UNK] = <span class="number">0</span></span><br><span class="line">    token2idx[BOS] = <span class="number">1</span></span><br><span class="line">    token2idx[EOS] = <span class="number">2</span></span><br><span class="line">    idx2token[token2idx[UNK]] = UNK</span><br><span class="line">    idx2token[token2idx[BOS]] = BOS</span><br><span class="line">    idx2token[token2idx[EOS]] = EOS</span><br></pre></td></tr></tbody></table></figure>
<h5 id="one-hot-encodings">One Hot Encodings</h5>
<p>My one_hot example:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_one_hot_encoding</span>(<span class="params">sen, token2idx</span>):</span></span><br><span class="line">    vocab_size = <span class="built_in">len</span>(token2idx)</span><br><span class="line">    seq_len = <span class="built_in">len</span>(sen.split())</span><br><span class="line">    dtype = torch.long</span><br><span class="line">    sen_tensor = torch.zeros(seq_len, vocab_size, dtype=dtype)</span><br><span class="line">    <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sen.split()):</span><br><span class="line">        sen_tensor[i][token2idx[word]] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> sen_tensor</span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    num_special_chars = <span class="number">3</span></span><br><span class="line">    sen_list = [sen1, sen2, sen3]</span><br><span class="line">    vocab = get_sorted_words(sen_list)</span><br><span class="line">    token2idx = {word: i + num_special_chars <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    idx2token = {i + num_special_chars: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    add_special_chars(token2idx, idx2token)</span><br><span class="line">    <span class="built_in">print</span>(vocab)</span><br><span class="line">    <span class="built_in">print</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(idx2token)</span><br><span class="line">    <span class="comment"># get one hot encoding</span></span><br><span class="line">    vocab_size = <span class="built_in">len</span>(token2idx)</span><br><span class="line">    sen2_one_hot = get_one_hot_encoding(sen2, token2idx)</span><br><span class="line">    <span class="built_in">print</span>(sen2_one_hot)</span><br><span class="line"><span class="comment"># ['first', 'is', 'second', 'sentence', 'the', 'this', 'three']</span></span><br><span class="line"><span class="comment"># {'first': 3, 'is': 4, 'second': 5, 'sentence': 6, 'the': 7, 'this': 8, 'three': 9, 'UNK': 0, 'BOS': 1, 'EOS': 2}</span></span><br><span class="line"><span class="comment"># {3: 'first', 4: 'is', 5: 'second', 6: 'sentence', 7: 'the', 8: 'this', 9: 'three', 0: 'UNK', 1: 'BOS', 2: 'EOS'}</span></span><br><span class="line"><span class="comment"># tensor([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],</span></span><br><span class="line"><span class="comment">#         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="use-nn.embedding">Use nn.Embedding</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    num_special_chars = <span class="number">3</span></span><br><span class="line">    sen_list = [sen1, sen2, sen3]</span><br><span class="line">    vocab = get_sorted_words(sen_list)</span><br><span class="line">    token2idx = {word: i + num_special_chars <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    idx2token = {i + num_special_chars: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    add_special_chars(token2idx, idx2token)</span><br><span class="line">    <span class="built_in">print</span>(vocab)</span><br><span class="line">    <span class="built_in">print</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(idx2token)</span><br><span class="line">    <span class="comment"># get one hot encoding</span></span><br><span class="line">    vocab_size = <span class="built_in">len</span>(token2idx)</span><br><span class="line">    sen2_idxes = [token2idx[token] <span class="keyword">for</span> token <span class="keyword">in</span> sen2.split()]</span><br><span class="line">    <span class="comment"># sen2_idxes = torch.LongTensor(sen2_idxes) # sen2_idxes shape [3] will give an emb shape [3,3]</span></span><br><span class="line">    sen2_idxes = torch.tensor([sen2_idxes], dtype=torch.long) <span class="comment"># shape [1,3] will give a emb shape [1,3,3]</span></span><br><span class="line">    <span class="built_in">print</span>(sen2_idxes.shape)</span><br><span class="line">    emb_dim = <span class="number">4</span></span><br><span class="line">    embedding = nn.Embedding(vocab_size, emb_dim) <span class="comment"># need to use the vocab_size here, the input is a list of indices</span></span><br><span class="line">    sen2_emb = embedding(sen2_idxes)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"sen2_idxes: <span class="subst">{sen2_idxes}</span> with shape: <span class="subst">{sen2_idxes.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(sen2_emb)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"sen2_emb shape: <span class="subst">{sen2_emb.shape}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ['first', 'is', 'second', 'sentence', 'the', 'this', 'three']</span></span><br><span class="line"><span class="comment"># {'first': 3, 'is': 4, 'second': 5, 'sentence': 6, 'the': 7, 'this': 8, 'three': 9, 'UNK': 0, 'BOS': 1, 'EOS': 2}</span></span><br><span class="line"><span class="comment"># {3: 'first', 4: 'is', 5: 'second', 6: 'sentence', 7: 'the', 8: 'this', 9: 'three', 0: 'UNK', 1: 'BOS', 2: 'EOS'}</span></span><br><span class="line"><span class="comment"># torch.Size([1, 3])</span></span><br><span class="line"><span class="comment"># sen2_idxes: tensor([[7, 5, 6]]) with shape: torch.Size([1, 3])</span></span><br><span class="line"><span class="comment"># tensor([[[-0.0826,  0.5612,  0.0476, -1.0772],</span></span><br><span class="line"><span class="comment">#          [-1.9274, -0.4209, -1.1506,  0.0678],</span></span><br><span class="line"><span class="comment">#          [-0.3492,  1.5020,  0.6328, -1.3320]]], grad_fn=&lt;EmbeddingBackward&gt;)</span></span><br><span class="line"><span class="comment"># sen2_emb shape: torch.Size([1, 3, 4])</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="mask-inputs">mask inputs</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_one_seq</span>(<span class="params">idxes_list, seq_max_len</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    mask the sequences and make them have the same length.</span></span><br><span class="line"><span class="string">    pad 0s to the shorter ones. idxes_list is a batch of samples</span></span><br><span class="line"><span class="string">    :param idxes_list: a batch of samples</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    idxes_narr = np.zeros((<span class="built_in">len</span>(idxes_list), seq_max_len))</span><br><span class="line">    <span class="keyword">for</span> i, <span class="built_in">input</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxes_list):</span><br><span class="line">        <span class="comment"># for each sample (a list of idxes), set it to the narr, the rest positions are 0</span></span><br><span class="line">        idxes_narr[i][:<span class="built_in">len</span>(<span class="built_in">input</span>)] = <span class="built_in">input</span></span><br><span class="line">    masked_seq = torch.from_numpy(idxes_narr)</span><br><span class="line">    <span class="keyword">return</span> masked_seq</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sen_idxes</span>(<span class="params">sen_list</span>):</span></span><br><span class="line">    idxes_list = []</span><br><span class="line">    max_seq_len = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sen_list:</span><br><span class="line">        idxes = [token2idx[token] <span class="keyword">for</span> token <span class="keyword">in</span> sen.split()]</span><br><span class="line">        <span class="keyword">if</span> max_seq_len &lt; <span class="built_in">len</span>(idxes):</span><br><span class="line">            max_seq_len = <span class="built_in">len</span>(idxes)</span><br><span class="line">        idxes_list.append(idxes)</span><br><span class="line">    masked_seq = mask_one_seq(idxes_list, max_seq_len)</span><br><span class="line">    <span class="keyword">return</span> masked_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    num_special_chars = <span class="number">3</span></span><br><span class="line">    sen_list = [sen1, sen2, sen3]</span><br><span class="line">    vocab = get_sorted_words(sen_list)</span><br><span class="line">    token2idx = {word: i + num_special_chars <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    idx2token = {i + num_special_chars: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    add_special_chars(token2idx, idx2token)</span><br><span class="line">    vocab_size = <span class="built_in">len</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(vocab)</span><br><span class="line">    <span class="built_in">print</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(idx2token)</span><br><span class="line">    samples = get_sen_idxes(sen_list)</span><br><span class="line">    <span class="built_in">print</span>(samples)</span><br><span class="line">    <span class="built_in">print</span>(samples.shape)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># tensor([[8., 4., 7., 3., 6.],</span></span><br><span class="line"><span class="comment">#         [7., 5., 6., 0., 0.],</span></span><br><span class="line"><span class="comment">#         [6., 9., 0., 0., 0.]], dtype=torch.float64)</span></span><br><span class="line"><span class="comment"># torch.Size([3, 5])</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="my-word-embedding-example">My word embedding example</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mask_one_seq</span>(<span class="params">idxes_list, seq_max_len</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    mask the sequences and make them have the same length.</span></span><br><span class="line"><span class="string">    pad 0s to the shorter ones. idxes_list is a batch of samples</span></span><br><span class="line"><span class="string">    :param idxes_list: a batch of samples</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    idxes_narr = np.zeros((<span class="built_in">len</span>(idxes_list), seq_max_len),</span><br><span class="line">                          dtype=np.long)  <span class="comment"># the default is float64 (double), idx needs to be of type long</span></span><br><span class="line">    <span class="keyword">for</span> i, <span class="built_in">input</span> <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxes_list):</span><br><span class="line">        <span class="comment"># for each sample (a list of idxes), set it to the narr, the rest positions are 0</span></span><br><span class="line">        idxes_narr[i][:<span class="built_in">len</span>(<span class="built_in">input</span>)] = <span class="built_in">input</span></span><br><span class="line">    <span class="comment"># masked_seq = torch.from_numpy(idxes_narr) # the values are of float64 (double), which is the default in numpy</span></span><br><span class="line">    masked_seqs = torch.from_numpy(idxes_narr)</span><br><span class="line">    <span class="keyword">return</span> masked_seqs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_sen_idxes</span>(<span class="params">sen_list</span>):</span></span><br><span class="line">    idxes_list = []</span><br><span class="line">    max_seq_len = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sen <span class="keyword">in</span> sen_list:</span><br><span class="line">        idxes = [token2idx[token] <span class="keyword">for</span> token <span class="keyword">in</span> sen.split()]</span><br><span class="line">        <span class="keyword">if</span> max_seq_len &lt; <span class="built_in">len</span>(idxes):</span><br><span class="line">            max_seq_len = <span class="built_in">len</span>(idxes)</span><br><span class="line">        idxes_list.append(idxes)</span><br><span class="line">    masked_seq = mask_one_seq(idxes_list, max_seq_len)</span><br><span class="line">    <span class="keyword">return</span> masked_seq</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    num_special_chars = <span class="number">3</span></span><br><span class="line">    sen_list = [sen1, sen2, sen3]</span><br><span class="line">    vocab = get_sorted_words(sen_list)</span><br><span class="line">    token2idx = {word: i + num_special_chars <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    idx2token = {i + num_special_chars: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)}</span><br><span class="line">    add_special_chars(token2idx, idx2token)</span><br><span class="line">    vocab_size = <span class="built_in">len</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(vocab)</span><br><span class="line">    <span class="built_in">print</span>(token2idx)</span><br><span class="line">    <span class="built_in">print</span>(idx2token)</span><br><span class="line">    samples = get_sen_idxes(sen_list)</span><br><span class="line">    <span class="built_in">print</span>(samples)</span><br><span class="line">    <span class="built_in">print</span>(samples.shape)</span><br><span class="line">    emb_dim = <span class="number">4</span></span><br><span class="line"><span class="comment">#    embedding = nn.Embedding(vocab_size, emb_dim)  # need to use the vocab_size here, the input is a list of indices</span></span><br><span class="line">    <span class="comment"># padding_idx, if given, pads the output with the embedding vector at the 'padding_idx' (initialized to zeros) whenever it encounters the index</span></span><br><span class="line">    embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=<span class="number">0</span>)</span><br><span class="line">    sample_emb = embedding(samples)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"samples: <span class="subst">{samples}</span> with shape: <span class="subst">{samples.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(sample_emb)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"sample_emb shape: <span class="subst">{sample_emb.shape}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note this output is not using padding_idx</span></span><br><span class="line"><span class="comment"># tensor([[8, 4, 7, 3, 6],</span></span><br><span class="line"><span class="comment">#         [7, 5, 6, 0, 0],</span></span><br><span class="line"><span class="comment">#         [6, 9, 0, 0, 0]])</span></span><br><span class="line"><span class="comment"># torch.Size([3, 5])</span></span><br><span class="line"><span class="comment"># samples: tensor([[8, 4, 7, 3, 6],</span></span><br><span class="line"><span class="comment">#         [7, 5, 6, 0, 0],</span></span><br><span class="line"><span class="comment">#         [6, 9, 0, 0, 0]]) with shape: torch.Size([3, 5])</span></span><br><span class="line"><span class="comment"># tensor([[[-1.2289, -0.0674, -1.2169,  0.2970],</span></span><br><span class="line"><span class="comment">#          [-0.5405,  0.4446,  0.0940,  0.9595],</span></span><br><span class="line"><span class="comment">#          [-1.5411, -0.3691, -1.6953,  0.1054],</span></span><br><span class="line"><span class="comment">#          [ 0.9177,  0.7611,  1.2239,  1.2920],</span></span><br><span class="line"><span class="comment">#          [-0.2935,  0.6460, -0.7679, -0.5651]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         [[-1.5411, -0.3691, -1.6953,  0.1054],</span></span><br><span class="line"><span class="comment">#          [ 0.6593,  0.9810,  0.1757,  0.1514],</span></span><br><span class="line"><span class="comment">#          [-0.2935,  0.6460, -0.7679, -0.5651],</span></span><br><span class="line"><span class="comment">#          [-1.7936,  0.0552,  0.1979,  1.3070],</span></span><br><span class="line"><span class="comment">#          [-1.7936,  0.0552,  0.1979,  1.3070]],</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#         [[-0.2935,  0.6460, -0.7679, -0.5651],</span></span><br><span class="line"><span class="comment">#          [ 1.3144,  0.9150, -2.2445, -0.1513],</span></span><br><span class="line"><span class="comment">#          [-1.7936,  0.0552,  0.1979,  1.3070],</span></span><br><span class="line"><span class="comment">#          [-1.7936,  0.0552,  0.1979,  1.3070],</span></span><br><span class="line"><span class="comment">#          [-1.7936,  0.0552,  0.1979,  1.3070]]], grad_fn=&lt;EmbeddingBackward&gt;)</span></span><br><span class="line"><span class="comment"># sample_emb shape: torch.Size([3, 5, 4])</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Obviously, this is with padding_idx=0</span></span><br><span class="line"><span class="comment"># tensor([[[-1.1534, -0.0579, -1.5649,  0.2346],</span></span><br><span class="line"><span class="comment">#          [-1.1978, -0.0591,  0.8753, -0.8763],</span></span><br><span class="line"><span class="comment">#          [-0.1510,  1.2334, -0.6427,  0.6944],</span></span><br><span class="line"><span class="comment">#          [-0.4776, -2.0108,  0.2050, -1.6090],</span></span><br><span class="line"><span class="comment">#          [ 0.8465, -0.0134, -2.1425, -0.6159]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[-0.1510,  1.2334, -0.6427,  0.6944],</span></span><br><span class="line"><span class="comment">#          [ 1.5434,  0.4408, -0.2785,  1.9225],</span></span><br><span class="line"><span class="comment">#          [ 0.8465, -0.0134, -2.1425, -0.6159],</span></span><br><span class="line"><span class="comment">#          [ 0.0000,  0.0000,  0.0000,  0.0000],</span></span><br><span class="line"><span class="comment">#          [ 0.0000,  0.0000,  0.0000,  0.0000]],</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#         [[ 0.8465, -0.0134, -2.1425, -0.6159],</span></span><br><span class="line"><span class="comment">#          [-0.7302,  0.8952, -0.5466,  1.5134],</span></span><br><span class="line"><span class="comment">#          [ 0.0000,  0.0000,  0.0000,  0.0000],</span></span><br><span class="line"><span class="comment">#          [ 0.0000,  0.0000,  0.0000,  0.0000],</span></span><br><span class="line"><span class="comment">#          [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=&lt;EmbeddingBackward&gt;)</span></span><br><span class="line"><span class="comment"># sample_emb shape: torch.Size([3, 5, 4])</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="neural-network-modules">Neural Network Modules</h2>
<h5 id="curr_config-file">curr_config file</h5>
<h3 id="common-code">Common Code</h3>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># common config parameters of a specific model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Common attributes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">####Training hyper parameters</span></span><br><span class="line">lr = <span class="number">0.1</span></span><br><span class="line">clip = <span class="number">1</span></span><br><span class="line">teacher_forcing_ratio = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">enc_vocab_size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = <span class="built_in">float</span>(<span class="string">'inf'</span>)</span><br><span class="line">end_epoch = <span class="number">500</span></span><br><span class="line">start_epoch = <span class="number">0</span></span><br><span class="line">curr_epoch = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">src_vocab_size = <span class="number">0</span></span><br><span class="line">trg_vocab_size = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">load_chkpoint = <span class="literal">False</span></span><br><span class="line"><span class="comment"># Encoder specs</span></span><br><span class="line">enc_emb_dim = <span class="number">16</span></span><br><span class="line">enc_hid_dim = <span class="number">32</span></span><br><span class="line">enc_num_layers = <span class="number">2</span></span><br><span class="line">enc_dropout = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">enc_use_dropout = <span class="literal">True</span></span><br><span class="line"><span class="comment"># Decoder specs</span></span><br><span class="line">dec_emb_dim = <span class="number">16</span></span><br><span class="line">dec_hid_dim = <span class="number">32</span></span><br><span class="line">dec_num_layers = <span class="number">2</span></span><br><span class="line">dec_dropout = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line">dec_vocab_size = <span class="number">0</span></span><br><span class="line">dec_use_dropout = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">previous_breakpoint = <span class="literal">None</span></span><br><span class="line">chkpoint_num = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">abs_prj_path = <span class="string">"/Users/jackz/Documents/P Macbook/Laptop/PyCharm Workspace/data science/deep_learning/examples/my_pytorch_tutorials/my_simple_rnn/"</span></span><br><span class="line">best_model_path = abs_prj_path + <span class="string">"checkpoint/best_model.pt"</span></span><br><span class="line">entire_model_path = abs_prj_path + <span class="string">'checkpoint/model_checkpoint.pt'</span></span><br><span class="line"></span><br><span class="line">chkpoint_dir = <span class="string">"/content/drive/MyDrive/GoogleDrive/Model/TextSumBase/chkpoint/"</span></span><br><span class="line">entire_model_name = <span class="string">'model_checkpoint'</span></span><br><span class="line">best_model_name = <span class="string">"best_model"</span></span><br><span class="line">break_point = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">UNK = <span class="string">"&lt;UNK&gt;"</span></span><br><span class="line">BOS = <span class="string">"&lt;BOS&gt;"</span></span><br><span class="line">EOS = <span class="string">"&lt;EOS&gt;"</span></span><br><span class="line">PAD = <span class="string">"&lt;PAD&gt;"</span></span><br><span class="line">pad_idx = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">log_eval = <span class="literal">False</span></span><br><span class="line">save_every = <span class="number">5</span>  <span class="comment"># save model checkpoint</span></span><br><span class="line">eval_log_points = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">eval_log_epoches = [<span class="number">5</span>,<span class="number">100</span>,<span class="number">300</span>,<span class="number">499</span>]</span><br><span class="line">PAD_IDX=<span class="number">0</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="back-propagation">Back-propagation</h3>
<h4 id="simple-gradient-calculation">Simple gradient calculation</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad1</span>():</span></span><br><span class="line">    x = torch.tensor(<span class="number">3</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.tensor(<span class="number">4</span>, dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># y = wx+b --&gt; y = 3x+4</span></span><br><span class="line">    <span class="comment"># dy_dx = 3, dy_db = 1?</span></span><br><span class="line">    y = <span class="number">3</span> * x + b</span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="built_in">print</span>(x.grad.data)</span><br><span class="line">    <span class="built_in">print</span>(b.grad.data)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># tensor(13., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(3.)</span></span><br><span class="line"><span class="comment"># tensor(1.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#     y = 4 * x + 2 * b</span></span><br><span class="line"><span class="comment">#  This will print the following:</span></span><br><span class="line"><span class="comment"># tensor(20., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(4.)</span></span><br><span class="line"><span class="comment"># tensor(2.)</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad2</span>():</span></span><br><span class="line">    x = torch.tensor(<span class="number">3</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.tensor(<span class="number">4</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># y = x^2 + b</span></span><br><span class="line">    <span class="comment"># dy_dx = 2*x, dy_db = 1, if x = 3, then dy_dx=6</span></span><br><span class="line">    y = x ** <span class="number">2</span> + b</span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span>(y)</span><br><span class="line">    <span class="built_in">print</span>(x.grad.data)</span><br><span class="line">    <span class="built_in">print</span>(b.grad.data)</span><br><span class="line"><span class="comment"># tensor(13., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(6.)</span></span><br><span class="line"><span class="comment"># tensor(1.)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># if using y = x ** 3 + b</span></span><br><span class="line"><span class="comment"># calling y.backward() will yield:</span></span><br><span class="line"><span class="comment"># tensor(31., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(27.)</span></span><br><span class="line"><span class="comment"># tensor(1.)</span></span><br></pre></td></tr></tbody></table></figure>
<h4 id="chain-rule-examples">Chain rule examples</h4>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad3</span>():</span></span><br><span class="line">    x = torch.tensor(<span class="number">3</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.tensor(<span class="number">4</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = <span class="number">4</span> * x</span><br><span class="line">    z = <span class="number">3</span> * y + b</span><br><span class="line">    <span class="comment"># dz_dx = dz_dy * dy_dx = 3 * 4 = 12</span></span><br><span class="line">    z.backward()</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(x.grad.data)</span><br><span class="line">    <span class="built_in">print</span>(b.grad.data)</span><br><span class="line"><span class="comment"># tensor(40., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(12.)</span></span><br><span class="line"><span class="comment"># tensor(1.)</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad3</span>():</span></span><br><span class="line">    x = torch.tensor(<span class="number">3</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.tensor(<span class="number">4</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = <span class="number">4</span> * x + <span class="number">2</span> * b</span><br><span class="line">    z = <span class="number">3</span> * y + <span class="number">3</span> * b</span><br><span class="line">    <span class="comment"># dz_dx = dz_dy * dy_dx = 3 * 4 = 12</span></span><br><span class="line">    <span class="comment"># dz_db = dz_dy * dy_db + dz_db = 3 * 2 + 3 = 9</span></span><br><span class="line">    z.backward()</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(x.grad.data)</span><br><span class="line">    <span class="built_in">print</span>(b.grad.data)</span><br><span class="line"><span class="comment"># tensor(72., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(12.)</span></span><br><span class="line"><span class="comment"># tensor(9.)</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad4</span>():</span></span><br><span class="line">    x = torch.tensor(<span class="number">3</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    b = torch.tensor(<span class="number">4</span>, dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = x ** <span class="number">3</span> + <span class="number">2</span> * b</span><br><span class="line">    z = <span class="number">2</span> * y + <span class="number">3</span> * b</span><br><span class="line">    <span class="comment"># dz_dx = dz_dy * dy_dx = 2*(3*x^2)=2*(3*3^2)=54</span></span><br><span class="line">    <span class="comment"># dz_db = dz_dy * dy_db = dz_db = 2*2+3=7</span></span><br><span class="line">    z.backward()</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(x.grad.data)</span><br><span class="line">    <span class="built_in">print</span>(b.grad.data)</span><br><span class="line"><span class="comment"># tensor(82., grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># tensor(54.)</span></span><br><span class="line"><span class="comment"># tensor(7.)</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="nn.sequential-example">nn.Sequential Example</h3>
<p>Looks like using nn,Sequential affects the result a little it.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size, out_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.out_size = out_size</span><br><span class="line">        self.lin1 = nn.Linear(in_size, <span class="number">32</span>)</span><br><span class="line">        self.lin2 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin3 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin4 = nn.Linear(<span class="number">32</span>, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        net1 = self.lin1(x)</span><br><span class="line">        a1 = torch.sigmoid(net1)</span><br><span class="line">        net2 = self.lin2(a1)</span><br><span class="line">        a2 = torch.sigmoid(net2)</span><br><span class="line">        net3 = self.lin3(a2)</span><br><span class="line">        a3 = torch.sigmoid(net3)</span><br><span class="line">        y_hat = self.lin4(a3)</span><br><span class="line">        <span class="keyword">return</span> y_hat  <span class="comment"># output, y_hat</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model_use_sequential</span>(<span class="params">in_size, out_size</span>):</span></span><br><span class="line">    model = nn.Sequential(</span><br><span class="line">        nn.Linear(in_size, <span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">        nn.Sigmoid(),</span><br><span class="line">        nn.Linear(<span class="number">32</span>, out_size)</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss_mse</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    loss_mse = loss_func(y_hat, y)</span><br><span class="line">    <span class="keyword">return</span> loss_mse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_train</span>(<span class="params">model: my_model, opt, data, iterations, print_iter</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">            x, y = d</span><br><span class="line">            pred = model.forward(x)</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">            <span class="keyword">if</span> (epoch % print_iter == <span class="number">0</span>):</span><br><span class="line">                <span class="comment"># print(f"iteration {i}: pred {pred} | loss {loss}")</span></span><br><span class="line">                loss_hist.append(loss)</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span>(<span class="params">model: my_model, data</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data is a list containing tuples,each tuple contains x, y</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :param label:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        x, y = d</span><br><span class="line">        pred = model.forward(x)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"iteration <span class="subst">{i}</span>: pred <span class="subst">{pred}</span> | loss <span class="subst">{loss}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"pred: <span class="subst">{pred}</span> | y: <span class="subst">{y}</span>"</span>)</span><br><span class="line">        loss_hist.append(loss)</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_and</span>():</span></span><br><span class="line">    result = []</span><br><span class="line">    data = [(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        d1, d2, d3 = d</span><br><span class="line">        x = torch.tensor([d1, d2], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        y = torch.tensor([d3], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        result.append((x, y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lr = <span class="number">0.02</span></span><br><span class="line">    data = get_data_and()</span><br><span class="line">    <span class="comment"># data = get_data_and_3()</span></span><br><span class="line">    in_size = <span class="number">2</span></span><br><span class="line">    out_size = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="comment"># if i % 100 == 0:</span></span><br><span class="line">        <span class="built_in">print</span>(d[<span class="number">0</span>], d[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># model = my_model(in_size, out_size)</span></span><br><span class="line">    model = get_model_use_sequential(in_size, out_size)</span><br><span class="line">    opt = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    print_iter = <span class="number">5000</span></span><br><span class="line">    train_loss = run_train(model, opt, data, <span class="number">10000</span>, print_iter)</span><br><span class="line">    test_data = data</span><br><span class="line">    eval_loss = run_eval(model, test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"train loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> train_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"eval loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> eval_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is using the model class:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0: pred tensor([0.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([0.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 1: pred tensor([0.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([0.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 2: pred tensor([0.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([0.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 3: pred tensor([1.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([1.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># train loss:</span></span><br><span class="line"><span class="comment"># tensor(0.0196, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.2112, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0231, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(1.3219, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># THis is using the nn.Sequential, the hyper parameters are the same:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([0., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0., 1.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 1.], requires_grad=True) tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 0: pred tensor([3.1305e-05], grad_fn=&lt;AddBackward0&gt;) | loss 9.799745459559972e-10</span></span><br><span class="line"><span class="comment"># pred: tensor([3.1305e-05], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 1: pred tensor([1.2824e-05], grad_fn=&lt;AddBackward0&gt;) | loss 1.6444953732097645e-10</span></span><br><span class="line"><span class="comment"># pred: tensor([1.2824e-05], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 2: pred tensor([-2.8301e-05], grad_fn=&lt;AddBackward0&gt;) | loss 8.009219398807943e-10</span></span><br><span class="line"><span class="comment"># pred: tensor([-2.8301e-05], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 3: pred tensor([1.0000], grad_fn=&lt;AddBackward0&gt;) | loss 1.68839164871315e-10</span></span><br><span class="line"><span class="comment"># pred: tensor([1.0000], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># train loss:</span></span><br><span class="line"><span class="comment"># tensor(0.1236, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0652, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0900, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.7433, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(1.9015e-09, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(9.3750e-10, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(2.6674e-10, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(4.1554e-10, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># tensor(9.7997e-10)</span></span><br><span class="line"><span class="comment"># tensor(1.6445e-10)</span></span><br><span class="line"><span class="comment"># tensor(8.0092e-10)</span></span><br><span class="line"><span class="comment"># tensor(1.6884e-10)</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="fnn-examples">FNN Examples</h3>
<h5 id="feedforward-nn-manual-cal-and-nn.linear">Feedforward NN Manual Cal and nn.Linear</h5>
<p>The manual calculation and using nn.Linear module yield the same result.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">5</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5</span>, <span class="number">3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w1 = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.7</span>], [<span class="number">0.4</span>, <span class="number">0.1</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.1</span>], [<span class="number">0.6</span>, <span class="number">0.5</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.tensor([<span class="number">0.4</span>, <span class="number">0.3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">net</span>):</span></span><br><span class="line">    sig_out = torch.sigmoid(net)</span><br><span class="line">    <span class="keyword">return</span> sig_out</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fnn_print</span>():</span></span><br><span class="line">    w1t = w1.T</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"w1.T:\t"</span>, w1t)</span><br><span class="line">    net1 = torch.matmul(x, w1t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net1 (before bias):\t"</span>, net1)</span><br><span class="line">    net1 = net1 + b1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net1 (after bias):\t"</span>, net1)</span><br><span class="line">    a1 = sigmoid(net1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"a1.shape: "</span>, a1.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"a1:\t"</span>, a1)</span><br><span class="line">    w2t = w2.T</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"w2t:\t"</span>, w2t)</span><br><span class="line">    net2 = torch.matmul(a1, w2t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net2 (before bias):\t"</span>, net2)</span><br><span class="line">    net2 = net2 + b2</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"net2 (after bias):\t"</span>, net2)</span><br><span class="line">    y_hat = sigmoid(net2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"y_hat:\t"</span>, y_hat)</span><br><span class="line">    loss = (y - y_hat) ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Loss:\t"</span>, loss)</span><br><span class="line">    total_loss = torch.<span class="built_in">sum</span>(loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"total_loss:\t"</span>, total_loss)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fnn</span>():</span></span><br><span class="line">    w1t = w1.T</span><br><span class="line">    net1 = torch.matmul(x, w1t)</span><br><span class="line">    net1 = net1 + b1</span><br><span class="line">    a1 = sigmoid(net1)</span><br><span class="line">    w2t = w2.T</span><br><span class="line">    net2 = torch.matmul(a1, w2t)</span><br><span class="line">    net2 = net2 + b2</span><br><span class="line">    y_hat = sigmoid(net2)</span><br><span class="line">    loss = (y - y_hat) ** <span class="number">2</span></span><br><span class="line">    total_loss = torch.<span class="built_in">sum</span>(loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"total_loss:\t"</span>, total_loss)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fnn_print()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># w1.T:	 tensor([[0.2000, 0.4000],</span></span><br><span class="line"><span class="comment">#         [0.7000, 0.1000]], dtype=torch.float64, grad_fn=&lt;PermuteBackward&gt;)</span></span><br><span class="line"><span class="comment"># net1 (before bias):	 tensor([4.1000, 1.7000], dtype=torch.float64, grad_fn=&lt;SqueezeBackward3&gt;)</span></span><br><span class="line"><span class="comment"># net1 (after bias):	 tensor([4.2000, 1.9000], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># a1.shape:  torch.Size([2])</span></span><br><span class="line"><span class="comment"># a1:	 tensor([0.9852, 0.8699], dtype=torch.float64, grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># w2t:	 tensor([[0.3000, 0.6000],</span></span><br><span class="line"><span class="comment">#         [0.1000, 0.5000]], dtype=torch.float64, grad_fn=&lt;PermuteBackward&gt;)</span></span><br><span class="line"><span class="comment"># net2 (before bias):	 tensor([0.3826, 1.0261], dtype=torch.float64, grad_fn=&lt;SqueezeBackward3&gt;)</span></span><br><span class="line"><span class="comment"># net2 (after bias):	 tensor([0.7826, 1.3261], dtype=torch.float64, grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># y_hat:	 tensor([0.6862, 0.7902], dtype=torch.float64, grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># Loss:	 tensor([18.6086,  4.8833], dtype=torch.float64, grad_fn=&lt;PowBackward0&gt;)</span></span><br><span class="line"><span class="comment"># total_loss:	 tensor(23.4919, dtype=torch.float64, grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h5 id="nn.linear-concrete-examples">nn.Linear concrete examples</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># Looks like I should use dtype=torch.float32 since linear layer is expecting float not double</span></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">5</span>], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5</span>, <span class="number">3</span>], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w1 = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.7</span>], [<span class="number">0.4</span>, <span class="number">0.1</span>]], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.1</span>], [<span class="number">0.6</span>, <span class="number">0.5</span>]], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w3 = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">1</span>]], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.tensor([<span class="number">0.4</span>, <span class="number">0.3</span>], dtype=torch.float64, requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here I use my own weights</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lin</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, my_w, bias</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lin, self).__init__()</span><br><span class="line">        <span class="comment"># self.w = w</span></span><br><span class="line">        <span class="comment"># self.b = b</span></span><br><span class="line">        self.lin = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.lin.weight.copy_(my_w.<span class="built_in">float</span>())</span><br><span class="line">            self.lin.bias.copy_(bias.<span class="built_in">float</span>())</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">net</span>):</span></span><br><span class="line">    sig_out = torch.sigmoid(net)</span><br><span class="line">    <span class="keyword">return</span> sig_out</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmul</span>(<span class="params">w, a</span>):</span></span><br><span class="line">    dot_net = torch.matmul(w, a)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"w shape: <span class="subst">{w.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"a shape: <span class="subst">{a.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(w)</span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"dot_net shape: <span class="subst">{dot_net.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(dot_net)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin1.parameters():</span><br><span class="line">      <span class="built_in">print</span>(p)</span><br><span class="line">    net1 = lin1.forward(x.<span class="built_in">float</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 net1: "</span>)</span><br><span class="line">    <span class="built_in">print</span>(net1)</span><br><span class="line">    out1 = sigmoid(net1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"out1 after sigmoid: "</span>)</span><br><span class="line">    <span class="built_in">print</span>(out1)</span><br><span class="line">    lin2 = Lin(w2, b2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 parameters: "</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin2.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p)</span><br><span class="line">    out2 = lin2.forward(out1.<span class="built_in">float</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 out2: "</span>)</span><br><span class="line">    <span class="built_in">print</span>(out2)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">And I got the same result <span class="keyword">as</span> I manually calculated:</span><br><span class="line"><span class="comment"># lin1 parameters:</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.2000, 0.7000],</span></span><br><span class="line"><span class="comment">#         [0.4000, 0.1000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.1000, 0.2000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin1 net1:</span></span><br><span class="line"><span class="comment"># tensor([4.2000, 1.9000], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># out1 after sigmoid:</span></span><br><span class="line"><span class="comment"># tensor([0.9852, 0.8699], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># lin2 parameters:</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.3000, 0.1000],</span></span><br><span class="line"><span class="comment">#         [0.6000, 0.5000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.4000, 0.3000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin2 out2:</span></span><br><span class="line"><span class="comment"># tensor([0.7826, 1.3261], grad_fn=&lt;AddBackward0&gt;)</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="simple-fnn-implementation">Simple FNN Implementation</h5>
<p>This is an example that I calculated manually first, then write this code to check. The calculations match. See the <a href="x-devonthink-item://82041268-A9DF-4615-AB41-3FFE9962F1BC" rel="external nofollow noreferrer">manual calculation draft</a>.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">5</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5</span>, <span class="number">3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># y_hat_manual_cal = torch.tensor([0.6862309, 0.7901916], dtype=torch.float32, requires_grad=True)</span></span><br><span class="line"></span><br><span class="line">w1 = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.7</span>], [<span class="number">0.4</span>, <span class="number">0.1</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.1</span>], [<span class="number">0.6</span>, <span class="number">0.5</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b1 = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.tensor([<span class="number">0.4</span>, <span class="number">0.3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">net</span>):</span></span><br><span class="line">    sig_out = torch.sigmoid(net)</span><br><span class="line">    <span class="keyword">return</span> sig_out</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_grad1_print2</span>():</span></span><br><span class="line">    w1t = w1.T</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"w1.T:\t"</span>, w1t)</span><br><span class="line">    net1 = torch.matmul(x, w1t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net1 (before bias):\t"</span>, net1)</span><br><span class="line">    net1 = net1 + b1</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net1 (after bias):\t"</span>, net1)</span><br><span class="line">    a1 = sigmoid(net1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"a1:\t"</span>, a1)</span><br><span class="line">    w2t = w2.T</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"w2t:\t"</span>, w2t)</span><br><span class="line">    net2 = torch.matmul(a1, w2t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"net2 (before bias):\t"</span>, net2)</span><br><span class="line">    net2 = net2 + b2</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"net2 (after bias):\t"</span>, net2)</span><br><span class="line">    y_hat = sigmoid(net2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"y_hat:\t"</span>, y_hat)</span><br><span class="line">    loss = (y - y_hat) ** <span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Loss:\t"</span>, loss)</span><br><span class="line">    total_loss = torch.<span class="built_in">sum</span>(loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"total_loss:\t"</span>, total_loss)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    fnn_print()</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># w1.T:	 tensor([[0.2000, 0.4000],</span></span><br><span class="line"><span class="comment">#         [0.7000, 0.1000]], grad_fn=&lt;PermuteBackward&gt;)</span></span><br><span class="line"><span class="comment"># net1 (before bias):	 tensor([4.1000, 1.7000], grad_fn=&lt;SqueezeBackward3&gt;)</span></span><br><span class="line"><span class="comment"># net1 (after bias):	 tensor([4.2000, 1.9000], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># a1:	 tensor([0.9852, 0.8699], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># w2t:	 tensor([[0.3000, 0.6000],</span></span><br><span class="line"><span class="comment">#         [0.1000, 0.5000]], grad_fn=&lt;PermuteBackward&gt;)</span></span><br><span class="line"><span class="comment"># net2 (before bias):	 tensor([0.3826, 1.0261], grad_fn=&lt;SqueezeBackward3&gt;)</span></span><br><span class="line"><span class="comment"># net2 (after bias):	 tensor([0.7826, 1.3261], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># y_hat:	 tensor([0.6862, 0.7902], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># Loss:	 tensor([18.6086,  4.8833], grad_fn=&lt;PowBackward0&gt;)</span></span><br><span class="line"><span class="comment"># total_loss:	 tensor(23.4919, grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="section"></h3>
<h5 id="fnn-nn.linear-forward-backward-iteration-1">FNN nn.Linear forward backward iteration 1</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">5</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5</span>, <span class="number">3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w1 = torch.tensor([[<span class="number">0.2</span>, <span class="number">0.7</span>], [<span class="number">0.4</span>, <span class="number">0.1</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">w2 = torch.tensor([[<span class="number">0.3</span>, <span class="number">0.1</span>], [<span class="number">0.6</span>, <span class="number">0.5</span>]], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># w3 = torch.tensor([[1, 1], [1, 1]], dtype=torch.float32, requires_grad=True)</span></span><br><span class="line">b1 = torch.tensor([<span class="number">0.1</span>, <span class="number">0.2</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">b2 = torch.tensor([<span class="number">0.4</span>, <span class="number">0.3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">net</span>):</span></span><br><span class="line">    sig_out = torch.sigmoid(net)</span><br><span class="line">    <span class="keyword">return</span> sig_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lin</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, param, in_dim, out_dim, retain_grad=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lin, self).__init__()</span><br><span class="line">        my_weights, bias = param</span><br><span class="line">        <span class="keyword">if</span> retain_grad:</span><br><span class="line">            my_weights.retain_grad()</span><br><span class="line">            bias.retain_grad()</span><br><span class="line">        self.lin = nn.Linear(in_dim, out_dim)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.lin.weight.copy_(my_weights)</span><br><span class="line">            self.lin.bias.copy_(bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># print(self.lin.weight)</span></span><br><span class="line">        <span class="comment"># print(self.lin.bias)</span></span><br><span class="line">        out = self.lin(x)</span><br><span class="line">        <span class="comment"># print(out)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss_mse</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    loss_mse = loss_func(y_hat, y, reduction=<span class="string">'sum'</span>)</span><br><span class="line">    <span class="keyword">return</span> loss_mse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_grad</span>():</span></span><br><span class="line">    param1 = (w1, b1)</span><br><span class="line">    lin1 = Lin(param1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 parameters: "</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin1.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p)</span><br><span class="line">    net1 = lin1.forward(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 net1:\t<span class="subst">{net1}</span>"</span>)</span><br><span class="line">    out1 = sigmoid(net1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"out1 after sigmoid:\t<span class="subst">{out1}</span>"</span>)</span><br><span class="line">    param2 = (w2, b2)</span><br><span class="line">    lin2 = Lin(param2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 parameters: "</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin2.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p)</span><br><span class="line">    net2 = lin2.forward(out1.<span class="built_in">float</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 net2:\t<span class="subst">{net2}</span>"</span>)</span><br><span class="line">    y_hat = sigmoid(net2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 y_hat:\t<span class="subst">{y_hat}</span>"</span>)</span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    mse_loss = loss_func(y_hat, y, reduction=<span class="string">'sum'</span>)</span><br><span class="line">    <span class="comment"># none leaf nodes retain grad</span></span><br><span class="line">    x.retain_grad()</span><br><span class="line">    net1.retain_grad()</span><br><span class="line">    out1.retain_grad()</span><br><span class="line">    net2.retain_grad()</span><br><span class="line">    y_hat.retain_grad()</span><br><span class="line">    mse_loss.retain_grad()</span><br><span class="line">    <span class="comment"># calculate gradient</span></span><br><span class="line">    mse_loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"mse_loss.grad:\t<span class="subst">{mse_loss.grad}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"y_hat.grad:\t<span class="subst">{y_hat.grad}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"net2.grad:\t<span class="subst">{net2.grad}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 parameters grad:"</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin2.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p.grad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"out1.grad:\t<span class="subst">{out1.grad}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"net1.grad:\t<span class="subst">{net1.grad}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 parameters grad:"</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin1.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p.grad)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"x.grad:\t<span class="subst">{x.grad}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    cal_grad()</span><br><span class="line">    lr = <span class="number">1</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Backward pass iteration 1</span></span><br><span class="line">           </span><br><span class="line"><span class="comment"># lin1 parameters: </span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.2000, 0.7000],</span></span><br><span class="line"><span class="comment">#         [0.4000, 0.1000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.1000, 0.2000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin1 net1:	tensor([4.2000, 1.9000], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># out1 after sigmoid:	tensor([0.9852, 0.8699], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># lin2 parameters: </span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.3000, 0.1000],</span></span><br><span class="line"><span class="comment">#         [0.6000, 0.5000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.4000, 0.3000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin2 net2:	tensor([0.7826, 1.3261], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># lin2 y_hat:	tensor([0.6862, 0.7902], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># mse_loss.grad:	1.0</span></span><br><span class="line"><span class="comment"># y_hat.grad:	tensor([-8.6275, -4.4196])</span></span><br><span class="line"><span class="comment"># net2.grad:	tensor([-1.8577, -0.7327])</span></span><br><span class="line"><span class="comment"># lin2 parameters grad:</span></span><br><span class="line"><span class="comment"># tensor([[-1.8302, -1.6160],</span></span><br><span class="line"><span class="comment">#         [-0.7219, -0.6374]])</span></span><br><span class="line"><span class="comment"># tensor([-1.8577, -0.7327])</span></span><br><span class="line"><span class="comment"># out1.grad:	tensor([-0.9969, -0.5521])</span></span><br><span class="line"><span class="comment"># net1.grad:	tensor([-0.0145, -0.0625])</span></span><br><span class="line"><span class="comment"># lin1 parameters grad:</span></span><br><span class="line"><span class="comment"># tensor([[-0.0435, -0.0726],</span></span><br><span class="line"><span class="comment">#         [-0.1875, -0.3124]])</span></span><br><span class="line"><span class="comment"># tensor([-0.0145, -0.0625])</span></span><br><span class="line"><span class="comment"># x.grad:	tensor([-0.0279, -0.0164])</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward pass iteration 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.2000, 0.7000],</span></span><br><span class="line"><span class="comment">#         [0.4000, 0.1000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.1000, 0.2000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin1 net1:	tensor([4.2000, 1.9000], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># out1 after sigmoid:	tensor([0.9852, 0.8699], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># lin2 parameters:</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.3000, 0.1000],</span></span><br><span class="line"><span class="comment">#         [0.6000, 0.5000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.4000, 0.3000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin2 net2:	tensor([0.7826, 1.3261], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># lin2 y_hat:	tensor([0.6862, 0.7902], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h5 id="feedforward-pass-with-pytorch-nn.linear">Feedforward Pass with Pytorch nn.Linear</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lin</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, param, in_dim, out_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Lin, self).__init__()</span><br><span class="line">        my_weights, bias = param</span><br><span class="line">        self.lin = nn.Linear(in_dim, out_dim)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            self.lin.weight.copy_(my_weights)</span><br><span class="line">            self.lin.bias.copy_(bias)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="comment"># print(self.lin.weight)</span></span><br><span class="line">        <span class="comment"># print(self.lin.bias)</span></span><br><span class="line">        out = self.lin(x)</span><br><span class="line">        <span class="comment"># print(out)</span></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span>():</span></span><br><span class="line">    param1 = (w1, b1)</span><br><span class="line">    lin1 = Lin(param1, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 parameters: "</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin1.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p)</span><br><span class="line">    net1 = lin1.forward(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin1 net1:\t<span class="subst">{net1}</span>"</span>)</span><br><span class="line">    out1 = sigmoid(net1)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"out1 after sigmoid:\t<span class="subst">{out1}</span>"</span>)</span><br><span class="line">    param2 = (w2, b2)</span><br><span class="line">    lin2 = Lin(param2, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 parameters: "</span>)</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> lin2.parameters():</span><br><span class="line">        <span class="built_in">print</span>(p)</span><br><span class="line">    net2 = lin2.forward(out1.<span class="built_in">float</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 net2:\t<span class="subst">{net2}</span>"</span>)</span><br><span class="line">    y_hat = sigmoid(net2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"lin2 y_hat:\t<span class="subst">{y_hat}</span>"</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    feedforward()</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.2000, 0.7000],</span></span><br><span class="line"><span class="comment">#         [0.4000, 0.1000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.1000, 0.2000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin1 net1:	tensor([4.2000, 1.9000], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># out1 after sigmoid:	tensor([0.9852, 0.8699], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"><span class="comment"># lin2 parameters: </span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([[0.3000, 0.1000],</span></span><br><span class="line"><span class="comment">#         [0.6000, 0.5000]], requires_grad=True)</span></span><br><span class="line"><span class="comment"># Parameter containing:</span></span><br><span class="line"><span class="comment"># tensor([0.4000, 0.3000], requires_grad=True)</span></span><br><span class="line"><span class="comment"># lin2 net2:	tensor([0.7826, 1.3261], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="comment"># lin2 y_hat:	tensor([0.6862, 0.7902], grad_fn=&lt;SigmoidBackward&gt;)</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h5 id="fnn-nn.linear-example-switch-numbers">FNN nn.Linear Example Switch Numbers</h5>
<p>In this test, I have x = [3,5], and the target is y =[5,3], which is the number switched. Also, remember that the output should not go through a sigmoid function cause the value will be squeezed into [0,1].</p>
<p>Also, setting the lr to 1 seems to have some problems.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">3</span>, <span class="number">5</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = torch.tensor([<span class="number">5</span>, <span class="number">3</span>], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.lin1 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.lin2 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        self.lin3 = nn.Linear(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        net1 = self.lin1(x)</span><br><span class="line">        a1 = torch.sigmoid(net1)</span><br><span class="line">        net2 = self.lin2(a1)</span><br><span class="line">        a2 = torch.sigmoid(net2)</span><br><span class="line">        y_hat = self.lin3(a2)</span><br><span class="line">        <span class="keyword">return</span> y_hat  <span class="comment"># output, y_hat</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss_mse</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    loss_mse = loss_func(y_hat, y, reduction=<span class="string">'sum'</span>)</span><br><span class="line">    <span class="keyword">return</span> loss_mse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">x, iterations, lr=<span class="number">1</span></span>):</span></span><br><span class="line">    model = my_model()</span><br><span class="line">    opt = optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">    opt.zero_grad()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        y_hat = model.forward(x)</span><br><span class="line">        loss = cal_loss_mse(y_hat, y)</span><br><span class="line">        <span class="keyword">if</span> (i % <span class="number">20</span> == <span class="number">0</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"iteration <span class="subst">{i}</span>: y_hat <span class="subst">{y_hat}</span> | loss <span class="subst">{loss}</span>"</span>)</span><br><span class="line">        loss.backward()</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lr = <span class="number">0.003</span></span><br><span class="line">    train(x, <span class="number">200</span>, lr)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># output</span></span><br><span class="line"><span class="comment"># Note the lr = 0.003, later I set it to 1 and it has problems</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0: y_hat tensor([0.3094, 0.3305], grad_fn=&lt;AddBackward0&gt;) | loss 29.12749481201172</span></span><br><span class="line"><span class="comment"># iteration 20: y_hat tensor([1.0825, 0.7520], grad_fn=&lt;AddBackward0&gt;) | loss 20.400636672973633</span></span><br><span class="line"><span class="comment"># iteration 40: y_hat tensor([1.7476, 1.1162], grad_fn=&lt;AddBackward0&gt;) | loss 14.12682819366455</span></span><br><span class="line"><span class="comment"># iteration 60: y_hat tensor([2.3226, 1.4335], grad_fn=&lt;AddBackward0&gt;) | loss 9.622461318969727</span></span><br><span class="line"><span class="comment"># iteration 80: y_hat tensor([2.8166, 1.7090], grad_fn=&lt;AddBackward0&gt;) | loss 6.434144020080566</span></span><br><span class="line"><span class="comment"># iteration 100: y_hat tensor([3.2360, 1.9458], grad_fn=&lt;AddBackward0&gt;) | loss 4.223036289215088</span></span><br><span class="line"><span class="comment"># iteration 120: y_hat tensor([3.5873, 2.1469], grad_fn=&lt;AddBackward0&gt;) | loss 2.7235960960388184</span></span><br><span class="line"><span class="comment"># iteration 140: y_hat tensor([3.8775, 2.3152], grad_fn=&lt;AddBackward0&gt;) | loss 1.7288709878921509</span></span><br><span class="line"><span class="comment"># iteration 160: y_hat tensor([4.1143, 2.4543], grad_fn=&lt;AddBackward0&gt;) | loss 1.0822662115097046</span></span><br><span class="line"><span class="comment"># iteration 180: y_hat tensor([4.3051, 2.5679], grad_fn=&lt;AddBackward0&gt;) | loss 0.6695038080215454</span></span><br></pre></td></tr></tbody></table></figure>
<p>@#?hypothesis</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Here I set the lr = 1, and this is the output, the loss is increasing dramatically and then stabalizes around a big number. The hypothesis is that the gradients updating process gets stuck because the lr is too big and it ossilate back and forth and skip the optimal minimum point. Setting the lr rate smaller will allow it to not skip (low chance) the optimum point</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0: y_hat tensor([ 0.4176, -0.6845], grad_fn=&lt;AddBackward0&gt;) | loss 34.57456970214844</span></span><br><span class="line"><span class="comment"># iteration 20: y_hat tensor([-9.0331, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.2239990234375</span></span><br><span class="line"><span class="comment"># iteration 40: y_hat tensor([-9.0331, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.22344970703125</span></span><br><span class="line"><span class="comment"># iteration 60: y_hat tensor([-9.0330, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.222900390625</span></span><br><span class="line"><span class="comment"># iteration 80: y_hat tensor([-9.0330, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.2223815917969</span></span><br><span class="line"><span class="comment"># iteration 100: y_hat tensor([-9.0330, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.22186279296875</span></span><br><span class="line"><span class="comment"># iteration 120: y_hat tensor([-9.0330, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.2213134765625</span></span><br><span class="line"><span class="comment"># iteration 140: y_hat tensor([-9.0330, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.22076416015625</span></span><br><span class="line"><span class="comment"># iteration 160: y_hat tensor([-9.0329, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.22021484375</span></span><br><span class="line"><span class="comment"># iteration 180: y_hat tensor([-9.0329, -7.5970], grad_fn=&lt;AddBackward0&gt;) | loss 309.2196960449219</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is to verify the point, setting lr=1.1 makes it much worse:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0: y_hat tensor([0.2971, 0.1851], grad_fn=&lt;AddBackward0&gt;) | loss 30.040245056152344</span></span><br><span class="line"><span class="comment"># iteration 20: y_hat tensor([-835.5213, -456.3098], grad_fn=&lt;AddBackward0&gt;) | loss 917441.5625</span></span><br><span class="line"><span class="comment"># iteration 40: y_hat tensor([-32218.6055, -17605.8516], grad_fn=&lt;AddBackward0&gt;) | loss 1348432384.0</span></span><br><span class="line"><span class="comment"># iteration 60: y_hat tensor([-1235371.5000,  -675078.4375], grad_fn=&lt;AddBackward0&gt;) | loss 1981890035712.0</span></span><br><span class="line"><span class="comment"># iteration 80: y_hat tensor([-47361400., -25881018.], grad_fn=&lt;AddBackward0&gt;) | loss 2912929766703104.0</span></span><br><span class="line"><span class="comment"># iteration 100: y_hat tensor([-1.8157e+09, -9.9222e+08], grad_fn=&lt;AddBackward0&gt;) | loss 4.281350669123715e+18</span></span><br><span class="line"><span class="comment"># iteration 120: y_hat tensor([-6.9611e+10, -3.8039e+10], grad_fn=&lt;AddBackward0&gt;) | loss 6.292618550546407e+21</span></span><br><span class="line"><span class="comment"># iteration 140: y_hat tensor([-2.6687e+12, -1.4583e+12], grad_fn=&lt;AddBackward0&gt;) | loss 9.24872708658409e+24</span></span><br><span class="line"><span class="comment"># iteration 160: y_hat tensor([-1.0231e+14, -5.5909e+13], grad_fn=&lt;AddBackward0&gt;) | loss 1.3593534982699037e+28</span></span><br><span class="line"><span class="comment"># iteration 180: y_hat tensor([-3.9224e+15, -2.1434e+15], grad_fn=&lt;AddBackward0&gt;) | loss 1.9979423779036574e+31</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="fnn-and-operation-example">FNN And Operation Example</h5>
<p>There are four inputs that specifies the AND on two binary digits. Looks like it needs a lot of iterations to train.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size, out_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.out_size = out_size</span><br><span class="line">        self.lin1 = nn.Linear(in_size, <span class="number">32</span>)</span><br><span class="line">        self.lin2 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin3 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin4 = nn.Linear(<span class="number">32</span>, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        net1 = self.lin1(x)</span><br><span class="line">        a1 = torch.sigmoid(net1)</span><br><span class="line">        net2 = self.lin2(a1)</span><br><span class="line">        a2 = torch.sigmoid(net2)</span><br><span class="line">        net3 = self.lin3(a2)</span><br><span class="line">        a3 = torch.sigmoid(net3)</span><br><span class="line">        y_hat = self.lin4(a3)</span><br><span class="line">        <span class="keyword">return</span> y_hat  <span class="comment"># output, y_hat</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss_mse</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    loss_mse = loss_func(y_hat, y)</span><br><span class="line">    <span class="keyword">return</span> loss_mse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_train</span>(<span class="params">model: my_model, opt, data, iterations, print_iter</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">            x, y = d</span><br><span class="line">            pred = model.forward(x)</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">            <span class="keyword">if</span> (epoch % print_iter == <span class="number">0</span>):</span><br><span class="line">                <span class="comment"># print(f"iteration {i}: pred {pred} | loss {loss}")</span></span><br><span class="line">                loss_hist.append(loss)</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span>(<span class="params">model: my_model, data</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data is a list containing tuples,each tuple contains x, y</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :param label:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        x, y = d</span><br><span class="line">        pred = model.forward(x)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"iteration <span class="subst">{i}</span>: pred <span class="subst">{pred}</span> | loss <span class="subst">{loss}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"pred: <span class="subst">{pred}</span> | y: <span class="subst">{y}</span>"</span>)</span><br><span class="line">        loss_hist.append(loss)</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_and</span>():</span></span><br><span class="line">    result = []</span><br><span class="line">    data = [(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        d1, d2, d3 = d</span><br><span class="line">        x = torch.tensor([d1, d2], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        y = torch.tensor([d3], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        result.append((x, y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_and_3</span>():</span></span><br><span class="line">    result = []</span><br><span class="line">    data = [(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">            (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        d1, d2, d3, d4 = d</span><br><span class="line">        x = torch.tensor([d1, d2, d3], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        y = torch.tensor([d4], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        result.append((x, y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lr = <span class="number">0.02</span></span><br><span class="line">    data = get_data_and()</span><br><span class="line">    in_size = <span class="number">2</span></span><br><span class="line">    out_size = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="comment"># if i % 100 == 0:</span></span><br><span class="line">        <span class="built_in">print</span>(d[<span class="number">0</span>], d[<span class="number">1</span>])</span><br><span class="line">    model = my_model(in_size, out_size)</span><br><span class="line">    opt = optim.SGD(model.parameters(), lr=lr)</span><br><span class="line">    print_iter = <span class="number">5000</span></span><br><span class="line">    train_loss = run_train(model, opt, data, <span class="number">10000</span>, print_iter)</span><br><span class="line">    test_data = data</span><br><span class="line">    eval_loss = run_eval(model, test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"train loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> train_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"eval loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> eval_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is the for hyper parameters: 3 hidden layers, lr = 0.02, optim.SGD, 10,000 epoches</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment"># tensor([0., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0., 1.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 1.], requires_grad=True) tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 0: pred tensor([-0.0404], grad_fn=&lt;AddBackward0&gt;) | loss 0.0016357980202883482</span></span><br><span class="line"><span class="comment"># pred: tensor([-0.0404], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 1: pred tensor([0.0164], grad_fn=&lt;AddBackward0&gt;) | loss 0.0002697624731808901</span></span><br><span class="line"><span class="comment"># pred: tensor([0.0164], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 2: pred tensor([0.0200], grad_fn=&lt;AddBackward0&gt;) | loss 0.0003989639808423817</span></span><br><span class="line"><span class="comment"># pred: tensor([0.0200], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 3: pred tensor([0.9964], grad_fn=&lt;AddBackward0&gt;) | loss 1.3094251698930748e-05</span></span><br><span class="line"><span class="comment"># pred: tensor([0.9964], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># train loss: </span></span><br><span class="line"><span class="comment"># tensor(0.0179, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0074, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0032, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.9284, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0334, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0154, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0146, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0040, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># eval loss: </span></span><br><span class="line"><span class="comment"># tensor(0.0016)</span></span><br><span class="line"><span class="comment"># tensor(0.0003)</span></span><br><span class="line"><span class="comment"># tensor(0.0004)</span></span><br><span class="line"><span class="comment"># tensor(1.3094e-05)</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The same setting as above, except with a different optimizer algo: opt = optim.Adam(model.parameters(), lr=lr), and the result looks much better.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([0., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0., 1.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 1.], requires_grad=True) tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 0: pred tensor([3.6798e-08], grad_fn=&lt;AddBackward0&gt;) | loss 1.3541042448095221e-15</span></span><br><span class="line"><span class="comment"># pred: tensor([3.6798e-08], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 1: pred tensor([1.1969e-09], grad_fn=&lt;AddBackward0&gt;) | loss 1.4325577069336363e-18</span></span><br><span class="line"><span class="comment"># pred: tensor([1.1969e-09], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 2: pred tensor([-2.9093e-08], grad_fn=&lt;AddBackward0&gt;) | loss 8.46397778422978e-16</span></span><br><span class="line"><span class="comment"># pred: tensor([-2.9093e-08], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 3: pred tensor([1.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([1.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># train loss: </span></span><br><span class="line"><span class="comment"># tensor(0.1502, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0342, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.1014, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.6249, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(8.4454e-10, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(5.2293e-10, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(4.9757e-11, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(5.4538e-09, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># eval loss: </span></span><br><span class="line"><span class="comment"># tensor(1.3541e-15)</span></span><br><span class="line"><span class="comment"># tensor(1.4326e-18)</span></span><br><span class="line"><span class="comment"># tensor(8.4640e-16)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br></pre></td></tr></tbody></table></figure>
<h5 id="fnn-xor-example">FNN XOR Example</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size, out_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.out_size = out_size</span><br><span class="line">        self.lin1 = nn.Linear(in_size, <span class="number">32</span>)</span><br><span class="line">        self.lin2 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin3 = nn.Linear(<span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">        self.lin4 = nn.Linear(<span class="number">32</span>, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        net1 = self.lin1(x)</span><br><span class="line">        a1 = torch.sigmoid(net1)</span><br><span class="line">        net2 = self.lin2(a1)</span><br><span class="line">        a2 = torch.sigmoid(net2)</span><br><span class="line">        net3 = self.lin3(a2)</span><br><span class="line">        a3 = torch.sigmoid(net3)</span><br><span class="line">        y_hat = self.lin4(a3)</span><br><span class="line">        <span class="keyword">return</span> y_hat  <span class="comment"># output, y_hat</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss_mse</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    loss_func = F.mse_loss</span><br><span class="line">    loss_mse = loss_func(y_hat, y)</span><br><span class="line">    <span class="keyword">return</span> loss_mse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_train</span>(<span class="params">model: my_model, opt, data, iterations, print_iter</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(iterations):</span><br><span class="line">        <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">            x, y = d</span><br><span class="line">            pred = model.forward(x)</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">            <span class="keyword">if</span> (epoch % print_iter == <span class="number">0</span>):</span><br><span class="line">                <span class="comment"># print(f"iteration {i}: pred {pred} | loss {loss}")</span></span><br><span class="line">                loss_hist.append(loss)</span><br><span class="line">            loss.backward()</span><br><span class="line">            opt.step()</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span>(<span class="params">model: my_model, data</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data is a list containing tuples,each tuple contains x, y</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :param label:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        x, y = d</span><br><span class="line">        pred = model.forward(x)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            loss = cal_loss_mse(pred, y)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"iteration <span class="subst">{i}</span>: pred <span class="subst">{pred}</span> | loss <span class="subst">{loss}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"pred: <span class="subst">{pred}</span> | y: <span class="subst">{y}</span>"</span>)</span><br><span class="line">        loss_hist.append(loss)</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_or</span>():</span></span><br><span class="line">    result = []</span><br><span class="line">    data = [(<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>), (<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>)]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">        d1, d2, d3 = d</span><br><span class="line">        x = torch.tensor([d1, d2], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        y = torch.tensor([d3], dtype=torch.float32, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        result.append((x, y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    lr = <span class="number">0.02</span></span><br><span class="line">    data = get_data_or()</span><br><span class="line">    in_size = <span class="number">2</span></span><br><span class="line">    out_size = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="comment"># if i % 100 == 0:</span></span><br><span class="line">        <span class="built_in">print</span>(d[<span class="number">0</span>], d[<span class="number">1</span>])</span><br><span class="line">    model = my_model(in_size, out_size)</span><br><span class="line">    <span class="comment"># opt = optim.SGD(model.parameters(), lr=lr)</span></span><br><span class="line">    opt = optim.Adam(model.parameters(), lr=lr)</span><br><span class="line">    print_iter = <span class="number">5000</span></span><br><span class="line">    train_loss = run_train(model, opt, data, <span class="number">10000</span>, print_iter)</span><br><span class="line">    test_data = data</span><br><span class="line">    eval_loss = run_eval(model, test_data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"train loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> train_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"eval loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> eval_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Looks like the result is pretty good:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor([0., 0.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([0., 1.], requires_grad=True) tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 0.], requires_grad=True) tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># tensor([1., 1.], requires_grad=True) tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 0: pred tensor([0.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([0.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 1: pred tensor([1.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([1.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 2: pred tensor([1.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([1.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 3: pred tensor([0.], grad_fn=&lt;AddBackward0&gt;) | loss 0.0</span></span><br><span class="line"><span class="comment"># pred: tensor([0.], grad_fn=&lt;AddBackward0&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># train loss: </span></span><br><span class="line"><span class="comment"># tensor(0.1051, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.4800, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0.0086, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(1.9586, grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># tensor(0., grad_fn=&lt;MseLossBackward&gt;)</span></span><br><span class="line"><span class="comment"># eval loss: </span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br><span class="line"><span class="comment"># tensor(0.)</span></span><br></pre></td></tr></tbody></table></figure>
<h3 id="rnn-examples">RNN Examples</h3>
<h4 id="lstm">LSTM</h4>
<h5 id="xor-lstm-example">XOR LSTM Example</h5>
<p>A few things to note:</p>
<ol type="1">
<li>Don’t use softmax or log_softmax on the output when there is only one element in the output like in this case.</li>
<li>Looks like increasing lr substantially increased the model performance even with smaller hidden size.</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_size, h_size, out_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.in_size = in_size</span><br><span class="line">        self.h_size = h_size</span><br><span class="line">        self.out_size = out_size</span><br><span class="line">        self.lstm = nn.LSTM(in_size, hidden_size=h_size)</span><br><span class="line">        self.fc = nn.Linear(h_size, out_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> x.dim() &lt; <span class="number">3</span>:</span><br><span class="line">            x = x.reshape(-<span class="number">1</span>, <span class="number">1</span>, self.in_size)</span><br><span class="line">        <span class="comment"># if h[0].dim() &lt; 3 or h[1].dim() &lt; 3:</span></span><br><span class="line">        <span class="comment">#     h = (h[0].reshape(-1, 1, self.h_size), h[1].reshape(-1, 1, self.h_size))</span></span><br><span class="line">        out, h = self.lstm(x)</span><br><span class="line">        out = self.fc(out.squeeze(dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># out = F.log_softmax(out, dim=1)</span></span><br><span class="line">        <span class="keyword">return</span> out, h</span><br><span class="line"></span><br><span class="line">loss_func = nn.functional.mse_loss</span><br><span class="line"><span class="comment">## !!! Don't use NNLoss or softmax on the output, because we only have</span></span><br><span class="line"><span class="comment"># dim 1 output, doing softmax will always give us 1, then log_softmax will</span></span><br><span class="line"><span class="comment"># always give 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss_func = nn.NLLLoss()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_train</span>(<span class="params">model: my_model, opt, data, epoches, print_point</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    epoch_loss = <span class="built_in">dict</span>()</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epoches):</span><br><span class="line">        curr_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">            <span class="comment"># x [0,1] y [1]</span></span><br><span class="line">            x, y = d</span><br><span class="line">            <span class="keyword">with</span> torch.autograd.set_detect_anomaly(<span class="literal">True</span>):</span><br><span class="line">                opt.zero_grad()</span><br><span class="line">                out, hidden = model.forward(x)</span><br><span class="line">                loss = loss_func(out.squeeze(dim=<span class="number">0</span>), y)</span><br><span class="line">                <span class="comment"># if (epoch % print_point == 0):</span></span><br><span class="line">                <span class="comment">#     # print(f"iteration {i}: pred {pred} | loss {loss}")</span></span><br><span class="line">                <span class="comment">#     loss_hist.append(loss)</span></span><br><span class="line">                loss.backward()</span><br><span class="line">                opt.step()</span><br><span class="line">            curr_loss += loss.item()</span><br><span class="line"></span><br><span class="line">        epoch_loss[epoch] = curr_loss</span><br><span class="line">    <span class="keyword">return</span> loss_hist, epoch_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_eval</span>(<span class="params">model: my_model, data</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data is a list containing tuples,each tuple contains x, y</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :param label:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    loss_hist = []</span><br><span class="line">    curr_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        x, y = d</span><br><span class="line">        pred, hidden = model.forward(x)</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            loss = loss_func(pred.squeeze(dim=<span class="number">0</span>), y)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"iteration <span class="subst">{i}</span>: pred <span class="subst">{pred}</span> | loss <span class="subst">{loss}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"pred: <span class="subst">{pred}</span> | y: <span class="subst">{y}</span>"</span>)</span><br><span class="line">        loss_hist.append(loss)</span><br><span class="line">        curr_loss += loss.item()</span><br><span class="line">    <span class="keyword">return</span> loss_hist, curr_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># def get_data_xor():</span></span><br><span class="line"><span class="comment">#     result = []</span></span><br><span class="line"><span class="comment">#     data = [(0, 0, 0), (0, 1, 1), (1, 0, 1), (1, 1, 0)]</span></span><br><span class="line"><span class="comment">#     for d in data:</span></span><br><span class="line"><span class="comment">#         d1, d2, d3 = d</span></span><br><span class="line"><span class="comment">#         x = torch.tensor([d1, d2], dtype=torch.float32, requires_grad=True)</span></span><br><span class="line"><span class="comment">#         y = torch.tensor([d3], dtype=torch.float32, requires_grad=True)</span></span><br><span class="line"><span class="comment">#         result.append((x, y))</span></span><br><span class="line"><span class="comment">#     return result</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tuple2str</span>(<span class="params">x</span>):</span></span><br><span class="line">    result = <span class="string">''</span>.join(<span class="built_in">str</span>(element) <span class="keyword">for</span> element <span class="keyword">in</span> x)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data_xor</span>(<span class="params">x_size=<span class="number">2</span>, dtype=torch.float32</span>):</span></span><br><span class="line">    lst = <span class="built_in">list</span>(itertools.product([<span class="number">0</span>, <span class="number">1</span>], repeat=x_size))</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> lst:</span><br><span class="line">        x = <span class="built_in">list</span>(b)  <span class="comment"># exclusive, not include the last one</span></span><br><span class="line">        y = chk_xor(tuple2str(b)) <span class="comment"># get the last element</span></span><br><span class="line">        x = torch.tensor(x, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        y = torch.tensor(y, dtype=dtype, requires_grad=<span class="literal">True</span>)</span><br><span class="line">        result.append((x, y))</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chk_xor</span>(<span class="params">x</span>):</span></span><br><span class="line">    idx0 = x.find(<span class="string">"0"</span>)</span><br><span class="line">    idx1 = x.find(<span class="string">"1"</span>)</span><br><span class="line">    result = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> (idx0 != -<span class="number">1</span> <span class="keyword">and</span> idx1 != -<span class="number">1</span>):</span><br><span class="line">        result = <span class="number">1</span></span><br><span class="line">    <span class="comment"># if contains 1 and 0, return 1, otherwise (all 1s or all 0s), return 0</span></span><br><span class="line">    <span class="keyword">return</span> [result]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    in_size=<span class="number">4</span></span><br><span class="line">    lr = <span class="number">0.1</span></span><br><span class="line">    out_size = <span class="number">1</span></span><br><span class="line">    h_size = <span class="number">16</span></span><br><span class="line">    data = get_data_xor(x_size=in_size)</span><br><span class="line">    <span class="comment"># print(type(data))</span></span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="comment"># if i % 100 == 0:</span></span><br><span class="line">        <span class="built_in">print</span>(d[<span class="number">0</span>], d[<span class="number">1</span>])</span><br><span class="line">    model = my_model(in_size, h_size, out_size)</span><br><span class="line">    <span class="comment"># opt = optim.SGD(model.parameters(), lr=lr)</span></span><br><span class="line">    opt = optim.Adagrad(model.parameters(), lr=lr)</span><br><span class="line">    print_iter = <span class="number">5000</span></span><br><span class="line">    train_loss = run_train(model, opt, data, <span class="number">5000</span>, print_iter)</span><br><span class="line">    test_data = data</span><br><span class="line">    eval_loss = run_eval(model, test_data)</span><br><span class="line">    <span class="comment"># print("train loss: ")</span></span><br><span class="line">    <span class="comment"># for l in train_loss:</span></span><br><span class="line">    <span class="comment">#     print(l)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"eval loss: "</span>)</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> eval_loss:</span><br><span class="line">        <span class="built_in">print</span>(l)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training using 5000 epochs with this hyper para:</span></span><br><span class="line"><span class="comment">#     in_size=4</span></span><br><span class="line"><span class="comment">#     lr = 0.02</span></span><br><span class="line"><span class="comment">#     out_size = 1</span></span><br><span class="line"><span class="comment">#     h_size = 16</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(0.0006), tensor(0.0024), tensor(0.0026), tensor(0.0097), tensor(0.0025), tensor(0.0089), tensor(0.0083), tensor(0.0237), tensor(0.0021), tensor(0.0079), tensor(0.0062), tensor(0.0198), tensor(0.0089), tensor(0.0227), tensor(0.0196), tensor(0.0501)]</span></span><br><span class="line"><span class="comment"># 0.1960659505566582</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Using the same setting except 5000 epochs:</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(0.0003), tensor(0.0025), tensor(0.0029), tensor(0.0082), tensor(0.0032), tensor(0.0075), tensor(0.0068), tensor(0.0142), tensor(0.0020), tensor(0.0094), tensor(0.0057), tensor(0.0161), tensor(0.0074), tensor(0.0179), tensor(0.0125), tensor(0.0296)]</span></span><br><span class="line"><span class="comment"># 0.14621099151554517</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now 5000 epochs with h_size 64:</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(0.0009), tensor(0.0041), tensor(0.0038), tensor(0.0102), tensor(0.0050), tensor(0.0109), tensor(0.0113), tensor(0.0231), tensor(0.0039), tensor(0.0103), tensor(0.0096), tensor(0.0216), tensor(0.0112), tensor(0.0233), tensor(0.0235), tensor(0.0456)]</span></span><br><span class="line"><span class="comment"># 0.21836828853702173</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now 5000 epochs with h_size 16 but lr=0.1</span></span><br><span class="line"><span class="comment"># iteration 14: pred tensor([[1.0000]], grad_fn=&lt;AddmmBackward&gt;) | loss 1.4210854715202004e-14</span></span><br><span class="line"><span class="comment"># pred: tensor([[1.0000]], grad_fn=&lt;AddmmBackward&gt;) | y: tensor([1.], requires_grad=True)</span></span><br><span class="line"><span class="comment"># iteration 15: pred tensor([[1.6391e-07]], grad_fn=&lt;AddmmBackward&gt;) | loss 2.6867397195928788e-14</span></span><br><span class="line"><span class="comment"># pred: tensor([[1.6391e-07]], grad_fn=&lt;AddmmBackward&gt;) | y: tensor([0.], requires_grad=True)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(3.5527e-15), tensor(1.4211e-14), tensor(3.1974e-14), tensor(5.6843e-14), tensor(3.1974e-14), tensor(1.4211e-14), tensor(0.), tensor(5.6843e-14), tensor(0.), tensor(1.4211e-14), tensor(1.4211e-14), tensor(3.5527e-15), tensor(5.6843e-14), tensor(3.1974e-14), tensor(1.4211e-14), tensor(2.6867e-14)]</span></span><br><span class="line"><span class="comment"># 3.714806240395774e-13</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># **************</span></span><br><span class="line"><span class="comment"># Looks like the issue is the lr set to low for the first few tries, after</span></span><br><span class="line"><span class="comment"># setting it to 0.1, which is 50 times more than the first few cases,</span></span><br><span class="line"><span class="comment"># the loss drops substantially.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h4 id="seq2seq-encoder-decoder">Seq2Seq, Encoder Decoder</h4>
<h5 id="lstm-seq2seq-reverse-sentences">LSTM Seq2Seq Reverse Sentences</h5>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> randint</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"><span class="keyword">from</span> examples.my_nn.my_const <span class="keyword">import</span> Token</span><br><span class="line"><span class="keyword">from</span> examples.my_nn.my_config <span class="keyword">import</span> config</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> examples.my_nn.my_data.SentenceData <span class="keyword">import</span> SentenceData</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, out_size, args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.emb_size = args.enc_emb_dim</span><br><span class="line">        self.h_size = args.enc_hid_dim</span><br><span class="line">        self.out_size = out_size</span><br><span class="line">        self.args = args  <span class="comment"># can remove others since using args</span></span><br><span class="line">        self.dropout = nn.Dropout(args.enc_dropout)</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, args.enc_emb_dim)</span><br><span class="line">        self.lstm = nn.LSTM(self.emb_size, hidden_size=self.h_size, num_layers=args.enc_num_layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        expected_dim = <span class="number">3</span>  <span class="comment"># originally: x shape 5, e.g. [8,4,7,3,6], after adding BOS and EOS, shape is 7; now x shape [6,1] [src_seq_len, batch_size]</span></span><br><span class="line">        embeds = self.embedding(</span><br><span class="line">            x)  <span class="comment"># x shape [seq_len, batch_size] (6,1), embeds shape [seq_len, batch_size,emb_dim) (6,1,16)</span></span><br><span class="line">        <span class="comment"># if x shape is correct [seq_len, batch_size], embeds shape [seq_len, batch_size, emb_dim], no need to change dim</span></span><br><span class="line">        <span class="keyword">if</span> embeds.dim() &lt; expected_dim:</span><br><span class="line">            <span class="keyword">if</span> embeds.dim() == <span class="number">1</span>:</span><br><span class="line">                embeds = embeds.reshape(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># now it should be [1,1,5] for one token with batch size 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                embeds = embeds.unsqueeze(dim=<span class="number">0</span>)  <span class="comment"># e.g. [7,16] -&gt; [7,1,16], [seq_len, batch_size,hid_dim]</span></span><br><span class="line">        <span class="keyword">if</span> self.args.enc_use_dropout:</span><br><span class="line">            embeds = self.dropout(embeds)</span><br><span class="line">        out, h = self.lstm(embeds)</span><br><span class="line">        <span class="comment"># out = self.fc(out)</span></span><br><span class="line">        <span class="keyword">return</span> out, h  <span class="comment"># out shape (now [7,1,32] [seq_len,batch_size,hidden_size]; h tuple 2 elements, each with shape [2,1,32] 2 because it's 2 layers</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, trg_vocab_size, args</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment"># trg_vocab_size should be the same as trg_vocab_size</span></span><br><span class="line">        <span class="comment"># self.trg_vocab_size = trg_vocab_size</span></span><br><span class="line">        self.args = args</span><br><span class="line">        self.out_size = trg_vocab_size</span><br><span class="line">        self.dropout = nn.Dropout(args.dec_dropout)</span><br><span class="line">        self.embedding = nn.Embedding(trg_vocab_size, args.dec_emb_dim)  <span class="comment"># the first param is the input_dim</span></span><br><span class="line">        self.lstm = nn.LSTM(args.dec_emb_dim, hidden_size=args.dec_hid_dim,</span><br><span class="line">                            num_layers=args.dec_num_layers)</span><br><span class="line">        self.fc = nn.Linear(args.dec_hid_dim, trg_vocab_size)  <span class="comment"># trg_vocab_size is the output vocab size</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, h</span>):</span></span><br><span class="line">        <span class="comment"># input [batch_size]</span></span><br><span class="line">        expected_dim = <span class="number">3</span></span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        embeds = self.embedding(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># embeds shape 5; e.g. [1,16] with input shape [1]; with input shape [1,1] (tensor([[7]]), embeds shape [1,1,16]</span></span><br><span class="line">        <span class="keyword">if</span> embeds.dim() &lt; expected_dim:</span><br><span class="line">            <span class="keyword">if</span> embeds.dim() == <span class="number">1</span>:</span><br><span class="line">                embeds = embeds.reshape(<span class="number">1</span>, <span class="number">1</span>, -<span class="number">1</span>)  <span class="comment"># now it should be [1,1,5] for one token with batch size 1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                embeds = embeds.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.args.dec_use_dropout:</span><br><span class="line">            embeds = self.dropout(embeds)</span><br><span class="line">        out, h = self.lstm(embeds, h)</span><br><span class="line">        out = self.fc(out)  <span class="comment"># outs shape [1,1,10]</span></span><br><span class="line">        <span class="keyword">return</span> out, h</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_model</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, src_vocab_size, args: config, trg_vocab_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(my_model, self).__init__()</span><br><span class="line">        self.vocab_size = src_vocab_size</span><br><span class="line">        self.args = args</span><br><span class="line">        self.trg_vocab_size = trg_vocab_size</span><br><span class="line">        <span class="comment"># add num of layers seems to make is worse, not doing it right</span></span><br><span class="line">        self.encoder = Encoder(src_vocab_size, trg_vocab_size, args=args)</span><br><span class="line">        <span class="comment"># src vocab size might be different than trg vocab size</span></span><br><span class="line">        <span class="comment"># dec_input_dim = trg_vocab_size</span></span><br><span class="line">        <span class="comment"># dec_out_dim = trg_vocab_size</span></span><br><span class="line">        self.decoder = Decoder(trg_vocab_size, args)</span><br><span class="line">        <span class="comment"># self.log_softmax_layer = nn.LogSoftmax(dim=1)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="comment"># x [src_len, batch_size]</span></span><br><span class="line">        <span class="comment"># y [trg_len, batch_size]</span></span><br><span class="line">        <span class="keyword">if</span> x.dim() == <span class="number">1</span>:  <span class="comment"># handling the test case where x and y is a list of index</span></span><br><span class="line">            x = x.unsqueeze(dim=<span class="number">1</span>)  <span class="comment"># e.g.[6,1]</span></span><br><span class="line">        <span class="keyword">if</span> y.dim() == <span class="number">1</span>:</span><br><span class="line">            y = y.unsqueeze(dim=<span class="number">1</span>)  <span class="comment"># if a list of index</span></span><br><span class="line">        out, h = self.encoder(x)  <span class="comment"># out shape [trg seq len, batch size, trg_vocab_size] e.g. [1,1,11]</span></span><br><span class="line">        trg_seq_len = y.shape[<span class="number">0</span>]</span><br><span class="line">        batch_size = out.shape[<span class="number">1</span>]</span><br><span class="line">        outputs = torch.zeros(trg_seq_len, batch_size, self.trg_vocab_size)</span><br><span class="line">        <span class="comment"># y is [1,..,2], so y[0] is the BOS symbol</span></span><br><span class="line">        decoder_x = y[<span class="number">0</span>, :]  <span class="comment"># the BOS symbol</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, trg_seq_len):</span><br><span class="line">            <span class="comment"># decoder output already go through a fc so the output size</span></span><br><span class="line">            out, h = self.decoder(decoder_x, h)</span><br><span class="line">            outputs[t] = out</span><br><span class="line">            <span class="comment"># add teacher forcing and beam search here</span></span><br><span class="line">            use_teacher_forcing = random.random() &lt; self.args.teacher_forcing_ratio</span><br><span class="line">            <span class="comment"># argmax(1) gives a [1,10] contains all 0s;use argmax() gives the index with max value; signature argmax(input, dim,...)</span></span><br><span class="line">            <span class="comment"># if top1 == train_batch.token2idx["EOS"]:  # stop either a "EOS" is the output or go through the trg seq len</span></span><br><span class="line">            <span class="comment">#     break</span></span><br><span class="line">            <span class="comment"># out.argmax(): tensor(7); out.argmax(dim=-1) result shape [1,1],tensor([[7]]), which makes embeds shape [1,1,1,16], not correct</span></span><br><span class="line">            <span class="comment"># seq len 1; out.squeeze(dim=0).argmax(dim=-1) size 1, tensor([7]), which makes embeds out shape [1,1,16]</span></span><br><span class="line">            top1 = out.squeeze(dim=<span class="number">0</span>).argmax(dim=-<span class="number">1</span>)</span><br><span class="line">            decoder_x = top1</span><br><span class="line">            <span class="keyword">if</span> use_teacher_forcing:</span><br><span class="line">                decoder_x = y[t]</span><br><span class="line">        <span class="keyword">return</span> outputs, h  <span class="comment"># outputs [7,1,10] for the first sen1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_loss_func</span>(<span class="params">name=<span class="string">"CrossEntropy"</span>, ignore_index=<span class="literal">False</span>, pad_index=<span class="number">0</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> ignore_index:</span><br><span class="line">        loss_func = nn.CrossEntropyLoss(ignore_index=pad_index)</span><br><span class="line">        <span class="keyword">return</span> loss_func</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loss_func = nn.CrossEntropyLoss()</span><br><span class="line">        <span class="keyword">return</span> loss_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PAD = <span class="string">"PAD"</span></span><br><span class="line"><span class="comment"># sen_data = SentenceData()</span></span><br><span class="line"><span class="comment"># loss_func = nn.functional.mse_loss</span></span><br><span class="line"><span class="comment"># loss_func = nn.NLLLoss()</span></span><br><span class="line">loss_func = build_loss_func(ignore_index=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_loss</span>(<span class="params">pred, label, trim_first_token=<span class="literal">False</span></span>):</span></span><br><span class="line">    criterion = loss_func</span><br><span class="line">    <span class="comment"># output = pred[1:].squeeze(dim=1)  # ignore the first token, (first token is 0 for output, and BOS for y)</span></span><br><span class="line">    <span class="keyword">if</span> trim_first_token:</span><br><span class="line">        pred = pred[<span class="number">1</span>:].squeeze(dim=<span class="number">1</span>)</span><br><span class="line">        label = label[<span class="number">1</span>:]</span><br><span class="line">    <span class="comment"># Label: (N,C) where C = number of classes, Target (N) where target value is between 0 and C-1 inclusive</span></span><br><span class="line">    loss = criterion(pred, label)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model</span>(<span class="params">model: my_model, optimizer: optim, args: config, loss_hist: defaultdict(<span class="params"><span class="built_in">list</span></span>), best_valid_loss: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">               epoch: <span class="built_in">int</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">               save_entire_model=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    save the model for later tranining or inference</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param optimizer:</span></span><br><span class="line"><span class="string">    :param train_hist:</span></span><br><span class="line"><span class="string">    :param valid_hist:</span></span><br><span class="line"><span class="string">    :param best_valid_loss:</span></span><br><span class="line"><span class="string">    :param epoch:</span></span><br><span class="line"><span class="string">    :param save_entire_model:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    abs_prj_path = args.abs_prj_path</span><br><span class="line">    best_model_path = args.best_model_path</span><br><span class="line">    entire_model_path = args.entire_model_path</span><br><span class="line">    <span class="keyword">if</span> save_entire_model:</span><br><span class="line">        torch.save({</span><br><span class="line">            Token.START_EPOCH: epoch,</span><br><span class="line">            Token.MODEL_STATE_DICT: model.state_dict(),</span><br><span class="line">            Token.OPTIM_STATE_DICT: optimizer.state_dict(),</span><br><span class="line">            Token.LOSS_HIST: loss_hist,</span><br><span class="line">            Token.BEST_VALID_LOSS: best_valid_loss</span><br><span class="line">        }, entire_model_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        torch.save(model.state_dict(), best_model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_optimizer</span>(<span class="params">model, lr=<span class="number">0.1</span>, optim_state_dict=<span class="literal">None</span></span>):</span></span><br><span class="line">    optimizer = optim.Adagrad(model.parameters(), lr=lr)</span><br><span class="line">    <span class="keyword">if</span> optim_state_dict:  <span class="comment"># load from the optimizer_state_dict</span></span><br><span class="line">        optimizer.load_state_dict(optim_state_dict)</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model: my_model, opt, epoch, train_batch, log_point: <span class="built_in">list</span>, args: config</span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    train for one epoch</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param opt:</span></span><br><span class="line"><span class="string">    :param epoch:</span></span><br><span class="line"><span class="string">    :param train_batch:</span></span><br><span class="line"><span class="string">    :param log_point:</span></span><br><span class="line"><span class="string">    :param args:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.train()</span><br><span class="line">    clip = args.clip</span><br><span class="line">    curr_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_batch):</span><br><span class="line">        x, y = sample</span><br><span class="line">        <span class="keyword">with</span> torch.autograd.set_detect_anomaly(<span class="literal">True</span>):</span><br><span class="line">            opt.zero_grad()</span><br><span class="line">            out, hidden = model.forward(x, y)</span><br><span class="line">            <span class="comment"># Input: (N,C) where C = number of classes, Target (N) where target value is between 0 and C-1 inclusive</span></span><br><span class="line">            output = out[<span class="number">1</span>:].squeeze(dim=<span class="number">1</span>)  <span class="comment"># ignore the first token, (first token is 0 for output, and BOS for y)</span></span><br><span class="line">            loss = cal_loss(output, y[<span class="number">1</span>:])  <span class="comment"># , ignore_index=train_batch.token2idx["PAD"]</span></span><br><span class="line">            loss.backward()</span><br><span class="line">            <span class="comment"># Gradient clipping</span></span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</span><br><span class="line">            opt.step()</span><br><span class="line">        curr_loss += loss.item()</span><br><span class="line">    <span class="comment"># epoch_loss[Token.EPOCH + str(epoch)] = curr_loss / len(train_batch)</span></span><br><span class="line">    <span class="keyword">return</span> curr_loss / <span class="built_in">len</span>(train_batch)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_train</span>(<span class="params">model: my_model, opt, sen_data, log_point: <span class="built_in">list</span>, args: config, loss_hist=defaultdict(<span class="params"><span class="built_in">list</span></span>)</span>):</span></span><br><span class="line">    save_point = args.start_epoch</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(args.start_epoch, args.end_epoch):</span><br><span class="line">        train_loss = train(model=model, opt=opt, epoch=epoch, train_batch=sen_data.samples, log_point=log_point,</span><br><span class="line">                           args=args)</span><br><span class="line">        valid_loss = <span class="built_in">eval</span>(model, sen_data, args=args, loss_hist=loss_hist, epoch=epoch)</span><br><span class="line">        <span class="comment"># check if it's improving, if yes replace the old with the better one</span></span><br><span class="line">        <span class="comment"># valid_loss = valid_hist[Token.VALID_LOSS + str(epoch)]</span></span><br><span class="line">        <span class="keyword">if</span> (valid_loss &lt; args.best_valid_loss <span class="keyword">and</span> epoch &gt; save_point):</span><br><span class="line">            best_valid_loss = valid_loss</span><br><span class="line">            save_model(model, opt, args, loss_hist, best_valid_loss, epoch, <span class="literal">True</span>)</span><br><span class="line">            save_point = save_point + args.save_every  <span class="comment"># save interval</span></span><br><span class="line">            <span class="keyword">if</span> (args.break_point <span class="keyword">and</span> epoch == args.break_point):</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">        loss_hist[Token.TRAIN_HIST].append(train_loss)</span><br><span class="line">        loss_hist[Token.VALID_HIST].append(valid_loss)</span><br><span class="line">    <span class="keyword">return</span> loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eval</span>(<span class="params">model: my_model, sen_data, args: config, loss_hist=defaultdict(<span class="params"><span class="built_in">list</span></span>), epoch=<span class="string">''</span></span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    data is a list containing tuples,each tuple contains x, y</span></span><br><span class="line"><span class="string">    :param model:</span></span><br><span class="line"><span class="string">    :param data:</span></span><br><span class="line"><span class="string">    :param label:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    curr_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        test_samples = sen_data.test_samples</span><br><span class="line">        <span class="keyword">for</span> i, sample <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_samples):</span><br><span class="line">            x, y = sample</span><br><span class="line">            <span class="comment"># turn off teacher_forcing</span></span><br><span class="line">            args.teacher_forcing_ratio = <span class="number">0</span></span><br><span class="line">            pred, hidden = model.forward(x, y) <span class="comment"># pred shape e.g. [6,1,11]</span></span><br><span class="line">            <span class="comment"># pred [trg_seq_len, batch_size, trg_output_dim]</span></span><br><span class="line">            <span class="comment"># output = pred[1:].squeeze(dim=1)</span></span><br><span class="line">            output = pred[<span class="number">1</span>:].view(-<span class="number">1</span>, args.trg_vocab_size)</span><br><span class="line">            loss = cal_loss(output, y[<span class="number">1</span>:])</span><br><span class="line">            <span class="comment"># print_preds(i, loss, pred, y)</span></span><br><span class="line">            <span class="comment"># loss_hist[Token.EVAL].append(loss)</span></span><br><span class="line">            curr_loss += loss.item()</span><br><span class="line">            <span class="keyword">if</span> (args.log_eval <span class="keyword">and</span> i <span class="keyword">in</span> args.eval_log_points):</span><br><span class="line">                loss_hist[Token.EVAL_LOG_POINTS].append(pred_y_str(i, loss, pred, x=x, y=y, vocab=sen_data))</span><br><span class="line">        <span class="comment"># loss_hist[Token.VALID_LOSS + str(epoch)] = curr_loss / len(eval_batch)</span></span><br><span class="line">    <span class="keyword">return</span> curr_loss / <span class="built_in">len</span>(test_samples)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pred_y_str</span>(<span class="params">i, loss, pred, x, y, vocab, print_out=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    a formated string contains the basic info of loss, x,y, pred</span></span><br><span class="line"><span class="string">    :param i:</span></span><br><span class="line"><span class="string">    :param loss:</span></span><br><span class="line"><span class="string">    :param pred:</span></span><br><span class="line"><span class="string">    :param x:</span></span><br><span class="line"><span class="string">    :param y:</span></span><br><span class="line"><span class="string">    :param print_out:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pred_format = pred.squeeze(dim=<span class="number">1</span>).argmax(dim=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> pred_format.dim() &gt; <span class="number">1</span>:</span><br><span class="line">        pred_format = pred_format.view(-<span class="number">1</span>)</span><br><span class="line">    pred_words = vocab.idx2sentence(pred_format)</span><br><span class="line">    trg_words = vocab.idx2sentence(y)</span><br><span class="line">    input_words = vocab.idx2sentence(x)</span><br><span class="line">    formated = <span class="string">f"iteration <span class="subst">{i}</span> loss: loss <span class="subst">{loss}</span>\n\tpred: <span class="subst">{pred_format}</span> | y: <span class="subst">{y}</span>\n\tx: <span class="subst">{input_words}</span>\n\ty_hat: <span class="subst">{pred_words}</span>\n\ty: <span class="subst">{trg_words}</span>"</span></span><br><span class="line">    <span class="keyword">if</span> print_out:</span><br><span class="line">        <span class="built_in">print</span>(formated)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> formated</span><br><span class="line">    <span class="comment"># print(f"iteration {i} loss: loss {loss}")</span></span><br><span class="line">    <span class="comment"># # print(f"pred(converted): {pred_words} | y: {trg_words}")</span></span><br><span class="line">    <span class="comment"># print(f"pred(formated): {pred_format} | y: {y}")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># def print_preds(i, loss, pred, y):</span></span><br><span class="line"><span class="comment">#     pred_format = pred.squeeze(dim=1).argmax(dim=-1)</span></span><br><span class="line"><span class="comment">#     if pred_format.dim() &gt; 1:</span></span><br><span class="line"><span class="comment">#         pred_format = pred_format.view(-1)</span></span><br><span class="line"><span class="comment">#     print(f"iteration {i} loss: loss {loss}")</span></span><br><span class="line"><span class="comment">#     pred_words = sen_data.idx2sentence(pred_format)</span></span><br><span class="line"><span class="comment">#     trg_words = sen_data.idx2sentence(y)</span></span><br><span class="line"><span class="comment">#     # print(f"pred(converted): {pred_words} | y: {trg_words}")</span></span><br><span class="line"><span class="comment">#     print(f"pred(formated): {pred_format} | y: {y}")</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model_optimizer</span>(<span class="params">vocab_size, args: config, load_chkpoint=<span class="literal">False</span></span>):</span></span><br><span class="line">    model = my_model(src_vocab_size=vocab_size, args=args, trg_vocab_size=vocab_size)</span><br><span class="line">    opt = build_optimizer(model, args.lr)</span><br><span class="line">    loss_hist = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    <span class="keyword">if</span> load_chkpoint:</span><br><span class="line">        chkpoint = torch.load(args.entire_model_path)</span><br><span class="line">        model_sd = chkpoint[Token.MODEL_STATE_DICT]</span><br><span class="line">        optim_sd = chkpoint[Token.OPTIM_STATE_DICT]</span><br><span class="line">        epoch = chkpoint[Token.START_EPOCH]</span><br><span class="line">        loss_hist = chkpoint[Token.LOSS_HIST]</span><br><span class="line">        best_valid_loss = chkpoint[Token.BEST_VALID_LOSS]</span><br><span class="line">        args.best_valid_loss = best_valid_loss</span><br><span class="line">        args.start_epoch = chkpoint[Token.START_EPOCH]</span><br><span class="line">        model.load_state_dict(model_sd)</span><br><span class="line">        opt.load_state_dict(optim_sd)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, opt, loss_hist</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_model</span>():</span></span><br><span class="line">    <span class="keyword">from</span> examples.my_nn.my_config <span class="keyword">import</span> curr_config</span><br><span class="line">    sen_data = SentenceData(load_from_file=<span class="literal">False</span>)</span><br><span class="line">    data = sen_data.samples</span><br><span class="line">    <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(data):</span><br><span class="line">        <span class="comment"># if i % 100 == 0:</span></span><br><span class="line">        <span class="built_in">print</span>(d[<span class="number">0</span>], d[<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># vocab_size initially 10, then add PAD, now 11</span></span><br><span class="line">    vocab_size = sen_data.vocab_size</span><br><span class="line">    epochs = <span class="number">500</span></span><br><span class="line">    log_point = [<span class="number">0</span>, epochs - <span class="number">1</span>]  <span class="comment"># log the first epoch and the last epoch stats</span></span><br><span class="line">    args = curr_config</span><br><span class="line">    load_chkpoint = <span class="literal">False</span></span><br><span class="line">    model, opt, loss_hist = build_model_optimizer(vocab_size, args, load_chkpoint)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"."</span> * <span class="number">30</span>, <span class="string">"Start Training"</span>, <span class="string">"."</span> * <span class="number">30</span>)</span><br><span class="line">    start_training = datetime.now()</span><br><span class="line">    loss_hist = run_train(model=model, opt=opt, sen_data=sen_data, log_point=log_point, args=args, loss_hist=loss_hist)</span><br><span class="line">    end_training = datetime.now()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"."</span> * <span class="number">30</span>, <span class="string">"Finish Training"</span>, <span class="string">"."</span> * <span class="number">30</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"training time <span class="subst">{end_training - start_training}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"training loss hist:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"last few in train_hist list: <span class="subst">{loss_hist[Token.TRAIN_HIST][-<span class="number">5</span>:]}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"last few in valid_hist list: <span class="subst">{loss_hist[Token.VALID_HIST][-<span class="number">5</span>:]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_model</span>():</span></span><br><span class="line">    <span class="keyword">from</span> examples.my_nn.my_config <span class="keyword">import</span> curr_config</span><br><span class="line">    sen_data = SentenceData(load_from_file=<span class="literal">False</span>)</span><br><span class="line">    data = sen_data.samples</span><br><span class="line">    <span class="comment"># vocab_size initially 10, then add PAD, now 11</span></span><br><span class="line">    <span class="comment"># num_layers = 2</span></span><br><span class="line">    lr = <span class="number">0.1</span></span><br><span class="line">    emb_size = <span class="number">16</span></span><br><span class="line">    h_size = <span class="number">64</span></span><br><span class="line">    vocab_size = sen_data.vocab_size</span><br><span class="line">    epochs = <span class="number">500</span></span><br><span class="line">    log_point = [<span class="number">0</span>, epochs - <span class="number">1</span>]  <span class="comment"># log the first epoch and the last epoch stats</span></span><br><span class="line">    args = curr_config</span><br><span class="line">    args.trg_vocab_size = vocab_size</span><br><span class="line">    args.src_vocab_size = vocab_size</span><br><span class="line">    model, opt, loss_hist = build_model_optimizer(vocab_size, args, load_chkpoint=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"."</span> * <span class="number">30</span>, <span class="string">"Start Testing"</span>, <span class="string">"."</span> * <span class="number">30</span>)</span><br><span class="line">    start_training = datetime.now()</span><br><span class="line">    <span class="comment"># loss_hist = run_train(model=model, opt=opt, sen_data=sen_data, log_point=log_point, args=args)</span></span><br><span class="line">    args.log_eval = <span class="literal">True</span></span><br><span class="line">    test_loss = <span class="built_in">eval</span>(model, sen_data, args=args, loss_hist=loss_hist)</span><br><span class="line">    end_training = datetime.now()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"."</span> * <span class="number">30</span>, <span class="string">"Finish Testing"</span>, <span class="string">"."</span> * <span class="number">30</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Testing time <span class="subst">{end_training - start_training}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"loss: "</span>, test_loss)</span><br><span class="line">    <span class="keyword">for</span> pred_result <span class="keyword">in</span> loss_hist[Token.EVAL_LOG_POINTS]:</span><br><span class="line">        <span class="built_in">print</span>(pred_result)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    run_model()</span><br><span class="line">    <span class="comment"># test_model()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------------------------------------------</span></span><br><span class="line"><span class="comment"># the following with 500 epochs, with ignore_idx,</span></span><br><span class="line"><span class="comment">#     lr = 0.1</span></span><br><span class="line"><span class="comment">#     emb_size = 16</span></span><br><span class="line"><span class="comment">#     h_size = 64</span></span><br><span class="line"><span class="comment">#     num_layers = 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0 loss: loss 0.00038905630935914814</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([0, 7, 4, 8, 5, 9, 2]) | y: tensor([1, 7, 4, 8, 5, 9, 2])</span></span><br><span class="line"><span class="comment"># iteration 1 loss: loss 0.0002469978644512594</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([0, 7, 6, 8, 2, 2, 2]) | y: tensor([1, 7, 6, 8, 2, 0, 0])</span></span><br><span class="line"><span class="comment"># iteration 2 loss: loss 0.0001996556093217805</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([ 0, 10,  7,  2,  2,  2,  2]) | y: tensor([ 1, 10,  7,  2,  0,  0,  0])</span></span><br><span class="line"><span class="comment"># training time 0:00:41.454838</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(0.0004), tensor(0.0002), tensor(0.0002)]</span></span><br><span class="line"><span class="comment"># 0.000835709783132188</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># VS</span></span><br><span class="line"><span class="comment"># with the same setting as above except without ignore_idx</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># iteration 0 loss: loss 0.00021162799384910613</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([0, 7, 4, 8, 5, 9, 2]) | y: tensor([1, 7, 4, 8, 5, 9, 2])</span></span><br><span class="line"><span class="comment"># iteration 1 loss: loss 0.00024118444707710296</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([0, 7, 6, 8, 2, 0, 0]) | y: tensor([1, 7, 6, 8, 2, 0, 0])</span></span><br><span class="line"><span class="comment"># iteration 2 loss: loss 0.0001666987664066255</span></span><br><span class="line"><span class="comment"># pred(formated): tensor([ 0, 10,  7,  2,  0,  0,  0]) | y: tensor([ 1, 10,  7,  2,  0,  0,  0])</span></span><br><span class="line"><span class="comment"># training time 0:00:39.467891</span></span><br><span class="line"><span class="comment"># eval loss:</span></span><br><span class="line"><span class="comment"># [tensor(0.0002), tensor(0.0002), tensor(0.0002)]</span></span><br><span class="line"><span class="comment"># 0.0006195112073328346</span></span><br><span class="line"></span><br><span class="line"><span class="comment">####Comments</span></span><br><span class="line"><span class="comment"># looks like setting ignore_idx will ignore the corrsponding element with its value = ignred index,</span></span><br><span class="line"><span class="comment"># don't set this will actually actually calculate the gradient for the padded part as well, which</span></span><br><span class="line"><span class="comment"># I think is why it's now has exactly the same value as y, if ignore_idx is set, we discard the padding part</span></span><br><span class="line"><span class="comment"># when calculating the gradient so the model doesn't learn the padding part, only the part we want to calculate.</span></span><br><span class="line"><span class="comment"># but it looks like it also changes the loss a little bit since if the pred and label has exactly the same padding part,</span></span><br><span class="line"><span class="comment"># it will decrease the loss, shouldn't them be the same? I think they should be the same if we don't care about the</span></span><br><span class="line"><span class="comment"># padding part, but looks like the loss is still considered although the actual gradient updating is not done on this part.</span></span><br><span class="line"><span class="comment"># this will give a false stats on how well the model is doing, and the actual loss stats.</span></span><br><span class="line"><span class="comment"># --------------------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># without dropout</span></span><br><span class="line"><span class="comment"># training time 0:01:15.876367</span></span><br><span class="line"><span class="comment"># training loss hist:</span></span><br><span class="line"><span class="comment"># last few in train_hist list: [0.0008435873120712737, 0.0008417083881795406, 0.0008398161735385656, 0.0008378611140263578, 0.0008360317830617229]</span></span><br><span class="line"><span class="comment"># last few in valid_hist list: [0.0008503035254155596, 0.0008483478062165281, 0.0008464172521295646, 0.0008445370379680147, 0.0008426210163937261]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># with encoder dropout - looks like it improves a little bit</span></span><br><span class="line"><span class="comment"># training time 0:01:15.705963</span></span><br><span class="line"><span class="comment"># training loss hist:</span></span><br><span class="line"><span class="comment"># last few in train_hist list: [0.000696311040276972, 0.0006907911107797796, 0.0006953847526650255, 0.0006900144362589344, 0.0006926181231392547]</span></span><br><span class="line"><span class="comment"># last few in valid_hist list: [0.000788785342592746, 0.0007867771346354857, 0.0007849847170291469, 0.0007829964063906422, 0.0007807132933521643]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This is with both encoder dropout and decoder dropout (0.3)</span></span><br><span class="line"><span class="comment"># training time 0:01:16.667416</span></span><br><span class="line"><span class="comment"># training loss hist:</span></span><br><span class="line"><span class="comment"># last few in train_hist list: [0.0007482070941478014, 0.0007803615687104563, 0.0007576028195520242, 0.0007112358774368962, 0.000728180049918592]</span></span><br><span class="line"><span class="comment"># last few in valid_hist list: [0.0007912752334959805, 0.0007896779376703004, 0.0007881191171084841, 0.0007865006919018924, 0.0007843688751260439]</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/reading-notes/tags/pytorch/">pytorch</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/reading-notes/cqNotes/ML/Pytorch/cq_pytorch_intro_202011/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg" onerror="onerror=null;src='/reading-notes/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">cq_pytorch_intro_202011</div></div></a></div><div class="next-post pull-right"><a href="/reading-notes/cqNotes/EnglishNotes/en-speaking-notes-01/"><img class="next-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2012/10/29/15/38/binary-code-63529_1280.jpg" onerror="onerror=null;src='/reading-notes/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Speaking Notes (01)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/reading-notes/cqNotes/ML/Pytorch/cq_pytorch_cheat%20sheet/" title="cq_pytorch_cheatsheet"><img class="cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2015/06/08/15/11/typewriter-801921_1280.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-01</div><div class="title">cq_pytorch_cheatsheet</div></div></a></div><div><a href="/reading-notes/cqNotes/ML/Pytorch/cq_pytorch_contiguous%20vs%20non-contiguous%20arrays/" title="cq_pytorch_contiguous vs non-contiguous arrays"><img class="cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-01</div><div class="title">cq_pytorch_contiguous vs non-contiguous arrays</div></div></a></div><div><a href="/reading-notes/cqNotes/ML/Pytorch/cq_pytorch_examples/" title="cq_pytorch_examples"><img class="cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2015/06/08/15/11/typewriter-801921_1280.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-01</div><div class="title">cq_pytorch_examples</div></div></a></div><div><a href="/reading-notes/cqNotes/ML/Pytorch/cq_pytorch_intro_202011/" title="cq_pytorch_intro_202011"><img class="cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://cdn.pixabay.com/photo/2021/09/09/04/38/binary-6609473_1280.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-01-01</div><div class="title">cq_pytorch_intro_202011</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch-impl-examples"><span class="toc-number">1.</span> <span class="toc-text">Pytorch Impl Examples</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#common-how-to"><span class="toc-number">1.1.</span> <span class="toc-text">Common How To</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#word-embeddings"><span class="toc-number">1.1.0.1.</span> <span class="toc-text">Word Embeddings</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#one-hot-encodings"><span class="toc-number">1.1.0.1.1.</span> <span class="toc-text">One Hot Encodings</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#use-nn.embedding"><span class="toc-number">1.1.0.1.2.</span> <span class="toc-text">Use nn.Embedding</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#mask-inputs"><span class="toc-number">1.1.0.1.3.</span> <span class="toc-text">mask inputs</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#my-word-embedding-example"><span class="toc-number">1.1.0.1.4.</span> <span class="toc-text">My word embedding example</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#neural-network-modules"><span class="toc-number">1.2.</span> <span class="toc-text">Neural Network Modules</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#curr_config-file"><span class="toc-number">1.2.0.0.1.</span> <span class="toc-text">curr_config file</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#common-code"><span class="toc-number">1.2.1.</span> <span class="toc-text">Common Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#back-propagation"><span class="toc-number">1.2.2.</span> <span class="toc-text">Back-propagation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#simple-gradient-calculation"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Simple gradient calculation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#chain-rule-examples"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Chain rule examples</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn.sequential-example"><span class="toc-number">1.2.3.</span> <span class="toc-text">nn.Sequential Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fnn-examples"><span class="toc-number">1.2.4.</span> <span class="toc-text">FNN Examples</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#feedforward-nn-manual-cal-and-nn.linear"><span class="toc-number">1.2.4.0.1.</span> <span class="toc-text">Feedforward NN Manual Cal and nn.Linear</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#nn.linear-concrete-examples"><span class="toc-number">1.2.4.0.2.</span> <span class="toc-text">nn.Linear concrete examples</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#simple-fnn-implementation"><span class="toc-number">1.2.4.0.3.</span> <span class="toc-text">Simple FNN Implementation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#section"><span class="toc-number">1.2.5.</span> <span class="toc-text"></span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#fnn-nn.linear-forward-backward-iteration-1"><span class="toc-number">1.2.5.0.1.</span> <span class="toc-text">FNN nn.Linear forward backward iteration 1</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#feedforward-pass-with-pytorch-nn.linear"><span class="toc-number">1.2.5.0.2.</span> <span class="toc-text">Feedforward Pass with Pytorch nn.Linear</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#fnn-nn.linear-example-switch-numbers"><span class="toc-number">1.2.5.0.3.</span> <span class="toc-text">FNN nn.Linear Example Switch Numbers</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#fnn-and-operation-example"><span class="toc-number">1.2.5.0.4.</span> <span class="toc-text">FNN And Operation Example</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#fnn-xor-example"><span class="toc-number">1.2.5.0.5.</span> <span class="toc-text">FNN XOR Example</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn-examples"><span class="toc-number">1.2.6.</span> <span class="toc-text">RNN Examples</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lstm"><span class="toc-number">1.2.6.1.</span> <span class="toc-text">LSTM</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#xor-lstm-example"><span class="toc-number">1.2.6.1.1.</span> <span class="toc-text">XOR LSTM Example</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#seq2seq-encoder-decoder"><span class="toc-number">1.2.6.2.</span> <span class="toc-text">Seq2Seq, Encoder Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#lstm-seq2seq-reverse-sentences"><span class="toc-number">1.2.6.2.1.</span> <span class="toc-text">LSTM Seq2Seq Reverse Sentences</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/reading-notes/js/utils.js"></script><script src="/reading-notes/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/reading-notes/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>