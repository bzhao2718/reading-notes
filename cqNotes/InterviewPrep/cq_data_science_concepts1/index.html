<!DOCTYPE html><html class="hide-aside" lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>cq_data_science_concepts1 | ReadingNotes</title><meta name="author"><meta name="copyright"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="General Algorithms and Theory Bias-Variance Tradeoff Bias This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A h">
<meta property="og:type" content="article">
<meta property="og:title" content="cq_data_science_concepts1">
<meta property="og:url" content="https://bzhao2718.github.io/reading-notes/cqNotes/InterviewPrep/cq_data_science_concepts1/index.html">
<meta property="og:site_name" content="ReadingNotes">
<meta property="og:description" content="General Algorithms and Theory Bias-Variance Tradeoff Bias This part of the generalization error is due to wrong assumptions, such as assuming that the data is linear when it is actually quadratic. A h">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://bzhao2718.github.io/reading-notes/default_cover/reading-7229927_1280.jpg">
<meta property="article:published_time" content="2021-01-01T03:53:22.000Z">
<meta property="article:modified_time" content="2021-11-25T22:05:36.162Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bzhao2718.github.io/reading-notes/default_cover/reading-7229927_1280.jpg"><link rel="shortcut icon" href="/reading-notes/img/favicon.png"><link rel="canonical" href="https://bzhao2718.github.io/reading-notes/cqNotes/InterviewPrep/cq_data_science_concepts1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/reading-notes/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/reading-notes/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'cq_data_science_concepts1',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-11-26 06:05:36'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/carousel-touch.min.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/archives/"><div class="headline">Articles</div><div class="length-num">106</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/tags/"><div class="headline">Tags</div><div class="length-num">22</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/reading-notes/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/reading-notes/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/reading-notes/">ReadingNotes</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/reading-notes/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/reading-notes/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">cq_data_science_concepts1</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-01-01T03:53:22.000Z" title="Created 2021-01-01 11:53:22">2021-01-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-11-25T22:05:36.162Z" title="Updated 2021-11-26 06:05:36">2021-11-26</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>33min</span></span></div></div></div><article class="post-content" id="article-container"><h1 id="general">General</h1>
<h2 id="algorithms-and-theory">Algorithms and Theory</h2>
<h3 id="bias-variance-tradeoff">Bias-Variance Tradeoff</h3>
<p><strong><em>Bias</em></strong> This part of the generalization error
is due to wrong assumptions, such as assuming that the data is linear
when it is actually quadratic. A high-bias mode is most likely to
underfit the training data. This can lead to the model underfitting your
data, making it hard for it to have high predictive accuracy and for you
to generalize your knowledge from the training set to the test set. Low
accuracy for both training set and test set. Try a more complex
model.</p>
<p><strong><em>Variance</em></strong> Variance is error due to too much
complexity in the learning algorithm you’re using. This part is due to
the model’s excessive sensitivity to small variations in the training
data. A model with many degrees of freedom (such as a high-degree
polynomial model) is likely to have high variance and thus overfit the
training data. This leads to the algorithm being highly sensitive to
high degrees of variation in your training data, which can lead your
model to overfit the data. Try a simpler model, more dataset,
regularizations.</p>
<p><strong><em>Variance</em> refers to the amount by which <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.891ex" role="img" focusable="false" viewBox="0 -1073 550 1278"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(457,279) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container></span> would change if we estimated it
using a different training data. Since the training data are used to fit
the statistical learning method, different training data sets will
result in a different <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.891ex" role="img" focusable="false" viewBox="0 -1073 550 1278"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(457,279) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container></span>.</strong> But ideally the
estimate for <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container></span> should not vary too
much between training sets. However, if a method has high variance then
small changes in the training data can result in large changes in <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.891ex" role="img" focusable="false" viewBox="0 -1073 550 1278"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(457,279) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container></span>. In general, more flexible
statistical methods have higher variance.</p>
<p><strong><em>Irreducible error</em></strong> This part is due to the
noisiness of the data itself. The only way to reduce this part of the
error is to clean up the data (e.g., fix the data sources, such as
broken sensors, or detect and remove outliers).</p>
<p>The bias-variance decomposition essentially decomposes the learning
error from any algorithm by adding the bias, the variance and a bit of
irreducible error due to noise in the underlying dataset. The goal is
typically to have a low bias and variance.</p>
<p>Increasing a model’s complexity will typically increase its variance
and reduce its bias. Conversely, reducing a model’s complexity increases
its bias and reduces its variance. This is why it is called a
trade-off.</p>
<h3 id="underfitting-and-overfitting">Underfitting and Overfitting</h3>
<p>Complex models such as deep neural networks can detect subtle
patterns in the data, but if the training set is noisy, or if it is too
small (which introduces sampling noise), then the model is likely to
detect patterns in the noise itself. Overfitting happens when the model
is too complex relative to the amount and noisiness of the training
data. This happens because our statistical learning procedure is working
too hard to find patterns in the training data, and may be picking up
some patterns that are just caused by random chance rather than by true
properties of the unknown function. Likely to be that the train error is
low but test errors are high.</p>
<p>Here ares possible solutions:</p>
<ul>
<li>Simplify the model by selecting one with fewer parameters (e.g., a
linear model rather than a high-degree polynomial model), by reducing
the number of attributes in the training data, or by constraining the
model.</li>
<li>Gather more training data.</li>
<li>Reduce the noise in the training data (e.g., fix data errors and
remove outliers).</li>
</ul>
<p>Constraining a model to make it simpler and reduce the risk of
overfitting is called <em>regularization</em>. You want to find the
right balance between fitting the training data perfectly and keeping
the model simply enough to ensure that it will generalize well.</p>
<p><strong><em>Underfitting</em></strong> is the opposite of
overfitting: it occurs when your model is too simple to learn the
underlying structure of the data. Likely to be that both train and test
errors are high. Here are the main options for fixing this problem:</p>
<ul>
<li>Select a more powerful model, with more parameters.</li>
<li>Feed better features to the learning algorithm (feature
engineering).</li>
<li>Reduce the constraints on the model (e.g., reduce the regularization
hyperparameter).</li>
</ul>
<h3 id="how-roc-curve-works">How ROC Curve works</h3>
<h3 id="precision-and-recall">Precision and Recall</h3>
<h3 id="bayes-theorem">Bayes’ Theorem</h3>
<p>Bayes’ Theorem gives you the posterior probability of an event given
what is known as prior knowledge.</p>
<p>Why Naive: Despite its practical applications, especially in text
mining, Naive Bayes is considered “Naive” because it makes an assumption
that is virtually impossible to see in real-life data: the conditional
probability is calculated as the pure product of the individual
probabilities of components. This implies the absolute independence of
features — a condition probably never met in real life.</p>
<h3 id="l1-and-l2-regularization">L1 and L2 Regularization</h3>
<h3 id="type-i-and-type-ii-error">Type-I and Type-II Error</h3>
<p>Type I error is a false positive, while Type II error is a false
negative. Briefly stated, Type I error means claiming something has
happened when it hasn’t, while Type II error means that you claim
nothing is happening when in fact something is.</p>
<p>A clever way to think about this is to think of Type I error as
telling a man he is pregnant, while Type II error means you tell a
pregnant woman she isn’t carrying a baby.</p>
<h3 id="fourier-transform">Fourier transform</h3>
<p>A Fourier transform is a generic method to decompose generic
functions into a superposition of symmetric functions. Or as this more
intuitive tutorial puts it, given a smoothie, it’s how we find the
recipe. The Fourier transform finds the set of cycle speeds, amplitudes
and phases to match any time signal. A Fourier transform converts a
signal from time to frequency domain — it’s a very common way to extract
features from audio signals or other time series such as sensor
data.</p>
<h3 id="probability-vs-likelihood">Probability vs Likelihood</h3>
<p>It's quite like the distinction between variables and parameters in a
differential equation: sometimes we want to study the solution (i.e., we
focus on the variables as the argument) and sometimes we want to study
how the solution varies with the parameters. The main distinction is
that in statistics we rarely need to study the simultaneous variation of
both sets of arguments; there is no statistical object that naturally
corresponds to changing both the data <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span> and the model parameters <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span>. That's why you hear more about
this dichotomy than you would in analogous mathematical settings.</p>
<p>The distinction between probability and likelihood is fundamentally
important: Probability attaches to possible results; likelihood attaches
to hypotheses.</p>
<p>This is from <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.quora.com/What-is-the-difference-between-probability-and-likelihood-1">quora</a>.</p>
<p>Suppose you have a probability model with parameters <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span>, <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.882ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2600 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1464,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1742,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2211,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> Has two names. It can be
called the <strong>probability of x</strong> (given <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span>), or the <strong>likelihood of
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span></strong> (Given that <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span> was observed). The likelihood is a
function of <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span>.</p>
<p>Similarly, <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.882ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2600 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1464,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1742,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(2211,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> is a
function of two variables. If you hold <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span> constant, you get the
<strong>probability function</strong> (function of <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span>), but if you hold <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span> constant, you get the
<strong>likelihood function</strong> (function of <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="1.061ex" height="1.618ex" role="img" focusable="false" viewBox="0 -705 469 715"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></svg></mjx-container></span>).</p>
<ul>
<li>Probability is the percentage that a success occur. For example, we
do the binomial experiment by tossing a coin. We suppose that the event
that we get the face of coin in success, so the probability of success
now is 0.5 because the probability of face and back of a coin is equal.
<strong>0.5 is the probability of a success.</strong></li>
<li>Likelihood is the conditional probability. The same example, we toss
the coin 10 times ,and we suppose that we get 7 success ( show the face)
and 3 failed ( show the back). The likelihood is calculated (for
binomial distribution, it can be vary depend on the distributions)
<ul>
<li><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="63.022ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 27855.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(681,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(1070,0)"><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z"></path></g><g data-mml-node="mo" transform="translate(1570,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(1848,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(3126,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mtext" transform="translate(3515,0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(3765,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(4250,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mtext" transform="translate(4701,0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(4951,0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(5632,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6021,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(6870.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(7926.6,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(8778.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(9167.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(10055.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(10943.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(11831.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(12719.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(13607.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(14495.6,0)"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(15383.6,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(16087.6,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(16791.6,0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(17495.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(17884.6,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(18162.6,0)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mo" transform="translate(18909.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(19965.1,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="35" d="M164 157Q164 133 148 117T109 101H102Q148 22 224 22Q294 22 326 82Q345 115 345 210Q345 313 318 349Q292 382 260 382H254Q176 382 136 314Q132 307 129 306T114 304Q97 304 95 310Q93 314 93 485V614Q93 664 98 664Q100 666 102 666Q103 666 123 658T178 642T253 634Q324 634 389 662Q397 666 402 666Q410 666 410 648V635Q328 538 205 538Q174 538 149 544L139 546V374Q158 388 169 396T205 412T256 420Q337 420 393 355T449 201Q449 109 385 44T229 -22Q148 -22 99 32T50 154Q50 178 61 192T84 210T107 214Q132 214 148 197T164 157Z" transform="translate(778,0)"></path></g><g data-mml-node="mo" transform="translate(21243.1,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(21909.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(22687.9,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(23132.6,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(23577.2,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(24021.9,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(25077.7,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(778,0)"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(1278,0)"></path><path data-c="37" d="M55 458Q56 460 72 567L88 674Q88 676 108 676H128V672Q128 662 143 655T195 646T364 644H485V605L417 512Q408 500 387 472T360 435T339 403T319 367T305 330T292 284T284 230T278 162T275 80Q275 66 275 52T274 28V19Q270 2 255 -10T221 -22Q210 -22 200 -19T179 0T168 40Q168 198 265 368Q285 400 349 489L395 552H302Q128 552 119 546Q113 543 108 522T98 479L95 458V455H55V458Z" transform="translate(1778,0)"></path><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" transform="translate(2278,0)"></path></g></g></g></svg></mjx-container></span>​​</li>
<li><strong>0.1171</strong> is the probability that the above event will
happen (got 7 successes out of 10 trials) by knowing that the
probability of one success is 0.5(toss one time)</li>
</ul></li>
</ul>
<p>Therefore, Likelihood is the probability (conditional probability) of
a event ( a set of success ) occur by knowing the probability of a
success occur. Probability is the percentage that a success occur. (This
example uses binomial distribution)</p>
<p>From <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Likelihood_function">Likelihood
function - Wikipedia</a> It has a good example.</p>
<p>In statistics, the likelihood function (often simply called the
likelihood) measures the goodness of fit of a statistical model to a
sample of data for given values of the unknown parameters. It is formed
from the joint probability distribution of the sample, but viewed and
used as a function of the parameters only, thus treating the random
variables as fixed at the observed values.</p>
<p>The likelihood function describes a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Hypersurface">hypersurface</a> whose
peak, if it exists, represents the combination of model parameter values
that maximize the probability of drawing the sample obtained. The
procedure for obtaining these <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Arg_max">arguments of the
maximum</a> of the likelihood function is known as <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">maximum
likelihood estimation</a>, which for computational convenience is
usually done using the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Natural_logarithm">natural
logarithm</a> of the likelihood, known as the <strong>log-likelihood
function</strong>.</p>
<h3 id="generative-vs-discriminative-models">Generative VS
Discriminative Models</h3>
<p>In General, A Discriminative model models the <strong>decision
boundary between the classes</strong>. A Generative Model explicitly
models the <strong>actual distribution of each class</strong>. In final
both of them is predicting the conditional probability P(Animal |
Features). But Both models learn different probabilities.</p>
<p>A Generative Model learns the <strong>joint probability distribution
<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="6.307ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2787.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1464,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1908.7,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2398.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>​.</strong> It predicts the
conditional probability with the help of <strong>Bayes Theorem</strong>.
A Discriminative model ‌learns the <strong>conditional probability
distribution <span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.93ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2621 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892,0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1382,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1660,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(2232,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span>​</strong>.
Both of these models were generally used in <strong>supervised
learning</strong> problems.</p>
<p><strong>Examples:</strong> Generative classifiers: Naive Bayes,
Bayesian networks, Markov random fields, Hidden Markov Models (HMM);</p>
<p>Discriminative Classifiers: Logistic regression, traditional neural
networks, nearest neighbor, conditional Radom Fields (CRF)s. Or see this
paper
http://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf</p>
<h3 id="cross-validation-techniques-to-use-on-a-time-series-dataset">Cross
validation techniques to use on a time series dataset</h3>
<p>Instead of using standard k-folds cross-validation, you have to pay
attention to the fact that a time series is not randomly distributed
data — it is inherently ordered by chronological order. If a pattern
emerges in later time periods for example, your model may still pick up
on it even if that effect doesn’t hold in earlier years!</p>
<p>You’ll want to do something like forward chaining where you’ll be
able to model on past data then look at forward-facing data.</p>
<ul>
<li>fold 1 : training [1], test [2]</li>
<li>fold 2 : training [1 2], test [3]</li>
<li>fold 3 : training [1 2 3], test [4]</li>
<li>fold 4 : training [1 2 3 4], test [5]</li>
<li>fold 5 : training [1 2 3 4 5], test [6]</li>
</ul>
<p>See this documentation: <a target="_blank" rel="noopener external nofollow noreferrer" href="https://scikit-learn.org/stable/modules/cross_validation.html">3.1.
Cross-validation: evaluating estimator performance — scikit-learn 0.24.2
documentation</a></p>
<h3 id="how-is-a-decision-tree-pruned">How is a decision tree
pruned</h3>
<p>Pruning is what happens in decision trees when branches that have
weak predictive power are removed in order to reduce the complexity of
the model and increase the predictive accuracy of a decision tree model.
Pruning can happen bottom-up and top-down, with approaches such as
reduced error pruning and cost complexity pruning.</p>
<p>Pruning reduces the complexity of the final <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Statistical_classification">classifier</a>,
and hence improves predictive accuracy by the reduction of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>
<p>A tree that is too large risks <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> the
training data and poorly generalizing to new samples. A small tree might
not capture important structural information about the sample space.
However, it is hard to tell when a tree algorithm should stop because it
is impossible to tell if the addition of a single extra node will
dramatically decrease error. This problem is known as the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Horizon_effect">horizon effect</a>.
A common strategy is to grow the tree until each node contains a small
number of instances then use pruning to remove nodes that do not provide
additional information.</p>
<p>Pruning processes can be divided into two types (pre- and
post-pruning).</p>
<p><strong>Pre-pruning</strong> procedures prevent a complete induction
of the training set by replacing a stop () criterion in the induction
algorithm (e.g. max. Tree depth or information gain (Attr)&gt; minGain).
<strong>Post-pruning</strong> (or just pruning) is the most common way
of simplifying trees. Here, nodes and subtrees are replaced with leaves
to reduce complexity. Pruning can not only significantly reduce the size
but also improve the classification accuracy of unseen objects.</p>
<p>Bottom-up pruning</p>
<p>These procedures start at the last node in the tree (the lowest
point). Following recursively upwards, they determine the relevance of
each individual node. If the relevance for the classification is not
given, the node is dropped or replaced by a leaf. The advantage is that
no relevant sub-trees can be lost with this method. These methods
include Reduced Error Pruning (REP), Minimum Cost Complexity Pruning
(MCCP), or Minimum Error Pruning (MEP).</p>
<p>Top-down pruning</p>
<p>In contrast to the bottom-up method, this method starts at the root
of the tree. Following the structure below, a relevance check is carried
out which decides whether a node is relevant for the classification of
all n items or not. By pruning the tree at an inner node, it can happen
that an entire sub-tree (regardless of its relevance) is dropped. One of
these representatives is pessimistic error pruning (PEP), which brings
quite good results with unseen items.</p>
<p><strong>Reduced Error Pruning</strong> One of the simplest forms of
pruning is reduced error pruning. Starting at the leaves, each node is
replaced with its most popular class. If the prediction accuracy is not
affected then the change is kept. While somewhat naive, reduced error
pruning has the advantage of <strong>simplicity and speed</strong>.</p>
<p>An example here <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.displayr.com/machine-learning-pruning-decision-trees/">Machine
Learning: Pruning Decision Trees | Displayr</a></p>
<p>And Here <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cs.cmu.edu/~bhiksha/courses/10-601/decisiontrees/">Decision
Trees</a></p>
<h3 id="which-is-more-important-model-accuracy-or-model-performance">Which
is more important, model accuracy, or model performance?</h3>
<p>There are models with higher accuracy that can perform worse in
predictive power — how does that make sense? <a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Accuracy_paradox">Accuracy paradox -
Wikipedia</a></p>
<p>Well, it has everything to do with how model accuracy is only a
subset of model performance, and at that, a sometimes misleading one.
For example, if you wanted to detect fraud in a massive dataset with a
sample of millions, a more accurate model would most likely predict no
fraud at all if only a vast minority of cases were fraud. However, this
would be useless for a predictive model — a model designed to find fraud
that asserted there was no fraud at all! Questions like this help you
demonstrate that you understand model accuracy isn’t the be-all and
end-all of model performance.</p>
<h3 id="f-1-score-how-would-you-use-it">F-1 score, how would you use
it</h3>
<p>The F1 score is a measure of a model’s performance. It is a weighted
average of the precision and recall of a model, with results tending to
1 being the best, and those tending to 0 being the worst. You would use
it in classification tests where true negatives don’t matter much.</p>
<h3 id="how-would-you-handle-an-imbalanced-dataset">How would you handle
an imbalanced dataset?</h3>
<p>An imbalanced dataset is when you have, for example, a classification
test and 90% of the data is in one class. That leads to problems: an
accuracy of 90% can be skewed if you have no predictive power on the
other category of data! Here are a few tactics to get over the hump:</p>
<ol type="1">
<li>Collect more data to even the imbalances in the dataset.</li>
<li>Resample the dataset to correct for imbalances.</li>
<li>Try a different algorithm altogether on your dataset.</li>
</ol>
<p>What’s important here is that you have a keen sense for what damage
an unbalanced dataset can cause, and how to balance that.</p>
<h3 id="when-should-you-use-classification-over-regression">When should
you use classification over regression?</h3>
<p>Classification produces discrete values and dataset to strict
categories, while regression gives you continuous results that allow you
to better distinguish differences between individual points. You would
use classification over regression if you wanted your results to reflect
the belongingness of data points in your dataset to certain explicit
categories (ex: If you wanted to know whether a name was male or female
rather than just how correlated they were with male and female
names.)</p>
<h3 id="name-an-example-where-ensemble-techniques-might-be-useful.">Name
an example where ensemble techniques might be useful.</h3>
<p>Ensemble techniques use a combination of learning algorithms to
optimize better predictive performance. They typically reduce
overfitting in models and make the model more robust (unlikely to be
influenced by small changes in the training data).</p>
<p>You could list some examples of ensemble methods, from bagging to
boosting to a “bucket of models” method and demonstrate how they could
increase predictive power.</p>
<h3 id="how-do-you-ensure-youre-not-overfitting-with-a-model">How do you
ensure you’re not overfitting with a model?</h3>
<p>This is a simple restatement of a fundamental problem in machine
learning: the possibility of overfitting training data and carrying the
noise of that data through to the test set, thereby providing inaccurate
generalizations.</p>
<p>There are three main methods to avoid overfitting:</p>
<ol type="1">
<li>Keep the model simpler: reduce variance by taking into account fewer
variables and parameters, thereby removing some of the noise in the
training data.</li>
<li>Use cross-validation techniques such as k-folds
cross-validation.</li>
<li>Use regularization techniques such as LASSO that penalize certain
model parameters if they’re likely to cause overfitting.</li>
</ol>
<h3 id="what-evaluation-approaches-would-you-work-to-gauge-the-effectiveness-of-a-machine-learning-model">What
evaluation approaches would you work to gauge the effectiveness of a
machine learning model</h3>
<p>You would first split the dataset into training and test sets, or
perhaps use cross-validation techniques to further segment the dataset
into composite sets of training and test sets within the data. You
should then implement a choice selection of performance metrics: here is
a fairly comprehensive list. You could use measures such as the F1
score, the accuracy, and the confusion matrix. What’s important here is
to demonstrate that you understand the nuances of how a model is
measured and how to choose the right performance measures for the right
situations.</p>
<h3 id="how-would-you-evaluate-a-logistic-regression-model">How would
you evaluate a logistic regression model?</h3>
<p>A subsection of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://stats.stackexchange.com/questions/71517/evaluating-a-logistic-regression#71522">the
question posed here</a>. You have to demonstrate an understanding of
what the typical goals of a logistic regression are (classification,
prediction, etc.) and bring up a few examples and use cases.</p>
<h3 id="whats-the-kernel-trick-and-how-is-it-useful">What’s the “kernel
trick” and how is it useful?</h3>
<p>The Kernel trick involves kernel functions that can enable in
higher-dimension spaces without explicitly calculating the coordinates
of points within that dimension: instead, kernel functions compute the
inner products between the images of all pairs of data in a feature
space. This allows them the very useful attribute of calculating the
coordinates of higher dimensions while being computationally cheaper
than the explicit calculation of said coordinates. Many algorithms can
be expressed in terms of inner products. Using the kernel trick enables
us effectively run algorithms in a high-dimensional space with
lower-dimensional data.</p>
<h2 id="programming">Programming</h2>
<h3 id="how-do-you-handle-missing-or-corrupted-data-in-a-dataset">How do
you handle missing or corrupted data in a dataset?</h3>
<p>In terms of data preparation for analysis or modeling, missing data
or corrupted data is pretty common, especially when data are collected
from various sources that are potentially having empty values. Handling
this issue actually depends on the data types, the size of the dataset,
and what type of tasks what we are doing. For example, if the size of
the dataset is large, we could just drop the rows that have null values.
Typically, I use pandas to read the dataset into a data frame object, so
we could just use is null() method to find columns of data with missing
or corrupted data. Then use the dropna() method to drop all the records
with null values or specify which axis we want to drop. When the dataset
is not too large, we probably want to use all of data, so we would
replace the missing values or corrupted values with the mean or median
value of that attribute, for example, the mean height, median price of
an item, etc. In pandas, we can use fillna() methods.</p>
<p>You could find missing/corrupted data in a dataset and either drop
those rows or columns, or decide to replace them with another value.</p>
<p>In Pandas, there are two very useful methods: isnull() and dropna()
that will help you find columns of data with missing or corrupted data
and drop those values. If you want to fill the invalid values with a
placeholder value (for example, 0), you could use the fillna()
method.</p>
<h3 id="do-you-have-experience-with-spark-or-big-data-tools-for-machine-learning">Do
you have experience with Spark or big data tools for machine
learning?</h3>
<p>You’ll want to get familiar with the meaning of big data for
different companies and the different tools they’ll want. Spark is the
big data tool most in demand now, able to handle immense datasets with
speed. Be honest if you don’t have experience with the tools demanded,
but also take a look at job descriptions and see what tools pop up:
you’ll want to invest in familiarizing yourself with them.</p>
<h3 id="pick-an-algorithm.-write-the-pseudocode-for-a-parallel-implementation.">Pick
an algorithm. Write the pseudocode for a parallel implementation.</h3>
<p>This kind of question demonstrates your ability to think in
parallelism and how you could handle concurrency in programming
implementations dealing with big data. Take a look at pseudocode
frameworks such as Peril-L and visualization tools such as Web Sequence
Diagrams to help you demonstrate your ability to write code that
reflects parallelism.</p>
<h3 id="what-are-some-differences-between-a-linked-list-and-an-array">What
are some differences between a linked list and an array?</h3>
<p>An array is an ordered collection of objects. A linked list is a
series of objects with pointers that direct how to process them
sequentially. An array assumes that every element has the same size,
unlike the linked list. A linked list can more easily grow organically:
an array has to be pre-defined or re-defined for organic growth.
Shuffling a linked list involves changing which points direct where —
meanwhile, shuffling an array is more complex and takes more memory.</p>
<h3 id="describe-a-hash-table.">Describe a hash table.</h3>
<p>A hash table is a data structure that produces an associative array.
A key is mapped to certain values through the use of a hash function.
They are often used for tasks such as database indexing.</p>
<h3 id="which-data-visualization-libraries-do-you-use-what-are-your-thoughts-on-the-best-data-visualization-tools">Which
data visualization libraries do you use? What are your thoughts on the
best data visualization tools?</h3>
<p>What’s important here is to define your views on how to properly
visualize data and your personal preferences when it comes to tools.
Popular tools include R’s ggplot, Python’s seaborn and matplotlib, and
tools such as Plot.ly and Tableau.</p>
<h3 id="given-two-strings-a-and-b-of-the-same-length-n-find-whether-it-is-possible-to-cut-both-strings-at-a-common-point-such-that-the-first-part-of-a-and-the-second-part-of-b-form-a-palindrome.">Given
two strings, A and B, of the same length n, find whether it is possible
to cut both strings at a common point such that the first part of A and
the second part of B form a palindrome.</h3>
<h3 id="how-are-primary-and-foreign-keys-related-in-sql">How are primary
and foreign keys related in SQL?</h3>
<p>Most machine learning engineers are going to have to be conversant
with a lot of different data formats. SQL is still one of the key ones
used. Your ability to understand how to manipulate SQL databases will be
something you’ll most likely need to demonstrate. In this example, you
can talk about how foreign keys allow you to match up and join tables
together on the primary key of the corresponding table—but just as
useful is to talk through how you would think about setting up SQL
tables and querying them.</p>
<h3 id="how-does-xml-and-csvs-compare-in-terms-of-size">How does XML and
CSVs compare in terms of size?</h3>
<p>In practice, XML is much more verbose than CSVs are and takes up a
lot more space. CSVs use some separators to categorize and organize data
into neat columns. XML uses tags to delineate a tree-like structure for
key-value pairs. You’ll often get XML back as a way to semi-structure
data from APIs or HTTP responses. In practice, you’ll want to ingest XML
data and try to process it into a usable CSV. This sort of question
tests your familiarity with data wrangling sometimes messy data
formats.</p>
<h3 id="what-are-the-data-types-supported-by-json">What are the data
types supported by JSON?</h3>
<p>This tests your knowledge of JSON, another popular file format that
wraps with JavaScript. There are six basic JSON datatypes you can
manipulate: strings, numbers, objects, arrays, booleans, and null
values.</p>
<h3 id="how-would-you-build-a-data-pipeline">How would you build a data
pipeline?</h3>
<p>Data pipelines are the bread and butter of machine learning
engineers, who take data science models and find ways to automate and
scale them. Make sure you’re familiar with the tools to build data
pipelines (such as Apache Airflow) and the platforms where you can host
models and pipelines (such as Google Cloud or AWS or Azure). Explain the
steps required in a functioning data pipeline and talk through your
actual experience building and scaling them in production.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://towardsdatascience.com/10-minutes-to-building-a-machine-learning-pipeline-with-apache-airflow-53cd09268977">10
Minutes to Building A Machine Learning Pipeline With Apache
Airflow</a></p>
<h2 id="company-and-industry-specific">Company and Industry
Specific</h2>
<p>These machine learning interview questions deal with how to implement
your general machine learning knowledge to a specific company’s
requirements. You’ll be asked to create case studies and extend your
knowledge of the company and industry you’re applying for with your
machine learning skills.</p>
<h3 id="what-do-you-think-is-the-most-valuable-data-in-our-business">What do
you think is the most valuable data in our business?</h3>
<p>This question or questions like it really try to test you on two
dimensions. The first is your knowledge of the business and the industry
itself, as well as your understanding of the business model. The second
is whether you can pick how correlated data is to business outcomes in
general, and then how you apply that thinking to your context about the
company. You’ll want to research the business model and ask good
questions to your recruiter—and start thinking about what business
problems they probably want to solve most with their data.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.accenture.com/no-en/insight-destination-digital-nordic-data-analytics">Three
Recommendations For Making The Most Of Valuable Data</a></p>
<h3 id="how-would-you-implement-a-recommendation-system-for-our-companys-users">How
would you implement a recommendation system for our company’s
users?</h3>
<p>A lot of machine learning interview questions of this type will
involve implementation of machine learning models to a company’s
problems. You’ll have to research the company and its industry in-depth,
especially the revenue drivers the company has, and the types of users
the company takes on in the context of the industry it’s in.</p>
<h3 id="how-can-we-use-your-machine-learning-skills-to-generate-revenue">How
can we use your machine learning skills to generate revenue?</h3>
<p>This is a tricky question. The ideal answer would demonstrate
knowledge of what drives the business and how your skills could relate.
For example, if you were interviewing for music-streaming startup
Spotify, you could remark that your skills at developing a better
recommendation model would increase user retention, which would then
increase revenue in the long run.</p>
<p>The startup metrics Slideshare linked above will help you understand
exactly what performance indicators are important for startups and tech
companies as they think about revenue and growth.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.slideshare.net/dmc500hats/startup-metrics-for-pirates-long-version">Startup
Metrics for Startups (500 Startups)</a></p>
<h3 id="what-do-you-think-of-our-current-data-process">What do you think
of our current data process?</h3>
<p>This kind of question requires you to listen carefully and impart
feedback in a manner that is constructive and insightful. Your
interviewer is trying to gauge if you’d be a valuable member of their
team and whether you grasp the nuances of why certain things are set the
way they are in the company’s data process based on company- or
industry-specific conditions. They’re trying to see if you can be an
intellectual peer. Act accordingly.</p>
<h2 id="general-machine-learning-interest">General Machine Learning
Interest</h2>
<p>This series of machine learning interview questions attempts to gauge
your passion and interest in machine learning. The right answers will
serve as a testament for your commitment to being a lifelong learner in
machine learning.</p>
<h3 id="what-are-the-last-machine-learning-papers-youve-read">What are
the last machine learning papers you’ve read?</h3>
<p>Keeping up with the latest scientific literature on machine learning
is a must if you want to demonstrate interest in a machine learning
position. <a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf">This
overview of deep learning in Nature</a> by the scions of deep learning
themselves (from Hinton to Bengio to LeCun) can be a good reference
paper and an overview of what’s happening in deep learning — and the
kind of paper you might want to cite.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.quora.com/What-are-some-of-the-best-research-papers-books-for-Machine-learning">What
are some of the best research papers/books for machine learning?</a></p>
<h3 id="do-you-have-research-experience-in-machine-learning">Do you have
research experience in machine learning?</h3>
<p>Related to the last point, most organizations hiring for machine
learning positions will look for your formal experience in the field.
Research papers, co-authored or supervised by leaders in the field, can
make the difference between you being hired and not. Make sure you have
a summary of your research experience and papers ready — and an
explanation for your background and lack of formal research experience
if you don’t.</p>
<h3 id="what-are-your-favorite-use-cases-of-machine-learning-models">What
are your favorite use cases of machine learning models?</h3>
<p>The <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.quora.com/What-are-the-typical-use-cases-for-different-machine-learning-algorithms-For-instance-under-what-typical-conditions-would-one-prefer-to-use-one-over-the-other-without-having-tested-the-accuracy-of-learning">Quora
thread here</a> contains some examples, such as decision trees that
categorize people into different tiers of intelligence based on IQ
scores. Make sure that you have a few examples in mind and describe what
resonated with you. It’s important that you demonstrate an interest in
how machine learning is implemented.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.quora.com/What-are-the-typical-use-cases-for-different-machine-learning-algorithms">What
are the typical use cases for different machine learning algorithms?
(Quora)</a></p>
<h3 id="how-would-you-approach-the-netflix-prize-competition">How would
you approach the “Netflix Prize” competition?</h3>
<p>The Netflix Prize was a famed competition where Netflix offered
$1,000,000 for a better collaborative filtering algorithm. The team that
won called BellKor had a 10% improvement and used an ensemble of
different methods to win. Some familiarity with the case and its
solution will help demonstrate you’ve paid attention to machine learning
for a while.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize
(Wikipedia)</a></p>
<h3 id="where-do-you-usually-source-datasets">Where do you usually
source datasets?</h3>
<p>Machine learning interview questions like these try to get at the
heart of your machine learning interest. Somebody who is truly
passionate about machine learning will have gone off and done side
projects on their own, and have a good idea of what great datasets are
out there. If you’re missing any, check out <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.quandl.com/">Quandl</a> for economic and financial
data, and <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.kaggle.com/datasets">Kaggle’s
Datasets</a> collection for another great list.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.springboard.com/blog/free-public-data-sets-data-science-project/">19
Free Public Data Sets For Your First Data Science Project
(Springboard)</a></p>
<h3 id="how-do-you-think-google-is-training-data-for-self-driving-cars">How
do you think Google is training data for self-driving cars?</h3>
<p>Machine learning interview questions like this one really test your
knowledge of different machine learning methods, and your inventiveness
if you don’t know the answer. Google is currently using <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.google.com/recaptcha">recaptcha</a> to source labeled
data on storefronts and traffic signs. They are also building on
training data collected by Sebastian Thrun at GoogleX — some of which
was obtained by his grad students driving buggies on desert dunes!</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://waymo.com/tech/">Waymo Tech</a></p>
<h3 id="how-would-you-simulate-the-approach-alphago-took-to-beat-lee-sidol-at-go">How
would you simulate the approach AlphaGo took to beat Lee Sidol at
Go?</h3>
<p>AlphaGo beating Lee Sidol, the best human player at Go, in a
best-of-five series was a truly seminal event in the history of machine
learning and deep learning. <a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">This
Nature paper</a> describes how this was accomplished with “Monte-Carlo
tree search with deep neural networks that have been trained by
supervised learning, from human expert games, and by reinforcement
learning from games of self-play.”</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Mastering
the game of Go with deep neural networks and tree search
(Nature)</a></p>
<h3 id="what-are-your-thoughts-on-gpt-3-and-openais-model">What are your
thoughts on GPT-3 and OpenAI’s model?</h3>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/openai/gpt-3">GPT-3</a> is a new language
generation model developed by OpenAI. It was marked as exciting because
with very little change in architecture, and a ton more data, GPT-3
could generate what seemed to be human-like conversational pieces, up to
and including novel-size works and the ability to create code from
natural language. There are many perspectives on GPT-3 throughout the
Internet — if it comes up in an interview setting, be prepared to
address this topic (and trending topics like it) intelligently to
demonstrate that you follow the latest advances in machine learning.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/2005.14165">Language Models are
Few-Shot Learners</a></p>
<h3 id="what-models-do-you-train-for-fun-and-what-gpuhardware-do-you-use">What
models do you train for fun, and what GPU/hardware do you use?</h3>
<p>Such machine learning interview questions tests whether you’ve worked
on <a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.springboard.com/blog/machine-learning-projects/">machine
learning projects</a> outside of a corporate role and whether you
understand the basics of how to resource projects and allocate GPU-time
efficiently. Expect questions like this to come from hiring managers
that are interested in getting a greater sense behind your portfolio,
and what you’ve done independently.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://code-love.com/2020/08/08/where-to-get-free-gpu-cloud-hours-for-machine-learning/">Where
to get free GPU cloud hours for machine learning</a></p>
<h3 id="what-are-some-of-your-favorite-apis-to-explore">What are some of
your favorite APIs to explore?</h3>
<p>If you’ve worked with external data sources, it’s likely you’ll have
a few favorite APIs that you’ve gone through. You can be thoughtful here
about the kinds of experiments and pipelines you’ve run in the past,
along with how you think about the APIs you’ve used before.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/TonnyL/Awesome_APIs">Awesome APIs</a></p>
<h4 id="how-do-you-think-quantum-computing-will-affect-machine-learning">How
do you think quantum computing will affect machine learning?</h4>
<p>With the recent announcement of more breakthroughs in quantum
computing, the question of how this new format and way of thinking
through hardware serves as a useful proxy to explain classical computing
and machine learning, and some of the hardware nuances that might make
some algorithms much easier to do on a quantum machine. Demonstrating
some knowledge in this area helps show that you’re interested in machine
learning at a much higher level than just implementation details.</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1611.09347">Quantum Machine
Learning</a></p>
<script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kity@2.0.4/dist/kity.min.js"></script><script type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/kityminder-core@1.4.50/dist/kityminder.core.min.js"></script><script defer="true" type="text&#x2F;javascript" src="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.js"></script><link rel="stylesheet" type="text&#x2F;css" href="https://cdn.jsdelivr.net/npm/hexo-simple-mindmap@0.7.0/dist/mindmap.min.css"></article><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/reading-notes/cqNotes/ML/cq-info-theory/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/reading-notes/default_cover/toucan-4185361_1280.jpg" onerror="onerror=null;src='/reading-notes/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">cq_info_theory</div></div></a></div><div class="next-post pull-right"><a href="/reading-notes/cqNotes/InterviewPrep/cq_data_science_interview/"><img class="next-cover" src= "data:image/gif;base64,R0lGODdhAQABAPAAAMPDwwAAACwAAAAAAQABAAACAkQBADs=" data-lazy-src="/reading-notes/default_cover/toucan-4185361_1280.jpg" onerror="onerror=null;src='/reading-notes/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">cq_data_science_interview</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#general"><span class="toc-number">1.</span> <span class="toc-text">General</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#algorithms-and-theory"><span class="toc-number">1.1.</span> <span class="toc-text">Algorithms and Theory</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bias-variance-tradeoff"><span class="toc-number">1.1.1.</span> <span class="toc-text">Bias-Variance Tradeoff</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#underfitting-and-overfitting"><span class="toc-number">1.1.2.</span> <span class="toc-text">Underfitting and Overfitting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-roc-curve-works"><span class="toc-number">1.1.3.</span> <span class="toc-text">How ROC Curve works</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#precision-and-recall"><span class="toc-number">1.1.4.</span> <span class="toc-text">Precision and Recall</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bayes-theorem"><span class="toc-number">1.1.5.</span> <span class="toc-text">Bayes’ Theorem</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#l1-and-l2-regularization"><span class="toc-number">1.1.6.</span> <span class="toc-text">L1 and L2 Regularization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#type-i-and-type-ii-error"><span class="toc-number">1.1.7.</span> <span class="toc-text">Type-I and Type-II Error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fourier-transform"><span class="toc-number">1.1.8.</span> <span class="toc-text">Fourier transform</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#probability-vs-likelihood"><span class="toc-number">1.1.9.</span> <span class="toc-text">Probability vs Likelihood</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#generative-vs-discriminative-models"><span class="toc-number">1.1.10.</span> <span class="toc-text">Generative VS
Discriminative Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cross-validation-techniques-to-use-on-a-time-series-dataset"><span class="toc-number">1.1.11.</span> <span class="toc-text">Cross
validation techniques to use on a time series dataset</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-is-a-decision-tree-pruned"><span class="toc-number">1.1.12.</span> <span class="toc-text">How is a decision tree
pruned</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#which-is-more-important-model-accuracy-or-model-performance"><span class="toc-number">1.1.13.</span> <span class="toc-text">Which
is more important, model accuracy, or model performance?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#f-1-score-how-would-you-use-it"><span class="toc-number">1.1.14.</span> <span class="toc-text">F-1 score, how would you use
it</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-handle-an-imbalanced-dataset"><span class="toc-number">1.1.15.</span> <span class="toc-text">How would you handle
an imbalanced dataset?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#when-should-you-use-classification-over-regression"><span class="toc-number">1.1.16.</span> <span class="toc-text">When should
you use classification over regression?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#name-an-example-where-ensemble-techniques-might-be-useful."><span class="toc-number">1.1.17.</span> <span class="toc-text">Name
an example where ensemble techniques might be useful.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-you-ensure-youre-not-overfitting-with-a-model"><span class="toc-number">1.1.18.</span> <span class="toc-text">How do you
ensure you’re not overfitting with a model?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-evaluation-approaches-would-you-work-to-gauge-the-effectiveness-of-a-machine-learning-model"><span class="toc-number">1.1.19.</span> <span class="toc-text">What
evaluation approaches would you work to gauge the effectiveness of a
machine learning model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-evaluate-a-logistic-regression-model"><span class="toc-number">1.1.20.</span> <span class="toc-text">How would
you evaluate a logistic regression model?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#whats-the-kernel-trick-and-how-is-it-useful"><span class="toc-number">1.1.21.</span> <span class="toc-text">What’s the “kernel
trick” and how is it useful?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#programming"><span class="toc-number">1.2.</span> <span class="toc-text">Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-you-handle-missing-or-corrupted-data-in-a-dataset"><span class="toc-number">1.2.1.</span> <span class="toc-text">How do
you handle missing or corrupted data in a dataset?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#do-you-have-experience-with-spark-or-big-data-tools-for-machine-learning"><span class="toc-number">1.2.2.</span> <span class="toc-text">Do
you have experience with Spark or big data tools for machine
learning?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pick-an-algorithm.-write-the-pseudocode-for-a-parallel-implementation."><span class="toc-number">1.2.3.</span> <span class="toc-text">Pick
an algorithm. Write the pseudocode for a parallel implementation.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-some-differences-between-a-linked-list-and-an-array"><span class="toc-number">1.2.4.</span> <span class="toc-text">What
are some differences between a linked list and an array?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#describe-a-hash-table."><span class="toc-number">1.2.5.</span> <span class="toc-text">Describe a hash table.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#which-data-visualization-libraries-do-you-use-what-are-your-thoughts-on-the-best-data-visualization-tools"><span class="toc-number">1.2.6.</span> <span class="toc-text">Which
data visualization libraries do you use? What are your thoughts on the
best data visualization tools?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#given-two-strings-a-and-b-of-the-same-length-n-find-whether-it-is-possible-to-cut-both-strings-at-a-common-point-such-that-the-first-part-of-a-and-the-second-part-of-b-form-a-palindrome."><span class="toc-number">1.2.7.</span> <span class="toc-text">Given
two strings, A and B, of the same length n, find whether it is possible
to cut both strings at a common point such that the first part of A and
the second part of B form a palindrome.</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-are-primary-and-foreign-keys-related-in-sql"><span class="toc-number">1.2.8.</span> <span class="toc-text">How are primary
and foreign keys related in SQL?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-does-xml-and-csvs-compare-in-terms-of-size"><span class="toc-number">1.2.9.</span> <span class="toc-text">How does XML and
CSVs compare in terms of size?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-the-data-types-supported-by-json"><span class="toc-number">1.2.10.</span> <span class="toc-text">What are the data
types supported by JSON?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-build-a-data-pipeline"><span class="toc-number">1.2.11.</span> <span class="toc-text">How would you build a data
pipeline?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#company-and-industry-specific"><span class="toc-number">1.3.</span> <span class="toc-text">Company and Industry
Specific</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-do-you-think-is-the-most-valuable-data-in-our-business"><span class="toc-number">1.3.1.</span> <span class="toc-text">What do
you think is the most valuable data in our business?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-implement-a-recommendation-system-for-our-companys-users"><span class="toc-number">1.3.2.</span> <span class="toc-text">How
would you implement a recommendation system for our company’s
users?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-can-we-use-your-machine-learning-skills-to-generate-revenue"><span class="toc-number">1.3.3.</span> <span class="toc-text">How
can we use your machine learning skills to generate revenue?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-do-you-think-of-our-current-data-process"><span class="toc-number">1.3.4.</span> <span class="toc-text">What do you think
of our current data process?</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#general-machine-learning-interest"><span class="toc-number">1.4.</span> <span class="toc-text">General Machine Learning
Interest</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-the-last-machine-learning-papers-youve-read"><span class="toc-number">1.4.1.</span> <span class="toc-text">What are
the last machine learning papers you’ve read?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#do-you-have-research-experience-in-machine-learning"><span class="toc-number">1.4.2.</span> <span class="toc-text">Do you have
research experience in machine learning?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-your-favorite-use-cases-of-machine-learning-models"><span class="toc-number">1.4.3.</span> <span class="toc-text">What
are your favorite use cases of machine learning models?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-approach-the-netflix-prize-competition"><span class="toc-number">1.4.4.</span> <span class="toc-text">How would
you approach the “Netflix Prize” competition?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#where-do-you-usually-source-datasets"><span class="toc-number">1.4.5.</span> <span class="toc-text">Where do you usually
source datasets?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-do-you-think-google-is-training-data-for-self-driving-cars"><span class="toc-number">1.4.6.</span> <span class="toc-text">How
do you think Google is training data for self-driving cars?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#how-would-you-simulate-the-approach-alphago-took-to-beat-lee-sidol-at-go"><span class="toc-number">1.4.7.</span> <span class="toc-text">How
would you simulate the approach AlphaGo took to beat Lee Sidol at
Go?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-your-thoughts-on-gpt-3-and-openais-model"><span class="toc-number">1.4.8.</span> <span class="toc-text">What are your
thoughts on GPT-3 and OpenAI’s model?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-models-do-you-train-for-fun-and-what-gpuhardware-do-you-use"><span class="toc-number">1.4.9.</span> <span class="toc-text">What
models do you train for fun, and what GPU&#x2F;hardware do you use?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#what-are-some-of-your-favorite-apis-to-explore"><span class="toc-number">1.4.10.</span> <span class="toc-text">What are some of
your favorite APIs to explore?</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#how-do-you-think-quantum-computing-will-affect-machine-learning"><span class="toc-number">1.4.10.1.</span> <span class="toc-text">How
do you think quantum computing will affect machine learning?</span></a></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="Increase font size"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="Decrease font size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/reading-notes/js/utils.js"></script><script src="/reading-notes/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/reading-notes/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'default',
      })
      false && mermaid.init()
    })
  }
}</script></div></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>