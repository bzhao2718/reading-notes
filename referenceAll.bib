
@article{agarwal2020,
  title = {Neural Additive Models: {{Interpretable}} Machine Learning with Neural Nets},
  author = {Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E},
  year = {2020},
  abstract = {Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of …},
  archiveprefix = {arXiv},
  eprint = {2004.13912},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2004.13912},
  note = {Times cited: 22},
  type = {Journal Article}
}


@article{al-maleh2020,
	title = {Arabic Text Summarization Using Deep Learning Approach},
	author = {Al-Maleh, Molham and Desouki, Said},
	date = {2020-12-11},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	volume = {7},
	pages = {109},
	issn = {2196-1115},
	doi = {10.1186/s40537-020-00386-7},
	url = {https://doi.org/10.1186/s40537-020-00386-7},
	abstract = {Natural language processing has witnessed remarkable progress with the advent of deep learning techniques. Text summarization, along other tasks like text translation and sentiment analysis, used deep neural network models to enhance results. The new methods of text summarization are subject to a sequence-to-sequence framework of encoder\textendash decoder model, which is composed of neural networks trained jointly on both input and output. Deep neural networks take advantage of big datasets to improve their results. These networks are supported by the attention mechanism, which can deal with long texts more efficiently by identifying focus points in the text. They are also supported by the copy mechanism that allows the model to copy words from the source to the summary directly. In this research, we are re-implementing the basic summarization model that applies the sequence-to-sequence framework on the Arabic language, which has not witnessed the employment of this model in the text summarization before. Initially, we build an Arabic data set of summarized article headlines. This data set consists of approximately 300 thousand entries, each consisting of an article introduction and the headline corresponding to this introduction. We then apply baseline summarization models to the previous data set and compare the results using the ROUGE scale.},
	number = {1}
}

@article{alguliev2013,
  title = {Multiple Documents Summarization Based on Evolutionary Optimization Algorithm},
  author = {Alguliev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R.},
  year = {2013},
  volume = {40},
  pages = {1675--1689},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.eswa.2012.09.014},
  journal = {Expert Systems with Applications},
  note = {Times cited: 46},
  number = {5},
  type = {Journal Article}
}

@article{allahyari2017,
  title = {Text {{Summarization Techniques}}: {{A Brief Survey}}},
  author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
  year = {2017},
  pages = {1707.02268v3},
  abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
  journal = {arXiv},
  note = {Some of references format have updated},
  type = {Journal Article}
}

@article{allen2019,
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  author = {Allen, Carl and Hospedales, Timothy},
  year = {2019},
  pages = {1901.09813v2},
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “\$w\_x\$ is to \$w\_y\$”. From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{anchieta2019,
  title = {{{SEMA}}: An {{Extended Semantic Evaluation Metric}} for {{AMR}}},
  author = {Anchieta, Rafael T. and Cabezudo, Marco A. S. and Pardo, Thiago A. S.},
  year = {2019},
  pages = {1905.12069v1},
  abstract = {Abstract Meaning Representation (AMR) is a recently designed semantic representation language intended to capture the meaning of a sentence, which may be represented as a single-rooted directed acyclic graph with labeled nodes and edges. The automatic evaluation of this structure plays an important role in the development of better systems, as well as for semantic annotation. Despite there is one available metric, smatch, it has some drawbacks. For instance, smatch creates a self-relation on the root of the graph, has weights for different error types, and does not take into account the dependence of the elements in the AMR structure. With these drawbacks, smatch masks several problems of the AMR parsers and distorts the evaluation of the AMRs. In view of this, in this paper, we introduce an extended metric to evaluate AMR parsers, which deals with the drawbacks of the smatch metric. Finally, we compare both metrics, using four well-known AMR parsers, and we argue that our metric is more refined, robust, fairer, and faster than smatch.},
  journal = {arXiv},
  note = {Accepted by CICLing 2019},
  type = {Journal Article}
}

@misc{arvindpdmn2020,
  title = {Text Summarization},
  author = {, Arvindpdmn},
  year = {2020},
  month = feb,
  howpublished = {https://devopedia.org/text-summarization},
  language = {british}
}

@book{aston2020,
  title = {Dive into {{Deep Learning}}},
  author = {Aston, Zhang and Zachary, C. Lipton and Mu, Li and Alexander, J. Smola},
  year = {2020}
}

@book{b.2014,
  title = {{{NICE}}: {{Network}}-Aware {{VM Consolidation}} Scheme for {{Energy Conservation}} in {{Data Centers}}},
  author = {B., Cao and X., Gao and G., Chen and Y., Jin},
  year = {2014},
  month = dec,
  abstract = {Energy conservation and network performance have become two of the most important issues in data center as the scale of cloud services continues growing. Recent researches usually consider these two issues separately. Energy conservation mainly deals with hosts, which reduces total energy consumption by consolidating virtual machine(VM)s to fewer hosts, and network performance mainly deals with network scalability and energy efficiency, which improves data center network(DCN) scalability by applying new network topologies or routing schemes and improves DCN energy efficiency by consolidating trafile. In this paper, we jointly consider these two issues and define Combined VM Consolidation (CVC) problem. We prove that CVC is NP-complete and is inapproximable by a factor of 3/2 ε unless P = NP. Next, we propose NICE: Network-aware VM Consolidation scheme for Energy Conservation in Data CEnter to solve CVC. Instead of taking the unrealistic hypothesis that migration cost is negligible, a common assumption in most literatures, we precisely analyze VM migration cost according to real-trace experiments in a 6-server testbed via VMware. Massive simulations validate the efficiency of NICE, In all, to the best of our knowledge, we arc the first work to combine VM consolidation with network optimization and migration cost.},
  keywords = {cloud computing,cloud service,combined VM consolidation,computational complexity,computer centres,CVC,data center,data center network,DCN,energy saving,migration cost,network optimization,network-aware VM consolidation scheme for energy conservation in data centers,NICE,Nonvolatile memory,NP-complete problem,optimisation,power aware computing,Silicon,virtual machine,virtual machines,VM consolidation,VM migration cost}
}

@article{bahdanau2014,
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance …},
  archiveprefix = {arXiv},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1409.0473},
  note = {Times cited: 14895},
  type = {Journal Article}
}

@article{bamler2017,
  title = {Dynamic {{Word Embeddings}}},
  author = {Bamler, Robert and Mandt, Stephan},
  year = {2017},
  pages = {1702.08359v2},
  abstract = {We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms–skip-gram smoothing and skip-gram filtering–that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.},
  journal = {arXiv},
  note = {In the proceedings of the International Conference on Machine Learning (ICML 2017); 8 pages + references and supplement},
  type = {Journal Article}
}

@book{banarescu2013,
  title = {Abstract Meaning Representation for Sembanking},
  author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
  year = {2013},
  volume = {Proceedings of the 7th linguistic annotation workshop and interoperability with discourse},
  abstract = {Abstract We describe Abstract Meaning Representation (AMR), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of AMR and tools associated with it.‏}
}

@article{barros2019,
  title = {{{NATSUM}}: {{Narrative}} Abstractive Summarization through Cross-Document Timeline Generation},
  author = {Barros, Cristina and Lloret, Elena and Saquete, Estela and {Navarro-Colorado}, Borja},
  year = {2019},
  volume = {56},
  pages = {1775--1793},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.ipm.2019.02.010},
  journal = {Information Processing \& Management},
  note = {Times cited: 7},
  number = {5},
  type = {Journal Article}
}

@article{barzilay2005,
  title = {Sentence Fusion for Multidocument News Summarization},
  author = {Barzilay, Regina and McKeown, Kathleen R},
  year = {2005},
  volume = {31},
  pages = {297--328},
  publisher = {{MIT Press}},
  abstract = {A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence …},
  isbn = {0891-2017},
  journal = {Computational Linguistics},
  note = {Times cited: 441},
  number = {3},
  type = {Journal Article}
}

@article{bauer1999,
  title = {An {{Empirical Comparison}} of {{Voting Classification Algorithms}}: {{Bagging}}, {{Boosting}}, and {{Variants}}},
  author = {Bauer, Eric and Kohavi, Ron},
  year = {1999},
  volume = {36},
  pages = {105--139},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1007515423169},
  journal = {Machine Learning},
  note = {Times cited: 1328},
  number = {1/2},
  type = {Journal Article}
}

@phdthesis{bayer2015,
  title = {Learning Sequence Representations},
  author = {Bayer, Justin Simon},
  year = {2015},
  abstract = {This work contributes to learning representations of data with Neural Networks (NNs), and RNNs in particular, in three ways. First, we will show how NNs can be augmented with additional calculations to allow the propagation of not only points, but random variables summarised by their expectation and variance through an NN. This generalises Fast Dropout (FD), which we show to be an outstanding regularisation method for RNNs. It further allows us to obtain approximations of the marginal likelihood and the predictive distribution of NNs, which we will use to implement Variational Bayes (VB) and related methods for the estimation of parameters. Second, we will introduce the framework of sequence reduction. It consists of using RNNs in conjunction with pooling operators to reduce sequences of arbitrary length to fixed-length points, enabling further analysis. Third, we will leverage advances in Variational Inference (VI) to learn latent state representations of sequences. These are obtained by stochastic Recurrent Networks (STORNs), where a standard RNN is augmented with stochastic units, making it able to represent arbitrarily complex distributions. The model is trained by means of Stochastic Gradient Variational Bayes (SGVB), making it probabilistic and paving the way for applications such as denoising, missing value imputation, synthesis and more.},
  school = {Technische Universität München},
  type = {{{PhD Thesis}}}
}

@article{belinkov2019,
  title = {Analysis Methods in Neural Language Processing: {{A}} Survey},
  author = {Belinkov, Yonatan and Glass, James},
  year = {2019},
  volume = {7},
  pages = {49--72},
  publisher = {{MIT Press}},
  isbn = {2307-387X},
  journal = {Transactions of the Association for Computational Linguistics},
  type = {Journal Article}
}

@article{beltagy2020,
  title = {Longformer: {{The}} Long-Document Transformer},
  author = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  year = {2020},
  abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence …},
  archiveprefix = {arXiv},
  eprint = {2004.05150},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2004.05150},
  note = {Times cited: 62},
  type = {Journal Article}
}

@article{bengio1994,
  title = {Learning Long-Term Dependencies with Gradient Descent Is Difficult},
  author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  year = {1994},
  volume = {5},
  pages = {157--166},
  publisher = {{IEEE}},
  doi = {10.1109/72.279181},
  abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between …},
  isbn = {1045-9227},
  journal = {IEEE transactions on neural networks},
  note = {Times cited: 5746},
  number = {2},
  type = {Journal Article}
}

@book{bengio2009,
  title = {Learning Deep Architectures for {{AI}}},
  author = {Bengio, Yoshua},
  year = {2009},
  publisher = {{Now Publishers Inc}},
  abstract = {Can machine learning deliver AI? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (eg in vision, language, and other AI-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional …}
}

@article{bengio2012,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2012},
  pages = {1206.5538v3},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
  journal = {arXiv},
  type = {Journal Article}
}

@book{benikova2016,
  title = {Bridging the Gap between Extractive and Abstractive Summaries: {{Creation}} and Evaluation of Coherent Extracts from Heterogeneous Sources},
  author = {Benikova, Darina and Mieskes, Margot and Meyer, Christian M and Gurevych, Iryna},
  year = {2016},
  volume = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
  abstract = {Coherent extracts are a novel type of summary combining the advantages of manually created abstractive summaries, which are fluent but difficult to evaluate, and low-quality automatically created extractive summaries, which lack coherence and structure. We use a corpus of heterogeneous documents to address the issue that information seekers usually face–a variety of different types of information sources. We directly extract information from these, but minimally redact and meaningfully order it to form a coherent text. Our qualitative …}
}

@article{berger1996,
  title = {A Maximum Entropy Approach to Natural Language Processing},
  author = {Berger, Adam and Della Pietra, Stephen A and Della Pietra, Vincent J},
  year = {1996},
  volume = {22},
  pages = {39--71},
  abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach …},
  journal = {Computational linguistics},
  note = {Times cited: 4276},
  number = {1},
  type = {Journal Article}
}

@article{bhandari2020,
  title = {Metrics Also {{Disagree}} in the {{Low Scoring Range}}: {{Revisiting Summarization Evaluation Metrics}}},
  author = {Bhandari, Manik and Gour, Pranav and Ashfaq, Atabak and Liu, Pengfei},
  year = {2020},
  pages = {2011.04096v1},
  abstract = {In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.},
  journal = {arXiv},
  note = {Accepted at COLING 2020},
  type = {Journal Article}
}

@article{bhandari2020a,
  title = {Re-Evaluating {{Evaluation}} in {{Text Summarization}}},
  author = {Bhandari, Manik and Gour, Pranav and Ashfaq, Atabak and Liu, Pengfei and Neubig, Graham},
  year = {2020},
  pages = {2010.07100v1},
  abstract = {Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.},
  journal = {arXiv},
  note = {Accepted at EMNLP 2020},
  type = {Journal Article}
}

@article{bi2019,
  title = {Incorporating {{External Knowledge}} into {{Machine Reading}} for {{Generative Question Answering}}},
  author = {Bi, Bin and Wu, Chen and Yan, Ming and Wang, Wei and Xia, Jiangnan and Li, Chenliang},
  year = {2019},
  pages = {1909.02745v1},
  abstract = {Commonsense and background knowledge is required for a QA model to answer many nontrivial questions. Different from existing work on knowledge-aware QA, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator (KEAG), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, KEAG adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that KEAG improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.},
  journal = {arXiv},
  note = {Accepted at EMNLP 2019},
  type = {Journal Article}
}

@article{bitzer2012,
  title = {Recognizing Recurrent Neural Networks ({{rRNN}}): {{Bayesian}} Inference for Recurrent Neural Networks.},
  author = {Bitzer, S and Kiebel, SJ},
  year = {2012},
  volume = {106},
  pages = {201--217},
  address = {{MPI for Human Cognitive and Brain Sciences, Stephanstr. 1a, 04107, Leipzig, Germany. bitzer@cbs.mpg.de}},
  doi = {10.1007/s00422-012-0490-x},
  abstract = {Recurrent neural networks (RNNs) are widely used in computational neuroscience and machine learning applications. In an RNN, each neuron computes its output as a nonlinear function of its integrated input. While the importance of RNNs, especially as models of brain processing, is undisputed, it is also widely acknowledged that the computations in standard RNN models may be an over-simplification of what real neuronal networks compute. Here, we suggest that the RNN approach may be made computationally more powerful by its fusion with Bayesian inference techniques for nonlinear dynamical systems. In this scheme, we use an RNN as a generative model of dynamic input caused by the environment, e.g. of speech or kinematics. Given this generative RNN model, we derive Bayesian update equations that can decode its output. Critically, these updates define a ‘recognizing RNN’ (rRNN), in which neurons compute and exchange prediction and prediction error messages. The rRNN has several desirable features that a conventional RNN does not have, e.g. fast decoding of dynamic stimuli and robustness to initial conditions and noise. Furthermore, it implements a predictive coding scheme for dynamic inputs. We suggest that the Bayesian inversion of RNNs may be useful both as a model of brain function and as a machine learning tool. We illustrate the use of the rRNN by an application to the online decoding (i.e. recognition) of human kinematics.},
  journal = {Biol Cybern},
  number = {4-5},
  type = {Journal Article}
}

@article{bohm2019,
  title = {Better Rewards Yield Better Summaries: {{Learning}} to Summarise without References},
  author = {Böhm, Florian and Gao, Yang and Meyer, Christian M and Shapira, Ori and Dagan, Ido and Gurevych, Iryna},
  year = {2019},
  abstract = {Reinforcement Learning (RL) based document summarisation systems yield state-of-the-art performance in terms of ROUGE scores, because they directly use ROUGE as the rewards during training. However, summaries with high ROUGE scores often receive low human judgement. To find a better reward function that can guide RL to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can …},
  archiveprefix = {arXiv},
  eprint = {1909.01214},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1909.01214},
  note = {Times cited: 18},
  type = {Journal Article}
}

@article{bos2019,
  title = {Separating {{Argument Structure}} from {{Logical Structure}} in {{AMR}}},
  author = {Bos, Johan},
  year = {2019},
  pages = {1908.01355v2},
  abstract = {The AMR (Abstract Meaning Representation) formalism for representing meaning of natural language sentences was not designed to deal with scope and quantifiers. By extending AMR with indices for contexts and formulating constraints on these contexts, a formalism is derived that makes correct prediction for inferences involving negation and bound variables. The attractive core predicate-argument structure of AMR is preserved. The resulting framework is similar to that of Discourse Representation Theory.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{bottou2016,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  year = {2016},
  pages = {1606.04838v3},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{boutkan2019,
  title = {Point-Less: {{More Abstractive Summarization}} with {{Pointer}}-{{Generator Networks}}},
  author = {Boutkan, Freek and Ranzijn, Jorn and Rau, David and van der Wel, Eelco},
  year = {2019},
  pages = {1905.01975v1},
  abstract = {The Pointer-Generator architecture has shown to be a big improvement for abstractive summarization seq2seq models. However, the summaries produced by this model are largely extractive as over 30\% of the generated sentences are copied from the source text. This work proposes a multihead attention mechanism, pointer dropout, and two new loss functions to promote more abstractive summaries while maintaining similar ROUGE scores. Both the multihead attention and dropout do not improve N-gram novelty, however, the dropout acts as a regularizer which improves the ROUGE score. The new loss function achieves significantly higher novel N-grams and sentences, at the cost of a slightly lower ROUGE score.},
  journal = {arXiv},
  note = {7 pages},
  type = {Journal Article}
}

@book{breiman1984,
  title = {Classification and {{Regression Trees}}},
  author = {Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
  year = {1984},
  publisher = {{Taylor \& Francis}}
}

@article{breiman1996,
  title = {Bagging Predictors},
  author = {Breiman, Leo},
  year = {1996},
  volume = {24},
  pages = {123--140},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf00058655},
  journal = {Machine Learning},
  note = {Times cited: 8399},
  number = {2},
  type = {Journal Article}
}

@article{breiman1997,
  title = {Arcing the Edge},
  author = {Breiman, Leo},
  year = {1997},
  publisher = {{Technical Report 486, Statistics Department, University of California at …}},
  abstract = {Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman [1996] …},
  note = {Times cited: 349},
  type = {Journal Article}
}

@article{breiman1999,
  title = {Pasting Small Votes for Classification in Large Databases and On-Line},
  author = {Breiman, Leo},
  year = {1999},
  volume = {36},
  pages = {85--103},
  publisher = {{Springer}},
  abstract = {Many databases have grown to the point where they cannot fit into the fast memory of even large memory machines, to say nothing of current workstations. If what we want to do is to use these data bases to construct predictions of various characteristics, then since the usual methods require that all data be held in fast memory, various work-arounds have to be used. This paper studies one such class of methods which give accuracy comparable to that which could have been obtained if all data could have been held in core and which are …},
  isbn = {1573-0565},
  journal = {Machine learning},
  note = {Times cited: 307},
  number = {1},
  type = {Journal Article}
}

@article{britz2017,
  title = {Massive Exploration of Neural Machine Translation Architectures},
  author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  year = {2017},
  abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and …},
  archiveprefix = {arXiv},
  eprint = {1703.03906},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1703.03906},
  note = {Times cited: 343},
  type = {Journal Article}
}

@article{cachola2020,
  title = {{{TLDR}}: {{Extreme Summarization}} of {{Scientific Documents}}},
  author = {Cachola, I and Lo, K and Cohan, A and Weld, DS},
  year = {2020},
  abstract = {We introduce TLDR generation for scientific papers, a new automatic summarization task with high source compression requiring expert background knowledge and complex language understanding. To facilitate research on this task, we introduce SciTLDR, a …},
  archiveprefix = {arXiv},
  eprint = {2004.15011},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2004.15011},
  note = {Times cited: 2},
  type = {Journal Article}
}

@book{calheiros2014,
  title = {Energy-{{Efficient Scheduling}} of {{Urgent Bag}}-of-{{Tasks Applications}} in {{Clouds}} through {{DVFS}}},
  author = {Calheiros, Rodrigo N. and Buyya, Rajkumar},
  year = {2014},
  publisher = {{IEEE}}
}

@article{camacho-collados2018,
  title = {From {{Word}} to {{Sense Embeddings}}: {{A Survey}} on {{Vector Representations}} of {{Meaning}}},
  author = {{Camacho-Collados}, Jose and Pilehvar, Mohammad Taher},
  year = {2018},
  pages = {1805.04032v3},
  abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
  journal = {arXiv},
  note = {46 pages, 8 figures. Published in Journal of Artificial Intelligence Research},
  type = {Journal Article}
}

@article{cambria2014,
  title = {Jumping {{NLP Curves}}: {{A Review}} of {{Natural Language Processing Research}} [{{Review Article}}]},
  author = {Cambria, Erik and White, Bebo},
  year = {2014},
  volume = {9},
  pages = {48--57},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/mci.2014.2307227},
  journal = {IEEE Computational Intelligence Magazine},
  note = {Times cited: 317},
  number = {2},
  type = {Journal Article}
}

@article{cao2017,
  title = {Faithful to the Original: {{Fact}} Aware Neural Abstractive Summarization},
  author = {Cao, Ziqiang and Wei, Furu and Li, Wenjie and Li, Sujian},
  year = {2017},
  abstract = {Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30\% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we …},
  archiveprefix = {arXiv},
  eprint = {1711.04434},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1711.04434},
  type = {Journal Article}
}

@article{celikyilmaz2018,
  title = {Deep {{Communicating Agents}} for {{Abstractive Summarization}}},
  author = {Celikyilmaz, Asli and Bosselut, Antoine and He, Xiaodong and Choi, Yejin},
  year = {2018},
  pages = {1803.10357v3},
  abstract = {We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.},
  journal = {arXiv},
  note = {Accepted for publication at NAACL 2018},
  type = {Journal Article}
}

@article{celikyilmaz2020,
  title = {Evaluation of {{Text Generation}}: {{A Survey}}},
  author = {Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
  year = {2020},
  pages = {2006.14799v1},
  abstract = {The paper surveys evaluation methods of natural language generation (NLG) systems that have been developed in the last few years. We group NLG evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed NLG tasks and neural NLG models. We then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.},
  journal = {arXiv},
  note = {42 pages},
  type = {Journal Article}
}

@book{chaganty2018,
  title = {The Price of Debiasing Automatic Metrics in Natural Language Evalaution},
  author = {Chaganty, Arun and Mussmann, Stephen and Liang, Percy},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@article{chandrasekaran2020,
  title = {Evolution of {{Semantic Similarity}} – {{A Survey}}},
  author = {Chandrasekaran, Dhivya and Mago, Vijay},
  year = {2020},
  pages = {2004.13820v1},
  abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing (NLP). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. In order to address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place, for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
  journal = {arXiv},
  note = {29 pages, 5 figures, submitted to “ACM Computing Survey”},
  type = {Journal Article}
}

@misc{chauhan2019,
  title = {Unsupervised Text Summarization Using Sentence Embeddings},
  author = {Chauhan, Kushal},
  year = {2019},
  month = jan,
  howpublished = {https://medium.com/jatana/unsupervised-text-summarization-using-sentence-embeddings-adb15ce83db1},
  language = {English}
}

@article{chawla2019,
  title = {Improving Generation Quality of Pointer Networks via Guided Attention},
  author = {Chawla, K and Krishna, K and Srinivasan, BV},
  year = {2019},
  abstract = {Pointer generator networks have been used successfully for abstractive summarization. Along with the capability to generate novel words, it also allows the model to copy from the input text to handle out-of-vocabulary words. In this paper, we point out two key shortcomings of the summaries generated with this framework via manual inspection, statistical analysis and human evaluation. The first shortcoming is the extractive nature of the generated summaries, since the network eventually learns to copy from the input article most …},
  archiveprefix = {arXiv},
  eprint = {1901.11492},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1901.11492},
  note = {Times cited: 1},
  type = {Journal Article}
}

@article{chen2016,
  title = {A {{Thorough Examination}} of the {{CNN}}/{{Daily Mail Reading Comprehension Task}}},
  author = {Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
  year = {2016},
  pages = {1606.02858v2},
  abstract = {Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6\% and 76.6\% on these two datasets, exceeding current state-of-the-art results by 7-10\% and approaching what we believe is the ceiling for performance on this task.},
  journal = {arXiv},
  note = {ACL 2016, updated results},
  type = {Journal Article}
}

@article{chen2017,
  title = {Neural {{Natural Language Inference Models Enhanced}} with {{External Knowledge}}},
  author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
  year = {2017},
  volume = {cs.CL},
  doi = {10.18653/v1/P18-1224},
  abstract = {Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.},
  note = {Accepted by ACL 2018},
  type = {Journal Article}
}

@article{chen2018,
  title = {Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting},
  author = {Chen, Yen-Chun and Bansal, Mohit},
  year = {2018},
  abstract = {Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (ie, compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) …},
  archiveprefix = {arXiv},
  eprint = {1805.11080},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1805.11080},
  note = {Times cited: 212},
  type = {Journal Article}
}

@article{chen2020,
  title = {{{MOCHA}}: {{A Dataset}} for {{Training}} and {{Evaluating Generative Reading Comprehension Metrics}}},
  author = {Chen, Anthony and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
  year = {2020},
  pages = {2010.03636v2},
  abstract = {Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: MOdeling Correctness with Human Annotations. MOCHA contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using MOCHA, we train a Learned Evaluation metric for Reading Comprehension, LERC, to mimic human judgement scores. LERC outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, LERC achieves 80\% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. MOCHA presents a challenging problem for developing accurate and robust generative reading comprehension metrics.},
  journal = {arXiv},
  type = {Journal Article}
}

@book{chin-yew2004,
  title = {Looking for a {{Few Good Metrics}}: {{Automatic Summarization Evaluation}} - {{How Many Samples Are Enough}}? {{NTCIR}}},
  author = {{Chin-Yew}, Lin},
  year = {2004}
}

@article{cho2014,
  title = {Learning {{Phrase Representations}} Using {{RNN Encoder}}-{{Decoder}} for {{Statistical Machine Translation}}},
  author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  year = {2014},
  pages = {1406.1078v3},
  abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  journal = {arXiv},
  note = {EMNLP 2014},
  type = {Journal Article}
}

@article{cho2014a,
  title = {On the Properties of Neural Machine Translation: {{Encoder}}-Decoder Approaches},
  author = {Cho, Kyunghyun and Van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  abstract = {… A number of recent papers have proposed to use neural networks to directly learnthe condi- tional distribution from a bilingual, parallel cor- pus (Kalchbrenner andBlunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). For …},
  archiveprefix = {arXiv},
  eprint = {1409.1259},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1409.1259},
  note = {Times cited: 3088},
  type = {Journal Article}
}

@article{cho2015,
  title = {Natural {{Language Understanding}} with {{Distributed Representation}}},
  author = {Cho, Kyunghyun},
  year = {2015},
  volume = {cs.CL},
  abstract = {This is a lecture note for the course DS-GA 3001 {$<$}Natural Language Understanding with Distributed Representation{$>$} at the Center for Data Science, New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.},
  type = {Journal Article}
}

@book{choi2019,
  title = {{{VAE}}-{{PGN}} Based {{Abstractive Model}} in {{Multi}}-Stage {{Architecture}} for {{Text Summarization}}},
  author = {Choi, Hyungtak and Ravuru, Lohith and Dryjanski, Tomasz and Rye, Sunghan and Lee, Donghyun and Lee, Hojung and Hwang, Inchul},
  year = {2019},
  volume = {Proceedings of the 12th International Conference on Natural Language Generation},
  abstract = {This paper describes our submission to the TL; DR challenge. Neural abstractive summarization models have been successful in generating fluent and consistent summaries with advancements like the copy (Pointer-generator) and coverage mechanisms. However, these models suffer from their extractive nature as they learn to copy words from the source text. In this paper, we propose a novel abstractive model based on Variational Autoencoder (VAE) to address this issue. We also propose a Unified Summarization Framework for the …}
}

@article{chollet2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, François},
  year = {2019},
  pages = {1911.01547v2},
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to “buy” arbitrary levels of skills for a system, in a way that masks the system’s own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  journal = {arXiv},
  type = {Journal Article}
}

@book{chopra2016,
  title = {Abstractive Sentence Summarization with Attentive Recurrent Neural Networks},
  author = {Chopra, Sumit and Auli, Michael and Rush, Alexander M},
  year = {2016},
  volume = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  abstract = {Abstract Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our …}
}

@article{chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  volume = {cs.NE},
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  note = {Presented in NIPS 2014 Deep Learning and Representation Learning Workshop},
  type = {Journal Article}
}

@book{clark2019,
  title = {Sentence {{Mover}}’s {{Similarity}}: {{Automatic Evaluation}} for {{Multi}}-{{Sentence Texts}}},
  author = {Clark, Elizabeth and Celikyilmaz, Asli and Smith, Noah A.},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@article{cohan2018,
  title = {A {{Discourse}}-{{Aware Attention Model}} for {{Abstractive Summarization}} of {{Long Documents}}},
  author = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
  year = {2018},
  pages = {1804.05685v2},
  abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.},
  journal = {arXiv},
  note = {NAACL HLT 2018},
  type = {Journal Article}
}

@book{conroy2008,
  title = {Mind the {{Gap Dangers}} of {{Divorcing Evaluations}} of {{Summary Content}} from {{Linguistic Quality}}},
  author = {Conroy, John M. and Dang, Hoa Trang},
  year = {2008},
  publisher = {{Association for Computational Linguistics}},
  address = {{Morristown, NJ, USA}}
}

@book{d.1986,
  title = {Learning Internal Representations by Error Propagation},
  author = {D., Rumelhart and Geoffrey, E. Hinton and R., J. Williams},
  year = {1986}
}

@book{dang2005,
  title = {Overview of {{DUC}} 2005},
  author = {Dang, Hoa Trang},
  year = {2005}
}

@article{david1992,
  title = {Stacked Generalization},
  author = {David, H. Wolpert},
  year = {1992},
  volume = {5},
  pages = {241--259},
  doi = {10.1016/S0893-6080(05)80023-1},
  abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation’s crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
  isbn = {0893-6080},
  journal = {Neural Networks},
  keywords = {Combining generalizers,cross-validation,Error estimation and correction,Generalization and induction,Learning set preprocessing},
  number = {2},
  type = {Journal Article}
}

@article{day2005,
  title = {A Knowledge-Based Approach to Citation Extraction},
  author = {Day, MY and Tsai, TH and Sung, CL and Lee…, CW},
  year = {2005},
  abstract = {Integration of the bibliographical information of scholarly publications available on the Internet is an important task in academic research. To accomplish this task, accurate reference metadata extraction for scholarly publications is essential for the integration of …},
  journal = {IRI-2005 IEEE …},
  note = {Times cited: 57},
  type = {Journal Article}
}

@book{delcorro2013,
  title = {Clausie: Clause-Based Open Information Extraction},
  author = {Del Corro, Luciano and Gemulla, Rainer},
  year = {2013},
  volume = {Proceedings of the 22nd international conference on World Wide Web},
  abstract = {We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of“useful’’pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of …}
}

@book{dernoncourt2018,
  title = {A Repository of Corpora for Summarization},
  author = {Dernoncourt, Franck and Ghassemi, Mohammad and Chang, Walter},
  year = {2018},
  volume = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  abstract = {Summarization corpora are numerous but fragmented, making it challenging for researchers to efficiently pinpoint corpora most suited to a given summarization task. In this paper, we introduce a repository containing corpora available to train and evaluate automatic summarization systems. We also present an overview of the main corpora with respect to the different summarization tasks, and identify various corpus parameters that researchers may want to consider when choosing a corpus. Lastly, as the recent successes of artificial neural …}
}

@article{deutsch2020,
  title = {Understanding the {{Extent}} to Which {{Summarization Evaluation Metrics Measure}} the {{Information Quality}} of {{Summaries}}},
  author = {Deutsch, Daniel and Roth, Dan},
  year = {2020},
  pages = {2010.12495v1},
  abstract = {Reference-based metrics such as ROUGE or BERTScore evaluate the content quality of a summary by comparing the summary to a reference. Ideally, this comparison should measure the summary’s information quality by calculating how much information the summaries have in common. In this work, we analyze the token alignments used by ROUGE and BERTScore to compare summaries and argue that their scores largely cannot be interpreted as measuring information overlap, but rather the extent to which they discuss the same topics. Further, we provide evidence that this result holds true for many other summarization evaluation metrics. The consequence of this result is that it means the summarization community has not yet found a reliable automatic metric that aligns with its research goal, to generate summaries with high-quality information. Then, we propose a simple and interpretable method of evaluating summaries which does directly measure information overlap and demonstrate how it can be used to gain insights into model behavior that could not be provided by other methods alone.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{devlin2018,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  pages = {1810.04805v2},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  journal = {arXiv},
  type = {Journal Article}
}

@article{dinan2018,
  title = {Wizard of {{Wikipedia}}: {{Knowledge}}-{{Powered Conversational}} Agents},
  author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
  year = {2018},
  pages = {1811.01241v2},
  abstract = {In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically “generate and hope” generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{dipietro2017,
  title = {Analyzing and {{Exploiting NARX Recurrent Neural Networks}} for {{Long}}-{{Term Dependencies}}},
  author = {DiPietro, Robert and Rupprecht, Christian and Navab, Nassir and Hager, Gregory D.},
  year = {2017},
  pages = {1702.07805v4},
  abstract = {Recurrent neural networks (RNNs) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training RNNs to capture long-term dependencies remains difficult. To date, the vast majority of successful RNN architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory (LSTM). We take an orthogonal approach and introduce MIST RNNs, a NARX RNN architecture that allows direct connections from the very distant past. We show that MIST RNNs 1) exhibit superior vanishing-gradient properties in comparison to LSTM and previously-proposed NARX RNNs; 2) are far more efficient than previously-proposed NARX RNN architectures, requiring even fewer computations than LSTM; and 3) improve performance substantially over LSTM and Clockwork RNNs on tasks requiring very long-term dependencies.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{dohare2017,
  title = {Text {{Summarization}} Using {{Abstract Meaning Representation}}},
  author = {Dohare, Shibhansh and Karnick, Harish and Gupta, Vivek},
  year = {2017},
  pages = {1706.01678v3},
  abstract = {With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation (AMR). The pipeline proposed by us first generates an AMR graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on AMR. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.},
  journal = {arXiv},
  note = {10 pages, 4 figures, Update: Added more results, corrected figures and tables},
  type = {Journal Article}
}

@article{dong2018,
  title = {A Survey on Neural Network-Based Summarization Methods},
  author = {Dong, Y},
  year = {2018},
  abstract = {Automatic text summarization, the automated process of shortening a text while reserving the main ideas of the document (s), is a critical research area in natural language processing. The aim of this literature review is to survey the recent work on neural-based …},
  archiveprefix = {arXiv},
  eprint = {1804.04589},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1804.04589},
  note = {Times cited: 15},
  type = {Journal Article}
}

@article{dong2018a,
  title = {Banditsum: {{Extractive}} Summarization as a Contextual Bandit},
  author = {Dong, Yue and Shen, Yikang and Crawford, Eric and {van Hoof}, Herke and Cheung, Jackie Chi Kit},
  year = {2018},
  abstract = {In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach BanditSum as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences …},
  archiveprefix = {arXiv},
  eprint = {1809.09672},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1809.09672},
  note = {Times cited: 66},
  type = {Journal Article}
}

@article{dong2019,
  title = {Unified {{Language Model Pre}}-Training for {{Natural Language Understanding}} and {{Generation}}},
  author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2019},
  pages = {1905.03197v3},
  abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
  journal = {arXiv},
  note = {Accepted by NeurIPS-19. Code and pre-trained models: https://github.com/microsoft/unilm},
  type = {Journal Article}
}

@article{durmus2020,
  title = {{{FEQA}}: {{A Question Answering Evaluation Framework}} for {{Faithfulness Assessment}} in {{Abstractive Summarization}}},
  author = {Durmus, Esin and He, He and Diab, Mona},
  year = {2020},
  pages = {2005.03754v1},
  abstract = {Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering (QA) based metric for faithfulness, FEQA, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.},
  journal = {arXiv},
  note = {Accepted to ACL 2020},
  type = {Journal Article}
}

@article{dusek2019,
  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},
  author = {Dušek, Ondřej and Novikova, Jekaterina and Rieser, Verena},
  year = {2019},
  pages = {1901.07931v3},
  abstract = {This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning SLUG system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.},
  journal = {arXiv},
  note = {Computer Speech and Language, final accepted manuscript (in press)},
  type = {Journal Article}
}

@article{edunov2017,
  title = {Classical {{Structured Prediction Losses}} for {{Sequence}} to {{Sequence Learning}}},
  author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marc’Aurelio},
  year = {2017},
  pages = {1711.04956v5},
  abstract = {There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both IWSLT’14 German-English translation as well as Gigaword abstractive summarization. On the larger WMT’14 English-French translation task, sequence-level training achieves 41.5 BLEU which is on par with the state of the art.},
  journal = {arXiv},
  note = {10 pages, NAACL 2018},
  type = {Journal Article}
}

@article{ellouze2017,
  title = {Mix {{Multiple Features}} to {{Evaluate}} the {{Content}} and the {{Linguistic Quality}} of {{Text Summaries}}},
  author = {Ellouze, Samira and Jaoua, Maher and Hadrich Belguith, Lamia},
  year = {2017},
  volume = {25},
  pages = {149--166},
  publisher = {{Faculty of Electrical Engineering and Computing, Univ. of Zagreb}},
  doi = {10.20532/cit.2017.1003398},
  journal = {Journal of Computing and Information Technology},
  number = {2},
  type = {Journal Article}
}

@article{elman1990,
  title = {Finding Structure in Time},
  author = {Elman, Jeffrey L},
  year = {1990},
  volume = {14},
  pages = {179--211},
  publisher = {{Wiley Online Library}},
  doi = {10.1207/s15516709cog1402_1},
  abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current …},
  isbn = {0364-0213},
  journal = {Cognitive science},
  note = {Times cited: 11804},
  number = {2},
  type = {Journal Article}
}

@book{enarvi2020,
  title = {Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models},
  author = {Enarvi, Seppo and Amoia, Marilisa and Teba, Miguel Del-Agua and Delaney, Brian and Diehl, Frank and Hahn, Stefan and Harris, Kristina and McGrath, Liam and Pan, Yue and Pinto, Joel},
  year = {2020},
  volume = {Proceedings of the first workshop on natural language processing for medical conversations},
  abstract = {We discuss automatic creation of medical reports from ASR-generated patient-doctor conversational transcripts using an end-to-end neural summarization approach. We explore both recurrent neural network (RNN) and Transformer-based sequence-to-sequence architectures for summarizing medical conversations. We have incorporated enhancements to these architectures, such as the pointer-generator network that facilitates copying parts of the conversations to the reports, and a hierarchical RNN encoder that makes RNN training …}
}

@article{ermakova2019,
  title = {A Survey on Evaluation of Summarization Methods},
  author = {Ermakova, Liana and Cossu, Jean Valère and Mothe, Josiane},
  year = {2019},
  volume = {56},
  pages = {1794--1814},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.ipm.2019.04.001},
  journal = {Information Processing \& Management},
  note = {Times cited: 8},
  number = {5},
  type = {Journal Article}
}

@book{evans1996,
  title = {Straightforward Statistics for the Behavioral Sciences.},
  author = {Evans, James D},
  year = {1996},
  publisher = {{Thomson Brooks/Cole Publishing Co}}
}

@article{fabbri2020,
  title = {{{SummEval}}: {{Re}}-Evaluating {{Summarization Evaluation}}},
  author = {Fabbri, Alexander R. and Kryscinski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  year = {2020},
  pages = {2007.12626v3},
  abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continues to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 12 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations, 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) we assemble the largest collection of summaries generated by models trained on the CNN/DailyMail news dataset and share it in a unified format, 4) we implement and share a toolkit that provides an extensible and unified API for evaluating summarization models across a broad range of automatic metrics, 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the CNN/Daily Mail dataset annotated by both expert judges and crowd source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgements.},
  journal = {arXiv},
  note = {10 pages, 4 tables, 1 figure},
  type = {Journal Article}
}

@article{fan2017,
  title = {Controllable {{Abstractive Summarization}}},
  author = {Fan, Angela and Grangier, David and Auli, Michael},
  year = {2017},
  pages = {1711.05217v2},
  abstract = {Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically. On the full text CNN-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-ROUGE1 40.38 vs. 39.53 and human evaluation).},
  journal = {arXiv},
  note = {ACL2018 Workshop on Neural Machine Translation and Generation (NMT@ACL)},
  type = {Journal Article}
}

@article{fendji2020,
  title = {From Web to {{SMS}}: {{A}} Text Summarization of {{Wikipedia}} Pages with Character Limitation},
  author = {Fendji, JLEK and Aminatou, BAH},
  year = {2020},
  volume = {7},
  pages = {165277},
  publisher = {{European Alliance for Innovation n.o.}},
  doi = {10.4108/eai.11-6-2020.165277},
  journal = {EAI Endorsed Transactions on Creative Technologies},
  number = {24},
  type = {Journal Article}
}

@article{ferreira2019,
  title = {Neural Data-to-Text Generation: {{A}} Comparison between Pipeline and End-to-End Architectures},
  author = {Ferreira, Thiago Castro and van der Lee, Chris and van Miltenburg, Emiel and Krahmer, Emiel},
  year = {2019},
  pages = {1908.09022v2},
  abstract = {Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units (GRU) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.},
  journal = {arXiv},
  note = {Preprint version of the EMNLP 2019 article},
  type = {Journal Article}
}

@article{ferreto2011,
  title = {Server Consolidation with Migration Control for Virtualized Data Centers},
  author = {Ferreto, Tiago C. and Netto, Marco A.S. and Calheiros, Rodrigo N. and De Rose, César A.F.},
  year = {2011},
  volume = {27},
  pages = {1027--1034},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.future.2011.04.016},
  journal = {Future Generation Computer Systems},
  note = {Times cited: 182},
  number = {8},
  type = {Journal Article}
}

@book{finkel2005,
  title = {Incorporating Non-Local Information into Information Extraction Systems by Gibbs Sampling},
  author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher D},
  year = {2005},
  volume = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05)},
  abstract = {Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this …}
}

@book{flanigan2014,
  title = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
  author = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime G and Dyer, Chris and Smith, Noah A},
  year = {2014},
  volume = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  abstract = {Abstract Abstract Meaning Representation (AMR) is a semantic formalism for which a growing set of annotated examples is available. We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement. The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general …‏}
}

@book{foland2017,
  title = {Abstract {{Meaning Representation Parsing}} Using {{LSTM Recurrent Neural Networks}}},
  author = {Foland, William and Martin, James H.},
  year = {2017},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@article{fortunato2017,
  title = {Bayesian {{Recurrent Neural Networks}}},
  author = {Fortunato, Meire and Blundell, Charles and Vinyals, Oriol},
  year = {2017},
  pages = {1704.02798v4},
  abstract = {In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\%. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian RNNs. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian RNNs are superior to traditional RNNs on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.},
  journal = {arXiv},
  note = {12th Women in Machine Learning Workshop (WiML 2017), co-located with the 31st Conference on Neural Information Processing Systems (NeurIPS 2017), Long Beach, CA, USA},
  type = {Journal Article}
}

@article{francois2015,
  title = {Keras},
  author = {François, Chollet},
  year = {2015},
  publisher = {{GitHub}},
  journal = {GitHub repository},
  note = {https://github.com/fchollet/keras},
  type = {Journal Article}
}

@article{freund1997,
  title = {A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting},
  author = {Freund, Yoav and Schapire, Robert E},
  year = {1997},
  volume = {55},
  pages = {119--139},
  publisher = {{Elsevier}},
  abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show …},
  isbn = {0022-0000},
  journal = {Journal of computer and system sciences},
  note = {Times cited: 21149},
  number = {1},
  type = {Journal Article}
}

@article{friedman2001,
  title = {Greedy Function Approximation: A Gradient Boosting Machine},
  author = {Friedman, Jerome H},
  year = {2001},
  pages = {1189--1232},
  publisher = {{JSTOR}},
  abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent” boosting” paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special …},
  isbn = {0090-5364},
  journal = {Annals of statistics},
  note = {Times cited: 12904},
  type = {Journal Article}
}

@book{friedman2001a,
  title = {The Elements of Statistical Learning},
  author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert and others},
  year = {2001},
  volume = {1(10)},
  publisher = {{Springer series in statistics New York}}
}

@article{gabriel2020,
  title = {Go {{Figure}}! {{A Meta Evaluation}} of {{Factuality}} in {{Summarization}}},
  author = {Gabriel, Saadia and Celikyilmaz, Asli and Jha, Rahul and Choi, Yejin and Gao, Jianfeng},
  year = {2020},
  pages = {2010.12834v1},
  abstract = {Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don’t have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{galassi2020,
  title = {Attention in {{Natural Language Processing}}},
  author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
  year = {2020},
  publisher = {{IEEE}},
  doi = {10.1109/TNNLS.2020.3019893},
  abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions …},
  isbn = {2162-237X},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  type = {Journal Article}
}

@article{ganesan2018,
  title = {{{ROUGE}} 2.0: {{Updated}} and {{Improved Measures}} for {{Evaluation}} of {{Summarization Tasks}}},
  author = {Ganesan, Kavita},
  year = {2018},
  pages = {1803.01937v1},
  abstract = {Evaluation of summarization tasks is extremely crucial to determining the quality of machine generated summaries. Over the last decade, ROUGE has become the standard automatic evaluation measure for evaluating summarization tasks. While ROUGE has been shown to be effective in capturing n-gram overlap between system and human composed summaries, there are several limitations with the existing ROUGE measures in terms of capturing synonymous concepts and coverage of topics. Thus, often times ROUGE scores do not reflect the true quality of summaries and prevents multi-faceted evaluation of summaries (i.e. by topics, by overall content coverage and etc). In this paper, we introduce ROUGE 2.0, which has several updated measures of ROUGE: ROUGE-N+Synonyms, ROUGE-Topic, ROUGE-Topic+Synonyms, ROUGE-TopicUniq and ROUGE-TopicUniq+Synonyms; all of which are improvements over the core ROUGE measures.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{gao2020,
  title = {{{SUPERT}}: {{Towards New Frontiers}} in {{Unsupervised Evaluation Metrics}} for {{Multi}}-{{Document Summarization}}},
  author = {Gao, Yang and Zhao, Wei and Eger, Steffen},
  year = {2020},
  pages = {2005.03724v1},
  abstract = {We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18-39\%. Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.},
  journal = {arXiv},
  note = {ACL 2020},
  type = {Journal Article}
}

@article{garbacea2020,
  title = {Neural {{Language Generation}}: {{Formulation}}, {{Methods}}, and {{Evaluation}}},
  author = {Garbacea, Cristina and Mei, Qiaozhu},
  year = {2020},
  pages = {2007.15780v1},
  abstract = {Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.},
  journal = {arXiv},
  note = {70 pages},
  type = {Journal Article}
}

@article{gatt2017,
  title = {Survey of the {{State}} of the {{Art}} in {{Natural Language Generation}}: {{Core}} Tasks, Applications and Evaluation},
  author = {Gatt, Albert and Krahmer, Emiel},
  year = {2017},
  pages = {1703.09902v1},
  abstract = {This paper surveys the current state of the art in Natural Language Generation (NLG), defined as the task of generating text or speech from non-linguistic input. A survey of NLG is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of NLG technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in NLG and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between NLG and other areas of artificial intelligence; (c) draw attention to the challenges in NLG evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
  journal = {arXiv},
  note = {111 pages, 8 figures, 2 tables},
  type = {Journal Article}
}

@article{gehrmann2018,
  title = {Bottom-{{Up Abstractive Summarization}}},
  author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander M.},
  year = {2018},
  pages = {1808.10792v2},
  abstract = {Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain.},
  journal = {arXiv},
  note = {EMNLP 2018},
  type = {Journal Article}
}

@article{geoffrey1989,
  title = {Connectionist {{Learning Procedures}}},
  author = {Geoffrey, E. Hinton},
  year = {1989},
  volume = {40},
  pages = {185--234},
  journal = {Artif. Intell.},
  type = {Journal Article}
}

@book{geron2019,
  title = {Hands-on Machine Learning with Scikit-Learn, Keras, and {{TensorFlow}}: {{Concepts}}, Tools, and Techniques to Build Intelligent Systems},
  author = {Geron, Aurelien},
  year = {2019},
  publisher = {{O’Reilly Media}},
  address = {{Sebastopol, CA}}
}

@article{gers1999,
  title = {Learning to Forget: {{Continual}} Prediction with {{LSTM}}},
  author = {Gers, Felix A and Schmidhuber, Jürgen and Cummins, Fred},
  year = {1999},
  publisher = {{IET}},
  abstract = {Long short-term memory (LSTM) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative …},
  note = {Times cited: 3432},
  type = {Journal Article}
}

@article{gers2000,
  title = {Recurrent Nets That Time and Count},
  author = {Gers, FA and {Schmidhuber - Proceedings of the IEEE-INNS}, J},
  year = {2000},
  abstract = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks (RNN) can in principle learn to make use of it. We focus on long short-term memory (LSTM) because it usually outperforms other RNN. Surprisingly, LSTM augmented by” peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes …},
  journal = {ieeexplore.ieee.org},
  type = {Journal Article}
}

@article{gers2002,
  title = {Learning Precise Timing with {{LSTM}} Recurrent Networks},
  author = {Gers, Felix A and Schraudolph, Nicol N and Schmidhuber, Jürgen},
  year = {2002},
  volume = {3},
  pages = {115--143},
  abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by” peephole connections” from its internal cells to its multiplicative gates can learn the fine …},
  journal = {Journal of machine learning research},
  note = {Times cited: 1192},
  number = {Aug},
  type = {Journal Article}
}

@article{giannakopoulos2008,
  title = {Summarization System Evaluation Revisited},
  author = {Giannakopoulos, George and Karkaletsis, Vangelis and Vouros, George and Stamatopoulos, Panagiotis},
  year = {2008},
  volume = {5},
  pages = {1--39},
  publisher = {{Association for Computing Machinery (ACM)}},
  doi = {10.1145/1410358.1410359},
  abstract = {{$<$}jats:p{$>$}This article presents a novel automatic method (AutoSummENG) for the evaluation of summarization systems, based on comparing the character n-gram graphs representation of the extracted summaries and a number of model summaries. The presented approach is language neutral, due to its statistical nature, and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods. Within this study, we measure the effectiveness of different representation methods, namely, word and character n-gram graph and histogram, different n-gram neighborhood indication methods as well as different comparison methods between the supplied representations. A theory for the a priori determination of the methods’ parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process.},
  journal = {ACM Transactions on Speech and Language Processing},
  note = {Times cited: 49},
  number = {3},
  type = {Journal Article}
}

@article{giles1994,
  title = {Dynamic Recurrent Neural Networks: {{Theory}} and Applications},
  author = {Giles, C Lee and Kuhn, Gary M and Williams, Ronald J},
  year = {1994},
  volume = {5},
  pages = {153--156},
  publisher = {{IEEE}},
  abstract = {This special issue illustrates both the scientific trends of the early work in recurrent neural networks, and the mathematics of training when at least some recurrent terms of the network derivatives can be non-zero. Herein is a brief description of each of the papers. We have organized this description into two parts. The first part contains the papers that are mainly theoretical, and the second part contains the papers that are mainly applications. The order of papers is alphabetical by first author.},
  isbn = {1045-9227},
  journal = {IEEE Transactions on Neural Networks},
  note = {Times cited: 127},
  number = {2},
  type = {Journal Article}
}

@book{gilpin2018,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2018},
  publisher = {{IEEE}}
}

@article{goldberg2015,
  title = {A {{Primer}} on {{Neural Network Models}} for {{Natural Language Processing}}},
  author = {Goldberg, Yoav},
  year = {2015},
  pages = {1510.00726v1},
  abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{goldberg2017,
  title = {Neural Network Methods for Natural Language Processing},
  author = {Goldberg, Yoav},
  year = {2017},
  volume = {10},
  pages = {1--309},
  publisher = {{Morgan \& Claypool Publishers}},
  doi = {10.2200/S00762ED1V01Y201703HLT037},
  abstract = {Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is …},
  isbn = {1947-4040},
  journal = {Synthesis Lectures on Human Language Technologies},
  number = {1},
  type = {Journal Article}
}

@article{gomaa2013,
  title = {A Survey of Text Similarity Approaches},
  author = {Gomaa, Wael H and Fahmy, Aly A},
  year = {2013},
  volume = {68},
  pages = {13--18},
  publisher = {{Citeseer}},
  abstract = {Measuring the similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. This survey discusses the existing works on text similarity through partitioning them into three approaches; String-based, Corpus-based and Knowledgebased similarities. Furthermore, samples of combination between these …},
  journal = {International Journal of Computer Applications},
  note = {Times cited: 667},
  number = {13},
  type = {Journal Article}
}

@book{gonzalez2014,
  title = {{{IPA}} and {{STOUT}}: {{Leveraging}} Linguistic and Source-Based Features for Machine Translation Evaluation},
  author = {Gonzalez, Meritxell and {Barrón-Cedeno}, Alberto and Màrquez, Lluís},
  year = {2014},
  volume = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  abstract = {This paper describes the UPC submissions to the WMT14 Metrics Shared Task: UPCIPA and UPC-STOUT. These metrics use a collection of evaluation measures integrated in ASIYA, a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than …‏}
}

@book{goodrich2019,
  title = {Assessing {{The Factual Accuracy}} of {{Generated Text}}},
  author = {Goodrich, Ben and Rao, Vinay and Liu, Peter J. and Saleh, Mohammad},
  year = {2019},
  publisher = {{ACM}},
  address = {{New York, NY, USA}}
}

@book{grandl2014,
  title = {Multi-Resource Packing for Cluster Schedulers},
  author = {Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya},
  year = {2014},
  publisher = {{ACM}},
  address = {{New York, NY, USA}}
}

@incollection{graves2012,
  title = {Supervised {{Sequence Labelling}}},
  author = {Graves, Alex},
  year = {2012},
  pages = {5--13},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}}
}

@article{graves2013,
  title = {Generating Sequences with Recurrent Neural Networks},
  author = {Graves, Alex},
  year = {2013},
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online …},
  archiveprefix = {arXiv},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1308.0850},
  type = {Journal Article}
}

@article{graves2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  pages = {1410.5401v2},
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{groschwitz2018,
  title = {{{AMR Dependency Parsing}} with a {{Typed Semantic Algebra}}},
  author = {Groschwitz, Jonas and Lindemann, Matthias and Fowlie, Meaghan and Johnson, Mark and Koller, Alexander},
  year = {2018},
  pages = {1805.11465v1},
  abstract = {We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.},
  journal = {arXiv},
  note = {This paper will be presented at ACL 2018 (see https://acl2018.org/programme/papers/)},
  type = {Journal Article}
}

@article{grusky2018,
  title = {Newsroom: {{A Dataset}} of 1.3 {{Million Summaries}} with {{Diverse Extractive Strategies}}},
  author = {Grusky, Max and Naaman, Mor and Artzi, Yoav},
  year = {2018},
  pages = {1804.11283v2},
  abstract = {We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.},
  journal = {arXiv},
  note = {Proceedings of NAACL-HLT 2018 (Long Paper)},
  type = {Journal Article}
}

@article{gu2016,
  title = {Incorporating {{Copying Mechanism}} in {{Sequence}}-to-{{Sequence Learning}}},
  author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.},
  year = {2016},
  pages = {1603.06393v3},
  abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.},
  journal = {arXiv},
  note = {10 pages, 5 figures, accepted by ACL2016},
  type = {Journal Article}
}

@article{gu2020,
  title = {Perception {{Score}}, {{A Learned Metric}} for {{Open}}-Ended {{Text Generation Evaluation}}},
  author = {Gu, Jing and Wu, Qingyang and Yu, Zhou},
  year = {2020},
  pages = {2008.03082v2},
  abstract = {Automatic evaluation for open-ended natural language generation tasks remains a challenge. Existing metrics such as BLEU show a low correlation with human judgment. We propose a novel and powerful learning-based evaluation metric: Perception Score. The method measures the overall quality of the generation and scores holistically instead of only focusing on one evaluation criteria, such as word overlapping. Moreover, it also shows the amount of uncertainty about its evaluation result. By connecting the uncertainty, Perception Score gives a more accurate evaluation for the generation system. Perception Score provides state-of-the-art results on two conditional generation tasks and two unconditional generation tasks.},
  journal = {arXiv},
  note = {8 pages, 2 figures},
  type = {Journal Article}
}

@article{gulcehre2016,
  title = {Pointing the {{Unknown Words}}},
  author = {Gulcehre, Caglar and Ahn, Sungjin and Nallapati, Ramesh and Zhou, Bowen and Bengio, Yoshua},
  year = {2016},
  pages = {1603.08148v3},
  abstract = {The problem of rare and unknown words is an important issue that can potentially influence the performance of many NLP systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an MLP which is conditioned on the context. We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.},
  journal = {arXiv},
  note = {ACL 2016 Oral Paper},
  type = {Journal Article}
}

@article{gunel2020,
  title = {Mind {{The Facts}}: {{Knowledge}}-{{Boosted Coherent Abstractive Text Summarization}}},
  author = {Gunel, Beliz and Zhu, Chenguang and Zeng, Michael and Huang, Xuedong},
  year = {2020},
  pages = {2006.15435v1},
  abstract = {Neural models have become successful at producing abstractive summaries that are human-readable and fluent. However, these models have two critical shortcomings: they often don’t respect the facts that are either included in the source article or are known to humans as commonsense knowledge, and they don’t produce coherent summaries when the source article is long. In this work, we propose a novel architecture that extends Transformer encoder-decoder architecture in order to improve on these shortcomings. First, we incorporate entity-level knowledge from the Wikidata knowledge graph into the encoder-decoder architecture. Injecting structural world knowledge from Wikidata helps our abstractive summarization model to be more fact-aware. Second, we utilize the ideas used in Transformer-XL language model in our proposed encoder-decoder architecture. This helps our model with producing coherent summaries even when the source article is long. We test our model on CNN/Daily Mail summarization dataset and show improvements on ROUGE scores over the baseline Transformer model. We also include model predictions for which our model accurately conveys the facts, while the baseline Transformer model doesn’t.},
  journal = {arXiv},
  note = {NeurIPS 2019, Knowledge Representation \& Reasoning Meets Machine Learning (KR2ML workshop)},
  type = {Journal Article}
}

@article{guo2018,
  title = {Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation},
  author = {Guo, H and Pasunuru, R and Bansal, M},
  year = {2018},
  abstract = {An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also …},
  archiveprefix = {arXiv},
  eprint = {1805.11004},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1805.11004},
  note = {Times cited: 73},
  type = {Journal Article}
}

@article{guyon2003,
  title = {An Introduction to Variable and Feature Selection},
  author = {Guyon, Isabelle and Elisseeff, André},
  year = {2003},
  volume = {3},
  pages = {1157--1182},
  abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The …},
  journal = {Journal of machine learning research},
  note = {Times cited: 16233},
  number = {Mar},
  type = {Journal Article}
}

@book{hall1999,
  title = {Feature {{Selection}} for {{Machine Learning}}: {{Comparing}} a {{Correlation}}-{{Based Filter Approach}} to the {{Wrapper}}.},
  author = {Hall, Mark A and Smith, Lloyd A},
  year = {1999},
  volume = {FLAIRS conference 1999},
  abstract = {Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a filter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs …}
}

@article{haonan2020,
  title = {Exploring {{Explainable Selection}} to {{Control Abstractive Summarization}}},
  author = {Haonan, Wang and Yang, Gao and Yu, Bai and Lapata, Mirella and Heyan, Huang},
  year = {2020},
  pages = {2004.11779v2},
  abstract = {Like humans, document summarization models can interpret a document’s contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for sentence novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality, and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and BERT-based configurations. In a series of experiments assessed with ROUGE metrics and two human evaluations, ESCA outperformed eight state-of-the-art models on the CNN/DailyMail and NYT50 benchmark datasets.},
  journal = {arXiv},
  note = {Accepted by AAAI’2021. Include all Appendices},
  type = {Journal Article}
}

@article{hardy2018,
  title = {Guided {{Neural Language Generation}} for {{Abstractive Summarization}} Using {{Abstract Meaning Representation}}},
  author = {{Hardy} and Vlachos, Andreas},
  year = {2018},
  pages = {1808.09160v1},
  abstract = {Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at https://github.com/sheffieldnlp/AMR2Text-summ},
  journal = {arXiv},
  note = {Accepted in EMNLP 2018},
  type = {Journal Article}
}

@article{hardy2019,
  title = {{{HighRES}}: {{Highlight}}-Based {{Reference}}-Less {{Evaluation}} of {{Summarization}}},
  author = {{Hardy} and Narayan, Shashi and Vlachos, Andreas},
  year = {2019},
  pages = {1906.01361v1},
  abstract = {There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.},
  journal = {arXiv},
  note = {Accepted for ACL 2019},
  type = {Journal Article}
}

@book{harnly2005,
  title = {Automation of Summary Evaluation by the Pyramid Method},
  author = {Harnly, Aaron and Nenkova, Ani and Passonneau, Rebecca and Rambow, Owen},
  year = {2005},
  volume = {Recent Advances in Natural Language Processing (RANLP)},
  abstract = {The manual Pyramid method for summary evaluation, which focuses on the task of determining if a summary expresses the same content as a set of manual models, has shown sufficient promise that the Document Understanding Conference 2005 effort will make use of it. However, an automated approach would make the method far more useful for developers and evaluators of automated summarization systems. We present an experimental environment for testing automated evaluation of summaries, pre-annotated for …}
}

@article{hashimoto2019,
  title = {Unifying {{Human}} and {{Statistical Evaluation}} for {{Natural Language Generation}}},
  author = {Hashimoto, Tatsunori B. and Zhang, Hugh and Liang, Percy},
  year = {2019},
  pages = {1904.02792v1},
  abstract = {How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call HUSE. On summarization and chit-chat dialogue, we show that (i) HUSE detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease HUSE due to decreased diversity.},
  journal = {arXiv},
  note = {NAACL Camera Ready Submission},
  type = {Journal Article}
}

@article{hastings1970,
  title = {Monte {{Carlo}} Sampling Methods Using {{Markov}} Chains and Their Applications},
  author = {Hastings, W. K.},
  year = {1970},
  volume = {57},
  pages = {97--109},
  publisher = {{Oxford University Press (OUP)}},
  doi = {10.1093/biomet/57.1.97},
  journal = {Biometrika},
  note = {Times cited: 7297},
  number = {1},
  type = {Journal Article}
}

@article{haviv2019,
  title = {Understanding and {{Controlling Memory}} in {{Recurrent Neural Networks}}},
  author = {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
  year = {2019},
  volume = {cs.LG},
  abstract = {To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network’s hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ‘slow point’. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
  note = {The link to the code was changed due to technical issues with the original repository. We no longer refer to the process shown in Figure 5C as a bifurcation diagram, but describe it in a more precise manner. We thank an anonymous reviewer for pointing this out},
  type = {Journal Article}
}

@article{he2018,
  title = {Jointly Predicting Predicates and Arguments in Neural Semantic Role Labeling},
  author = {He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
  year = {2018},
  archiveprefix = {arXiv},
  eprint = {1805.04787},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1805.04787},
  type = {Journal Article}
}

@article{hermann2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  pages = {1506.03340v3},
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  journal = {arXiv},
  note = {Appears in: Advances in Neural Information Processing Systems 28 (NIPS 2015). 14 pages, 13 figures},
  type = {Journal Article}
}

@article{hinton2006,
  title = {A Fast Learning Algorithm for Deep Belief Nets},
  author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  year = {2006},
  volume = {18},
  pages = {1527--1554},
  publisher = {{MIT Press}},
  abstract = {We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed …},
  isbn = {0899-7667},
  journal = {Neural computation},
  note = {Times cited: 14070},
  number = {7},
  type = {Journal Article}
}

@article{hinton2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
  year = {2012},
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This” overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in …},
  archiveprefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1207.0580},
  note = {Times cited: 5635},
  type = {Journal Article}
}

@book{ho1995,
  title = {Random {{Decision Forests}}},
  author = {Ho, Tin Kam},
  year = {1995},
  publisher = {{IEEE Computer Society}},
  address = {{USA}},
  abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.},
  keywords = {complexity,decision theory,decision trees,generalization accuracy,handwriting recognition,handwritten digits,optical character recognition,random decision forests,stochastic modeling,suboptimal accuracy,tree-based classifiers}
}

@article{hochreiter1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year = {1997},
  volume = {9},
  pages = {1735--1780},
  publisher = {{MIT Press}},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter’s (1991) analysis of this problem, then address it by introducing a novel …},
  isbn = {0899-7667},
  journal = {Neural computation},
  note = {Times cited: 38015},
  number = {8},
  type = {Journal Article}
}

@article{hochreiter2001,
  title = {Gradient Flow in Recurrent Nets: The Difficulty of Learning Long-Term Dependencies},
  author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen},
  year = {2001},
  publisher = {{A field guide to dynamical recurrent neural networks. IEEE Press}},
  abstract = {Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take …},
  note = {Times cited: 1312},
  type = {Journal Article}
}

@article{hovy1999,
  title = {Automated Text Summarization in {{SUMMARIST}}},
  author = {Hovy, Eduard and Lin, Chin-Yew},
  year = {1999},
  volume = {14},
  pages = {81--94},
  publisher = {{MIT press Cambridge, MA}},
  abstract = {SUMMARIST is an attempt to create a robust automated text summarization system, based on the’equation’: summarization= topic identification+ interpretation+ generation. Each of these stages contains several independent modules, many of them trained on large corpora …},
  journal = {Advances in automatic text summarization},
  note = {Times cited: 922},
  type = {Journal Article}
}

@article{hsu2018,
  title = {A {{Unified Model}} for {{Extractive}} and {{Abstractive Summarization}} Using {{Inconsistency Loss}}},
  author = {Hsu, Wan-Ting and Lin, Chieh-Kai and Lee, Ming-Ying and Min, Kerui and Tang, Jing and Sun, Min},
  year = {2018},
  pages = {1805.06266v2},
  abstract = {We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.},
  journal = {arXiv},
  note = {9 pages, ACL 2018 oral. Project page: https://hsuwanting.github.io/unified\_summ/. Code: https://github.com/HsuWanTing/unified-summarization},
  type = {Journal Article}
}

@book{hu2014,
  title = {Automatic Generation of Related Work Sections in Scientific Papers: An Optimization Approach},
  author = {Hu, Yue and Wan, Xiaojun},
  year = {2014},
  volume = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  abstract = {In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called ARWG to address this task. It first exploits a PLSA model to split the sentence set of the given papers into different topic-biased parts, and then applies …}
}

@article{hu2015,
  title = {{{LCSTS}}: {{A Large Scale Chinese Short Text Summarization Dataset}}},
  author = {Hu, Baotian and Chen, Qingcai and Zhu, Fangze},
  year = {2015},
  volume = {cs.CL},
  abstract = {Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public http://icrc.hitsz.edu.cn/Article/show/139.html. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.},
  note = {Recently, we received feedbacks from Yuya Taguchi from NAIST in Japan and Qian Chen from USTC of China, that the results in the EMNLP2015 version seem to be underrated. So we carefully checked our results and find out that we made a mistake while using the standard ROUGE. Then we re-evaluate all methods in the paper and get corrected results listed in Table 2 of this version},
  type = {Journal Article}
}

@article{hu2018,
  title = {An {{Introductory Survey}} on {{Attention Mechanisms}} in {{NLP Problems}}},
  author = {Hu, Dichao},
  year = {2018},
  pages = {1811.05544v1},
  abstract = {First derived from human intuition, later adapted to machine translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this paper, we survey through recent works and conduct an introductory summary of the attention mechanism in different NLP problems, aiming to provide our readers with basic knowledge on this widely used method, discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.},
  journal = {arXiv},
  note = {9 pages},
  type = {Journal Article}
}

@article{huang2020,
  title = {What {{Have We Achieved}} on {{Text Summarization}}},
  author = {Huang, Dandan and Cui, Leyang and Yang, Sen and Bao, Guangsheng and Wang, Kun and Xie, Jun and Zhang, Yue},
  year = {2020},
  pages = {2010.04529v1},
  abstract = {Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric(MQM) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.},
  journal = {arXiv},
  note = {Accepted by EMNLP 2020},
  type = {Journal Article}
}

@article{huang2020a,
  title = {Knowledge {{Graph}}-{{Augmented Abstractive Summarization}} with {{Semantic}}-{{Driven Cloze Reward}}},
  author = {Huang, Luyang and Wu, Lingfei and Wang, Lu},
  year = {2020},
  pages = {2005.01159v1},
  abstract = {Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present ASGARD, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven RewarD. We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher ROUGE scores than a variant without knowledge graph as input on both New York Times and CNN/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.},
  journal = {arXiv},
  note = {Accepted as a long paper to ACL 2020},
  type = {Journal Article}
}

@book{hussin2011,
  title = {Efficient {{Energy Management Using Adaptive Reinforcement Learning}}-{{Based Scheduling}} in {{Large}}-{{Scale Distributed Systems}}},
  author = {Hussin, Masnida and Lee, Young Choon and Zomaya, Albert Y.},
  year = {2011},
  publisher = {{IEEE}}
}

@book{i.2010,
  title = {Energy-Efficient Application-Aware Online Provisioning for Virtualized Clouds and Data Centers},
  author = {I., Rodero and J., Jaramillo and A., Quiroz and M., Parashar and F., Guim and S., Poole},
  year = {2010},
  month = aug,
  abstract = {As energy efficiency and associated costs become key concerns, consolidated and virtualized data centers and clouds are attractive computing platforms for data- and compute- intensive applications. These platforms provide an abstraction of nearly-unlimited computing resources through the elastic use of pools of consolidated resources, and provide opportunities for higher utilization and energy savings. Recently, these platforms are also being considered for more traditional high-performance computing (HPC) applications that have typically targeted Grids and similar conventional HPC platforms. However, maximizing energy efficiency, cost-effectiveness, and utilization for these applications while ensuring performance and other Quality of Service (QoS) guarantees, requires leveraging important and extremely challenging tradeoffs. These include, for example, the tradeoff between the need to efficiently create and provision Virtual Machines (VMs) on data center resources and the need to accommodate the heterogeneous resource demands and runtimes of these applications. In this paper we present an energy-aware online provisioning approach for HPC applications on consolidated and virtualized computing platforms. Energy efficiency is achieved using a workload-aware, just-right dynamic provisioning mechanism and the ability to power down subsystems of a host system that are not required by the VMs mapped to it. We evaluate the presented approach using real HPC workload traces from widely distributed production systems. The results presented demonstrated that compared to typical reactive or predefined provisioning, our approach achieves significant improvements in energy efficiency with an acceptable QoS penalty.},
  keywords = {Algorithm design and analysis,Autonomic Computing,Cloud Computing,Clouds,Clustering algorithms,computer centres,Data Center,data center resources,distributed production systems,dynamic provisioning mechanism,Efficiency,Energy,energy efficiency,energy-aware online provisioning approach,energy-efficient application-aware online provisioning,grid computing,heterogeneous resource demands,high-performance computing,HPC applications,Internet,QoS guarantees,quality of service,Quality of service,Resource management,Resource Provisioning,Runtime,Servers,virtual machines,Virtualization,virtualized clouds,virtualized computing platforms,virtualized data centers}
}

@book{ian2016,
  title = {Deep {{Learning}}},
  author = {Ian, Goodfellow and Yoshua, Bengio and Aaron, Courville},
  year = {2016},
  publisher = {{MIT Press}}
}

@article{ibrahimaltmami2020,
  title = {Automatic Summarization of Scientific Articles: {{A}} Survey},
  author = {Ibrahim Altmami, Nouf and El Bachir Menai, Mohamed},
  year = {2020},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jksuci.2020.04.020},
  journal = {Journal of King Saud University - Computer and Information Sciences},
  type = {Journal Article}
}

@book{james2014,
  title = {An {{Introduction}} to {{Statistical Learning}}: {{With Applications}} in {{R}}},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2014},
  publisher = {{Springer Publishing Company, Incorporated}},
  abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.}
}

@article{jiang2018,
  title = {Closed-{{Book Training}} to {{Improve Summarization Encoder Memory}}},
  author = {Jiang, Yichen and Bansal, Mohit},
  year = {2018},
  pages = {1809.04585v1},
  abstract = {A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder’s memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional ‘closed-book’ decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can’t rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.},
  journal = {arXiv},
  note = {EMNLP 2018 (16 pages)},
  type = {Journal Article}
}

@book{jozefowicz2015,
  title = {An Empirical Exploration of Recurrent Network Architectures},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  year = {2015},
  volume = {International conference on machine learning},
  abstract = {Abstract The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We …}
}

@book{jung2010,
  title = {Mistral: {{Dynamically Managing Power}}, {{Performance}}, and {{Adaptation Cost}} in {{Cloud Infrastructures}}},
  author = {Jung, Gueyoung and Hiltunen, Matti A. and Joshi, Kaustubh R. and Schlichting, Richard D. and Pu, Calton},
  year = {2010},
  publisher = {{IEEE}}
}

@book{jurafsky2009,
  title = {Speech and Language Processing ({{2Nd}} Edition)},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2009},
  publisher = {{Prentice-Hall, Inc.}},
  address = {{Upper Saddle River, NJ, USA}},
  isbn = {0-13-187321-0}
}

@article{k.2005,
  title = {High-Performance, Power-Aware Distributed Computing for Scientific Applications},
  author = {K., W. Cameron and Rong, Ge and Xizhou, Feng},
  year = {2005},
  month = nov,
  volume = {38},
  pages = {40--47},
  doi = {10.1109/MC.2005.380},
  abstract = {The PowerPack framework enables distributed systems to profile, analyze, and conserve energy in scientific applications using dynamic voltage scaling. For one common benchmark, the framework achieves more than 30 percent energy savings with minimal performance impact.},
  isbn = {1558-0814},
  journal = {Computer},
  keywords = {Biological system modeling,Computational modeling,Costs,Distributed computing,Distributed systems,DNA,dynamic voltage scaling,Dynamic voltage scaling,Energy consumption,Large-scale systems,natural sciences computing,parallel processing,parallel systems,power consumption,Power system reliability,Power-aware computing,power-aware distributed computing,PowerPack framework,scientific applications,Temperature,Weather forecasting},
  number = {11},
  type = {Journal Article}
}

@book{k.2011,
  title = {Computing {{Krippendorff}}’s {{Alpha}}-{{Reliability}}},
  author = {K., Krippendorff},
  year = {2011}
}

@article{kedzie2018,
  title = {Content {{Selection}} in {{Deep Learning Models}} of {{Summarization}}},
  author = {Kedzie, Chris and McKeown, Kathleen and III, Hal Daume},
  year = {2018},
  pages = {1810.12343v2},
  abstract = {We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the summarization task.},
  journal = {arXiv},
  note = {Revised to correct for error in AMI oracle results. Originally published at EMNLP 2018},
  type = {Journal Article}
}

@article{khemka2015,
  title = {Utility Maximizing Dynamic Resource Management in an Oversubscribed Energy-Constrained Heterogeneous Computing System},
  author = {Khemka, Bhavesh and Friese, Ryan and Pasricha, Sudeep and Maciejewski, Anthony A. and Siegel, Howard Jay and Koenig, Gregory A. and Powers, Sarah and Hilton, Marcia and Rambharos, Rajendra and Poole, Steve},
  year = {2015},
  volume = {5},
  pages = {14--30},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.suscom.2014.08.001},
  journal = {Sustainable Computing: Informatics and Systems},
  note = {Times cited: 19},
  type = {Journal Article}
}

@book{kim2012,
  title = {Application-Specific {{Cloud Provisioning Model Using Job Profiles Analysis}}},
  author = {Kim, Seoyoung and Kim, Yoonhee},
  year = {2012},
  publisher = {{IEEE}}
}

@article{kim2018,
  title = {Abstractive {{Summarization}} of {{Reddit Posts}} with {{Multi}}-Level {{Memory Networks}}},
  author = {Kim, Byeongchang and Kim, Hyunwoo and Kim, Gunhee},
  year = {2018},
  pages = {1811.00783v2},
  abstract = {We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit TIFU dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks (MMN), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit TIFU dataset is highly abstractive and the MMN outperforms the state-of-the-art summarization models.},
  journal = {arXiv},
  note = {Published in NAACL-HLT 2019 (Oral)},
  type = {Journal Article}
}

@article{kingma2014,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2014},
  pages = {1412.6980v9},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  journal = {arXiv},
  note = {Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  type = {Journal Article}
}

@book{klein2003,
  title = {Accurate Unlexicalized Parsing},
  author = {Klein, Dan and Manning, Christopher D},
  year = {2003},
  volume = {Proceedings of the 41st annual meeting of the association for computational linguistics},
  abstract = {We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its …}
}

@article{kliazovich2013,
  title = {{{DENS}}: Data Center Energy-Efficient Network-Aware Scheduling},
  author = {Kliazovich, Dzmitry and Bouvry, Pascal and Khan, Samee Ullah},
  year = {2013},
  volume = {16},
  pages = {65--75},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10586-011-0177-4},
  journal = {Cluster Computing},
  note = {Times cited: 125},
  number = {1},
  type = {Journal Article}
}

@article{kohavi1997,
  title = {Wrappers for Feature Subset Selection},
  author = {Kohavi, Ron and John, George H},
  year = {1997},
  volume = {97},
  pages = {273--324},
  publisher = {{Elsevier}},
  abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a …},
  isbn = {0004-3702},
  journal = {Artificial intelligence},
  note = {Times cited: 9862},
  number = {1-2},
  type = {Journal Article}
}

@article{krantz2018,
  title = {Abstractive {{Summarization Using Attentive Neural Techniques}}},
  author = {Krantz, Jacob and Kalita, Jugal},
  year = {2018},
  pages = {1810.08838v1},
  abstract = {In a world of proliferating data, the ability to rapidly summarize text is growing in importance. Automatic summarization of text can be thought of as a sequence to sequence problem. Another area of natural language processing that solves a sequence to sequence problem is machine translation, which is rapidly evolving due to the development of attention-based encoder-decoder networks. This work applies these modern techniques to abstractive summarization. We perform analysis on various attention mechanisms for summarization with the goal of developing an approach and architecture aimed at improving the state of the art. In particular, we modify and optimize a translation model with self-attention for generating abstractive sentence summaries. The effectiveness of this base model along with attention variants is compared and analyzed in the context of standardized evaluation sets and test metrics. However, we show that these metrics are limited in their ability to effectively score abstractive summaries, and propose a new approach based on the intuition that an abstractive model requires an abstractive evaluation.},
  journal = {arXiv},
  note = {Accepted for oral presentation at the 15th International Conference on Natural Language Processing (ICON 2018)},
  type = {Journal Article}
}

@article{kreutzer2019,
  title = {Joey {{NMT}}: {{A}} Minimalist {{NMT}} Toolkit for Novices},
  author = {Kreutzer, Julia and Bastings, Joost and Riezler, Stefan},
  year = {2019},
  abstract = {We present Joey NMT, a minimalist neural machine translation toolkit based on PyTorch that is specifically designed for novices. Joey NMT provides many popular NMT features in a small and simple code base, so that novices can easily and quickly learn to use it and adapt it to their needs. Despite its focus on simplicity, Joey NMT supports classic architectures (RNNs, transformers), fast beam search, weight tying, and more, and achieves performance comparable to more complex toolkits on standard benchmarks. We evaluate the accessibility …},
  archiveprefix = {arXiv},
  eprint = {1907.12484},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1907.12484},
  note = {Times cited: 28},
  type = {Journal Article}
}

@article{krippendorff2011,
  title = {Computing {{Krippendorff}}’s Alpha-Reliability},
  author = {Krippendorff, Klaus},
  year = {2011},
  abstract = {Krippendorff’s alpha (α) is a reliability coefficient developed to measure the agreement among observers, coders, judges, raters, or measuring instruments drawing distinctions among typically unstructured phenomena or assign computable values to them. α emerged in content analysis but is widely applicable wherever two or more methods of generating data are applied to the same set of objects, units of analysis, or items and the question is how much the resulting data can be trusted to represent something real.},
  note = {Times cited: 638},
  type = {Journal Article}
}

@article{kryscinski2018,
  title = {Improving Abstraction in Text Summarization},
  author = {Kryściński, Wojciech and Paulus, Romain and Xiong, Caiming and Socher, Richard},
  year = {2018},
  abstract = {Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document …},
  archiveprefix = {arXiv},
  eprint = {1808.07913},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1808.07913},
  note = {Times cited: 71},
  type = {Journal Article}
}

@article{kryscinski2019,
  title = {Evaluating the {{Factual Consistency}} of {{Abstractive Text Summarization}}},
  author = {Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  pages = {1910.12840v1},
  abstract = {Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.},
  journal = {arXiv},
  note = {11 pages, 7 tables, 1 algorithm},
  type = {Journal Article}
}

@book{kumar2016,
  title = {Ask Me Anything: {{Dynamic}} Memory Networks for Natural Language Processing},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  year = {2016},
  volume = {International conference on machine learning},
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic …}
}

@article{laban2020,
  title = {The Summary Loop: {{Learning}} to Write Abstractive Summaries without Examples},
  author = {Laban, P and Hsi, A and Canny, J and Hearst, MA},
  year = {2020},
  abstract = {This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score …},
  journal = {… of the 58th Annual Meeting of the …},
  note = {Times cited: 1},
  type = {Journal Article}
}

@book{lavie2007,
  title = {{{METEOR}}: {{An}} Automatic Metric for {{MT}} Evaluation with High Levels of Correlation with Human Judgments},
  author = {Lavie, Alon and Agarwal, Abhaya},
  year = {2007},
  volume = {Proceedings of the second workshop on statistical machine translation},
  abstract = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year’s shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric …}
}

@book{lee2011,
  title = {Stanford’s Multi-Pass Sieve Coreference Resolution System at the Conll-2011 Shared Task},
  author = {Lee, Heeyoung and Peirsman, Yves and Chang, Angel and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
  year = {2011},
  volume = {Proceedings of the 15th conference on computational natural language learning: Shared task},
  publisher = {{Association for Computational Linguistics}},
  abstract = {This paper details the coreference resolution system submitted by Stanford at the CoNLL-2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was …}
}

@article{leerodgers1988,
  title = {Thirteen {{Ways}} to {{Look}} at the {{Correlation Coefficient}}},
  author = {Lee Rodgers, Joseph and Nicewander, W. Alan},
  year = {1988},
  volume = {42},
  pages = {59--66},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/00031305.1988.10475524},
  journal = {The American Statistician},
  note = {Times cited: 351},
  number = {1},
  type = {Journal Article}
}

@article{lei2018,
  title = {Opening the Black Box of Deep Learning},
  author = {Lei, Dian and Chen, Xiaoxiao and Zhao, Jianfei},
  year = {2018},
  volume = {cs.LG},
  abstract = {The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.},
  type = {Journal Article}
}

@article{lewis2019,
  title = {{{BART}}: {{Denoising Sequence}}-to-{{Sequence Pre}}-Training for {{Natural Language Generation}}, {{Translation}}, and {{Comprehension}}},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  year = {2019},
  pages = {1910.13461v1},
  abstract = {We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{li2015,
  title = {A {{Diversity}}-{{Promoting Objective Function}} for {{Neural Conversation Models}}},
  author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  year = {2015},
  pages = {1510.03055v3},
  abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., “I don’t know”) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.},
  journal = {arXiv},
  note = {In. Proc of NAACL 2016},
  type = {Journal Article}
}

@article{li2016,
  title = {A {{Simple}}, {{Fast Diverse Decoding Algorithm}} for {{Neural Generation}}},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2016},
  pages = {1611.08562v2},
  abstract = {In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning (RL). We observe a further performance boost from this RL technique. This paper includes material from the unpublished script “Mutual Information and Diverse Decoding Improve Neural Machine Translation” (Li and Jurafsky, 2016).},
  journal = {arXiv},
  type = {Journal Article}
}

@article{li2017,
  title = {Deep {{Recurrent Generative Decoder}} for {{Abstractive Text Summarization}}},
  author = {Li, Piji and Lam, Wai and Bing, Lidong and Wang, Zihao},
  year = {2017},
  pages = {1708.00625v1},
  abstract = {We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder (DRGN). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that DRGN achieves improvements over the state-of-the-art methods.},
  journal = {arXiv},
  note = {10 pages, EMNLP 2017},
  type = {Journal Article}
}

@article{li2020,
  title = {Text {{Summarization Method Based}} on {{Double Attention Pointer Network}}},
  author = {Li, Zhixin and Peng, Zhi and Tang, Suqin and Zhang, Canlong and Ma, Huifang},
  year = {2020},
  volume = {8},
  pages = {11279--11288},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/access.2020.2965575},
  journal = {IEEE Access},
  note = {Times cited: 3},
  type = {Journal Article}
}

@article{li2021,
  title = {Abstractive {{Multi}}-{{Document Summarization Based}} on {{Semantic Link Network}}},
  author = {Li, Wei and Zhuge, Hai},
  year = {2021},
  volume = {33},
  pages = {43--54},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tkde.2019.2922957},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  note = {Times cited: 2},
  number = {1},
  type = {Journal Article}
}

@article{liao2018,
  title = {Abstract {{Meaning Representation}} for {{Multi}}-{{Document Summarization}}},
  author = {Liao, Kexin and Lebanoff, Logan and Liu, Fei},
  year = {2018},
  pages = {1806.05655v1},
  abstract = {Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation (AMR), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the AMR formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.},
  journal = {arXiv},
  note = {13 pages},
  type = {Journal Article}
}

@article{lim2020,
  title = {I {{Know What You Asked}}: {{Graph Path Learning}} Using {{AMR}} for {{Commonsense Reasoning}}},
  author = {Lim, Jungwoo and Oh, Dongsuk and Jang, Yoonna and Yang, Kisu and Lim, Heuiseok},
  year = {2020},
  pages = {2011.00766v2},
  abstract = {CommonsenseQA is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an AMR-ConceptNet-Pruned (ACP) graph. The ACP graph is pruned from a full integrated graph encompassing Abstract Meaning Representation (AMR) graph generated from input questions and an external commonsense knowledge graph, ConceptNet (CN). Then the ACP graph is exploited to interpret the reasoning path as well as to predict the correct answer on the CommonsenseQA task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the ACP graph. Moreover, ACP-based models are shown to outperform the baselines.},
  journal = {arXiv},
  note = {Accepted to COLING 2020},
  type = {Journal Article}
}

@book{lin2003,
  title = {Automatic Evaluation of Summaries Using {{N}}-Gram Co-Occurrence Statistics},
  author = {Lin, Chin-Yew and Hovy, Eduard},
  year = {2003},
  publisher = {{Association for Computational Linguistics}},
  address = {{Morristown, NJ, USA}}
}

@book{lin2004,
  title = {Rouge: {{A}} Package for Automatic Evaluation of Summaries},
  author = {Lin, Chin-Yew},
  year = {2004},
  volume = {Text summarization branches out},
  abstract = {ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in …}
}

@book{lin2006,
  title = {An Information Theoretic Approach to Automatic Evaluation of Summaries},
  author = {Lin, Chin Yew and Cao, Guihong and Gao, Jianfeng and Nie, Jian Yun},
  year = {2006},
  volume = {Proceedings of the Human Language Technology Conference of the NAACL, Main Conference},
  abstract = {Until recently there are no common, convenient, and repeatable evaluation methods that could be easily applied to support fast turn-around development of automatic text summarization systems. In this paper, we introduce an informationtheoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries. Several variants of the approach are also considered and compared. The results indicate that JS …}
}

@book{lin2012,
  title = {Combining Coherence Models and Machine Translation Evaluation Metrics for Summarization Evaluation},
  author = {Lin, Ziheng and Liu, Chang and Ng, Hwee Tou and Kan, Min-Yen},
  year = {2012},
  volume = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  abstract = {An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current AESOP task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure …}
}

@book{lin2016,
  title = {A {{Reinforcement Learning}}-{{Based Power Management Framework}} for {{Green Computing Data Centers}}},
  author = {Lin, Xue and Wang, Yanzhi and Pedram, Massoud},
  year = {2016},
  publisher = {{IEEE}}
}

@article{lipton2015,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  year = {2015},
  volume = {cs.LG},
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
  type = {Journal Article}
}

@article{liu2016,
  title = {How Not to Evaluate Your Dialogue System: {{An}} Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation},
  author = {Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
  year = {2016},
  abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model’s generated …},
  archiveprefix = {arXiv},
  eprint = {1603.08023},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1603.08023},
  note = {Times cited: 787},
  type = {Journal Article}
}

@article{liu2018,
  title = {Generating {{Wikipedia}} by {{Summarizing Long Sequences}}},
  author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  year = {2018},
  pages = {1801.10198v1},
  abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
  journal = {arXiv},
  note = {Published as a conference paper at ICLR 2018},
  type = {Journal Article}
}

@article{liu2018a,
  title = {An {{AMR Aligner Tuned}} by {{Transition}}-Based {{Parser}}},
  author = {Liu, Yijia and Che, Wanxiang and Zheng, Bo and Qin, Bing and Liu, Ting},
  year = {2018},
  pages = {1810.03541v1},
  abstract = {In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score.},
  journal = {arXiv},
  note = {EMNLP2018},
  type = {Journal Article}
}

@book{liu2018b,
  title = {Controlling Length in Abstractive Summarization Using a Convolutional Neural Network},
  author = {Liu, Yizhu and Luo, Zhiyi and Zhu, Kenny},
  year = {2018},
  volume = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  abstract = {Convolutional neural networks (CNNs) have met great success in abstractive summarization, but they cannot effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in abstractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this …}
}

@article{liu2019,
  title = {Text {{Summarization}} with {{Pretrained Encoders}}},
  author = {Liu, Yang and Lapata, Mirella},
  year = {2019},
  pages = {1908.08345v2},
  abstract = {Bidirectional Encoder Representations from Transformers (BERT) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how BERT can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on BERT which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/PreSumm},
  journal = {arXiv},
  note = {fix typos},
  type = {Journal Article}
}

@article{liu2019a,
  title = {Topic-Aware {{Pointer}}-{{Generator Networks}} for {{Summarizing Spoken Conversations}}},
  author = {Liu, Zhengyuan and Ng, Angela and Lee, Sheldon and Aw, Ai Ti and Chen, Nancy F.},
  year = {2019},
  pages = {1910.01335v1},
  abstract = {Due to the lack of publicly available resources, conversation summarization has received far less attention than text summarization. As the purpose of conversations is to exchange information between at least two interlocutors, key information about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers. This phenomenon is more pronounced during spoken conversations, where speech characteristics such as backchanneling and false-starts might interrupt the topical flow. Moreover, topic diffusion and (intra-utterance) topic drift are also more common in human-to-human conversations. Such linguistic characteristics of dialogue topics make sentence-level extractive summarization approaches used in spoken documents ill-suited for summarizing conversations. Pointer-generator networks have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural modeling in text summarization. To the best of our knowledge, to date no one has adopted it for summarizing conversations. In this work, we propose a topic-aware architecture to exploit the inherent hierarchical structure in conversations to further adapt the pointer-generator model. Our approach significantly outperforms competitive baselines, achieves more efficient learning outcomes, and attains more robust performance.},
  journal = {arXiv},
  note = {To appear in ASRU2019},
  type = {Journal Article}
}

@article{louis2013,
  title = {Automatically {{Assessing Machine Summary Content Without}} a {{Gold Standard}}},
  author = {Louis, Annie and Nenkova, Ani},
  year = {2013},
  volume = {39},
  pages = {267--300},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/coli_a_00123},
  abstract = {{$<$}jats:p{$>$} The most widely adopted approaches for evaluation of summary content follow some protocol for comparing a summary with gold-standard human summaries, which are traditionally called model summaries. This evaluation paradigm falls short when human summaries are not available and becomes less accurate when only a single model is available. We propose three novel evaluation techniques. Two of them are model-free and do not rely on a gold standard for the assessment. The third technique improves standard automatic evaluations by expanding the set of available model summaries with chosen system summaries. {$<$}/jats:p{$><$}jats:p{$>$} We show that quantifying the similarity between the source text and its summary with appropriately chosen measures produces summary scores which replicate human assessments accurately. We also explore ways of increasing evaluation quality when only one human model summary is available as a gold standard. We introduce pseudomodels, which are system summaries deemed to contain good content according to automatic evaluation. Combining the pseudomodels with the single human model to form the gold-standard leads to higher correlations with human judgments compared to using only the one available model. Finally, we explore the feasibility of another measure—similarity between a system summary and the pool of all other system summaries for the same input. This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9.},
  journal = {Computational Linguistics},
  note = {Times cited: 29},
  number = {2},
  type = {Journal Article}
}

@book{louppe2012,
  title = {Ensembles on Random Patches},
  author = {Louppe, Gilles and Geurts, Pierre},
  year = {2012},
  volume = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  publisher = {{Springer}},
  abstract = {In this paper, we consider supervised learning under the assumption that the available memory is small compared to the dataset size. This general framework is relevant in the context of big data, distributed databases and embedded systems. We investigate a very …}
}

@article{luong2015,
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  pages = {1508.04025v5},
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  journal = {arXiv},
  note = {11 pages, 7 figures, EMNLP 2015 camera-ready version, more training details},
  type = {Journal Article}
}

@book{ma2017,
  title = {Blend: A Novel Combined {{MT}} Metric Based on Direct Assessment—Casict-Dcu Submission to {{WMT17}} Metrics Task},
  author = {Ma, Qingsong and Graham, Yvette and Wang, Shugen and Liu, Qun},
  year = {2017},
  volume = {Proceedings of the second conference on machine translation},
  abstract = {Existing metrics to evaluate the quality of Machine Translation hypotheses take different perspectives into account. DPM-Fcomb, a metric combining the merits of a range of metrics, achieved the best performance for evaluation of to-English language pairs in the previous two years of WMT Metrics Shared Tasks. This year, we submit a novel combined metric, Blend, to WMT17 Metrics task. Compared to DPMFcomb, Blend includes the following adaptations: i) We use DA human evaluation to guide the training process with a vast …‏}
}

@book{mairesse2010,
  title = {Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning},
  author = {Mairesse, François and Gasic, Milica and Jurcicek, Filip and Keizer, Simon and Thomson, Blaise and Yu, Kai and Young, Steve},
  year = {2010},
  volume = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  abstract = {Most previous work on trainable language generation has focused on two paradigms:(a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents BAGEL, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows …}
}

@book{malvino1977,
  title = {Digital Computer Electronics},
  author = {Malvino, Albert Paul.},
  year = {1977},
  publisher = {{Gregg Division, McGraw-Hill}},
  address = {{New York}}
}

@article{mamun2020,
  title = {Intra- and {{Inter}}-{{Server Smart Task Scheduling}} for {{Profit}} and {{Energy Optimization}} of {{HPC Data Centers}}},
  author = {Mamun, Sayed Ashraf and Gilday, Alexander and Singh, Amit Kumar and Ganguly, Amlan and Merrett, Geoff V. and Wang, Xiaohang and {Al-Hashimi}, Bashir M.},
  year = {2020},
  volume = {10},
  pages = {32},
  publisher = {{MDPI AG}},
  doi = {10.3390/jlpea10040032},
  abstract = {{$<$}jats:p{$>$}Servers in a data center are underutilized due to over-provisioning, which contributes heavily toward the high-power consumption of the data centers. Recent research in optimizing the energy consumption of High Performance Computing (HPC) data centers mostly focuses on consolidation of Virtual Machines (VMs) and using dynamic voltage and frequency scaling (DVFS). These approaches are inherently hardware-based, are frequently unique to individual systems, and often use simulation due to lack of access to HPC data centers. Other approaches require profiling information on the jobs in the HPC system to be available before run-time. In this paper, we propose a reinforcement learning based approach, which jointly optimizes profit and energy in the allocation of jobs to available resources, without the need for such prior information. The approach is implemented in a software scheduler used to allocate real applications from the Princeton Application Repository for Shared-Memory Computers (PARSEC) benchmark suite to a number of hardware nodes realized with Odroid-XU3 boards. Experiments show that the proposed approach increases the profit earned by 40\% while simultaneously reducing energy consumption by 20\% when compared to a heuristic-based approach. We also present a network-aware server consolidation algorithm called Bandwidth-Constrained Consolidation (BCC), for HPC data centers which can address the under-utilization problem of the servers. Our experiments show that the BCC consolidation technique can reduce the power consumption of a data center by up-to 37\%.},
  journal = {Journal of Low Power Electronics and Applications},
  number = {4},
  type = {Journal Article}
}

@article{mani2002,
  title = {{{SUMMAC}}: A Text Summarization Evaluation},
  author = {MANI, INDERJEET and KLEIN, GARY and HOUSE, DAVID and HIRSCHMAN, LYNETTE and FIRMIN, THERESE and SUNDHEIM, BETH},
  year = {2002},
  volume = {8},
  pages = {43--68},
  publisher = {{Cambridge University Press (CUP)}},
  doi = {10.1017/s1351324901002741},
  abstract = {{$<$}jats:p{$>$}The TIPSTER Text Summarization Evaluation (SUMMAC) has developed several new extrinsic and intrinsic methods for evaluating summaries. It has established definitively that automatic text summarization is very effective in relevance assessment tasks on news articles. Summaries as short as 17\% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in accuracy. Analysis of feedback forms filled in after each decision indicated that the intelligibility of present-day machine-generated summaries is high. Systems that performed most accurately in the production of indicative and informative topic-related summaries used term frequency and co-occurrence statistics, and vocabulary overlap comparisons between text passages. However, in the absence of a topic, these statistical methods do not appear to provide any additional leverage: in the case of generic summaries, the systems were indistinguishable in accuracy. The paper discusses some of the tradeoffs and challenges faced by the evaluation, and also lists some of the lessons learned, impacts, and possible future directions. The evaluation methods used in the},
  journal = {Natural Language Engineering},
  note = {Times cited: 48},
  number = {1},
  type = {Journal Article}
}

@book{manning2014,
  title = {The {{Stanford CoreNLP}} Natural Language Processing Toolkit},
  author = {Manning, Christopher D and Surdeanu, Mihai and Bauer, John and Finkel, Jenny Rose and Bethard, Steven and McClosky, David},
  year = {2014},
  volume = {Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations},
  abstract = {We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.}
}

@article{manning2020,
  title = {A {{Human Evaluation}} of {{AMR}}-to-{{English Generation Systems}}},
  author = {Manning, Emma and Wein, Shira and Schneider, Nathan},
  year = {2020},
  pages = {2004.06814v2},
  abstract = {Most current state-of-the art systems for generating English text from Abstract Meaning Representation (AMR) have been evaluated only using automated metrics, such as BLEU, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent AMR generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems.},
  journal = {arXiv},
  note = {COLING 2020},
  type = {Journal Article}
}

@book{mao2016,
  title = {Resource {{Management}} with {{Deep Reinforcement Learning Proceedings}} of the 15th {{ACM Workshop}} on {{Hot Topics}} in {{Networks}}},
  author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
  year = {2016},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA Atlanta, GA, USA}},
  abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for AI problems, we consider building systems that learn to manage resources directly from experience. We present DeepRM, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that DeepRM performs comparably to state-of-the-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.}
}

@book{matt2015,
  title = {From {{Word Embeddings To Document Distances ICML}}},
  author = {Matt, J. Kusner and Yu, Sun and Nicholas, I. Kolkin and Kilian, Q. Weinberger},
  year = {2015}
}

@article{maynez2020,
  title = {On {{Faithfulness}} and {{Factuality}} in {{Abstractive Summarization}}},
  author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  year = {2020},
  pages = {2005.00661v1},
  abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
  journal = {arXiv},
  note = {ACL 2020, 14 pages},
  type = {Journal Article}
}

@article{mcculloch1943,
  title = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
  author = {McCulloch, Warren S and Pitts, Walter},
  year = {1943},
  volume = {5},
  pages = {115--133},
  publisher = {{Springer}},
  abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical …},
  isbn = {1522-9602},
  journal = {The bulletin of mathematical biophysics},
  note = {Times cited: 20895},
  number = {4},
  type = {Journal Article}
}

@book{mieskes2020,
  title = {A {{Data Set}} for the {{Analysis}} of {{Text Quality Dimensions}} in {{Summarization Evaluation}}},
  author = {Mieskes, Margot and Mencía, Eneldo Loza and Kronsbein, Tim},
  year = {2020},
  volume = {Proceedings of The 12th Language Resources and Evaluation Conference}
}

@article{mihalcea2006,
  title = {Corpus-Based and Knowledge-Based Measures of Text Semantic Similarity},
  author = {Mihalcea, R and Corley, C and Strapparava, C},
  year = {2006},
  abstract = {This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (eg text classification, information retrieval) or individual words (eg synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (eg abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring …},
  journal = {Aaai},
  note = {Times cited: 1423},
  type = {Journal Article}
}

@article{mikolov2012,
  title = {Statistical Language Models Based on Neural Networks},
  author = {Mikolov, Tomáš},
  year = {2012},
  volume = {80},
  pages = {26},
  abstract = {Hidden layer s is orders of magnitude smaller (50-1000 neurons) U is the matrix of weights between input and hidden layer, V is the matrix of weights between hidden and output layer Without the recurrent weights W, this model would be a bigram neural network language model},
  journal = {Presentation at Google, Mountain View, 2nd April},
  note = {Times cited: 694},
  type = {Journal Article}
}

@article{mikolov2013,
  title = {Distributed {{Representations}} of {{Words}} and {{Phrases}} and Their {{Compositionality}}},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  pages = {1310.4546v1},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{miller2017,
  title = {{{ParlAI}}: {{A Dialog Research Software Platform}}},
  author = {Miller, Alexander H. and Feng, Will and Fisch, Adam and Lu, Jiasen and Batra, Dhruv and Bordes, Antoine and Parikh, Devi and Weston, Jason},
  year = {2017},
  pages = {1705.06476v4},
  abstract = {We introduce ParlAI (pronounced “par-lay”), an open-source software platform for dialog research implemented in Python, available at http://parl.ai. Its goal is to provide a unified framework for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others’ models, and improving upon existing architectures. Over 20 tasks are supported in the first release, including popular datasets such as SQuAD, bAbI tasks, MCTest, WikiQA, QACNN, QADailyMail, CBT, bAbI Dialog, Ubuntu, OpenSubtitles and VQA. Several models are integrated, including neural models such as memory networks, seq2seq and attentive LSTMs.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{ming2017,
  title = {Understanding {{Hidden Memories}} of {{Recurrent Neural Networks}}},
  author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
  year = {2017},
  volume = {cs.CL},
  abstract = {Recurrent neural networks (RNNs) have been successfully applied to various natural language processing (NLP) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing RNN models for NLP tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on RNNs’ hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an RNN’s hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.},
  note = {Published at IEEE Conference on Visual Analytics Science and Technology (IEEE VAST 2017)},
  type = {Journal Article}
}

@article{mnih2013,
  title = {Playing Atari with Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose …},
  archiveprefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1312.5602},
  note = {Times cited: 5934},
  type = {Journal Article}
}

@article{mohebbi2016,
  title = {Texts Semantic Similarity Detection Based Graph Approach.},
  author = {Mohebbi, M and Talebpour, A},
  year = {2016},
  abstract = {Similarity of text documents is important to analyze and extract useful information from text documents and generation of the appropriate data. Several cases of lexical matching techniques offered to determine the similarity between documents that have been successful to a certain limit and these methods are failing to find the semantic similarity between two texts. Therefore, the semantic similarity approaches were suggested, such as corpus-based methods and knowledge based methods eg, WordNet based methods. This paper, offers a …},
  journal = {Int. Arab J. Inf. Technol.},
  note = {Times cited: 4},
  type = {Journal Article}
}

@article{moratanch2016,
  title = {A Survey on Abstractive Text Summarization},
  author = {Moratanch, N and Chitrakala, S},
  year = {2016},
  pages = {1--7},
  publisher = {{IEEE}},
  abstract = {Text Summarization is the task of extracting salient information from the original text document. In this process, the extracted information is generated as a condensed report and presented as a concise summary to the user. It is very difficult for humans to understand and interpret the content of the text. In this paper, an exhaustive survey on abstractive text summarization methods has been presented. The two broad abstractive summarization methods are structured based approach and semantic based approach. This paper …},
  isbn = {150901277X},
  journal = {ieeexplore.ieee.org},
  type = {Journal Article}
}

@incollection{mozer1992,
  title = {Induction of {{Multiscale Temporal Structure Advances}} in {{Neural Information Processing Systems}} 4},
  author = {Mozer, Michael C},
  editor = {J., E. Moody and S., J. Hanson and R., P. Lippmann},
  year = {1992},
  pages = {275--282},
  publisher = {{Morgan-Kaufmann}}
}

@article{nallapati2016,
  title = {{{SummaRuNNer}}: {{A Recurrent Neural Network}} Based {{Sequence Model}} for {{Extractive Summarization}} of {{Documents}}},
  author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
  year = {2016},
  pages = {1611.04230v1},
  abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
  journal = {arXiv},
  note = {Published at AAAI 2017, The Thirty-First AAAI Conference on Artificial Intelligence (AAAI-2017)},
  type = {Journal Article}
}

@article{nallapati2016a,
  title = {Abstractive {{Text Summarization Using Sequence}}-to-{{Sequence RNNs}} and {{Beyond}}},
  author = {Nallapati, Ramesh and Zhou, Bowen and dos {santos}, Cicero Nogueira and Gulcehre, Caglar and Xiang, Bing},
  year = {2016},
  pages = {1602.06023v5},
  abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
  journal = {arXivThe SIGNLL Conference on Computational Natural Language Learning (CoNLL), 2016},
  type = {Journal Article}
}

@article{narayan2018,
  title = {Ranking Sentences for Extractive Summarization with Reinforcement Learning},
  author = {Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  year = {2018},
  abstract = {Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and …},
  archiveprefix = {arXiv},
  eprint = {1802.08636},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1802.08636},
  note = {Times cited: 268},
  type = {Journal Article}
}

@article{narayan2018a,
  title = {Don’t {{Give Me}} the {{Details}}, {{Just}} the {{Summary}}! {{Topic}}-{{Aware Convolutional Neural Networks}} for {{Extreme Summarization}}},
  author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
  year = {2018},
  pages = {1808.08745v1},
  abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
  journal = {arXiv},
  note = {11, 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
  type = {Journal Article}
}

@article{nath2019,
  title = {Training {{Recurrent Neural Networks Online}} by {{Learning Explicit State Variables}}},
  author = {Nath, S and Liu, V and Chan, A and Li, X and White…, A},
  year = {2019},
  abstract = {Recurrent neural networks (RNNs) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an RNN: the sensitivity of the learning algorithm’s performance to truncation length and and long training times. There are variety of strategies to improve training in RNNs, the mostly notably Backprop Through Time (BPTT) and by Real-Time Recurrent Learning. These strategies, however, are typically …},
  journal = {… Conference on Learning …},
  type = {Journal Article}
}

@book{nenkova2004,
  title = {Evaluating Content Selection in Summarization: {{The}} Pyramid Method},
  author = {Nenkova, Ani and Passonneau, Rebecca J},
  year = {2004},
  volume = {Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-naacl 2004},
  abstract = {We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.‏}
}

@incollection{nenkova2012,
  title = {A {{Survey}} of {{Text Summarization Techniques}}},
  author = {Nenkova, Ani and McKeown, Kathleen},
  year = {2012},
  pages = {43--76},
  publisher = {{Springer US}},
  address = {{Boston, MA}}
}

@article{ng2015,
  title = {Better Summarization Evaluation with Word Embeddings for Rouge},
  author = {Ng, Jun-Ping and Abrecht, Viktoria},
  year = {2015},
  archiveprefix = {arXiv},
  eprint = {1508.06034},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1508.06034},
  type = {Journal Article}
}

@article{nova2019,
  title = {{{STRASS}}: {{A Light}} and {{Effective Method}} for {{Extractive Summarization Based}} on {{Sentence Embeddings}}},
  author = {NOVA, EURA},
  year = {2019},
  volume = {1},
  pages = {243},
  abstract = {This paper introduces STRASS: Summarization by TRAnsformation Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model learns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the transformation is only …},
  journal = {ACL 2019},
  number = {2},
  type = {Journal Article}
}

@article{novikova2017,
  title = {Why {{We Need New Evaluation Metrics}} for {{NLG}}},
  author = {Novikova, Jekaterina and Dušek, Ondřej and Curry, Amanda Cercas and Rieser, Verena},
  year = {2017},
  pages = {1707.06875v1},
  abstract = {The majority of NLG evaluation relies on automatic metrics, such as BLEU. In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end NLG. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.},
  journal = {arXivProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2231-2242, Copenhagen, Denmark, September 7-11, 2017},
  note = {accepted to EMNLP 2017},
  type = {Journal Article}
}

@misc{opidi2019,
  title = {A Gentle Introduction to Text Summarization in Machine Learning},
  author = {Opidi, Alfrick},
  year = {2019},
  month = sep
}

@article{opitz1999,
  title = {Popular {{Ensemble Methods}}: {{An Empirical Study}}},
  author = {Opitz, D. and Maclin, R.},
  year = {1999},
  volume = {11},
  pages = {169--198},
  publisher = {{AI Access Foundation}},
  doi = {10.1613/jair.614},
  abstract = {{$<$}jats:p{$>$}An ensemble consists of a set of individually trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman, 1996c) and Boosting (Freund \&amp; Shapire, 1996; Shapire, 1990) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm. Our results clearly indicate a number of conclusions. First, while Bagging is almost always more accurate than a single classifier, it is sometimes much less accurate than Boosting. On the other hand, Boosting can create ensembles that are less accurate than a single classifier – especially when using neural networks. Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined. In fact, further results show that Boosting ensembles may overfit noisy data sets, thus decreasing its performance. Finally, consistent with previous studies, our work suggests that most of the gain in an ensemble’s performance comes in the first few classifiers combined; however, relatively large gains can be seen up to 25 classifiers when Boosting decision trees.},
  journal = {Journal of Artificial Intelligence Research},
  note = {Times cited: 1202},
  type = {Journal Article}
}

@article{opitz2019,
  title = {Automatic Summarisation: 25 Years {{On}}},
  author = {Opitz, Juri and Parcalabescu, Letitia and Frank, Anette and Orăsan, Constantin},
  year = {2019},
  volume = {25},
  pages = {735--751},
  publisher = {{Cambridge University Press}},
  abstract = {Automatic text summarisation is a topic that has been receiving attention from the research community from the early days of computational linguistics, but it really took off around 25 years ago. This article presents the main developments from the last 25 years. It starts by defining what a summary is and how its definition changed over time as a result of the interest in processing new types of documents. The article continues with a brief history of the field and highlights the main challenges posed by the evaluation of summaries. The …},
  isbn = {1351-3249},
  journal = {Natural Language Engineering},
  note = {Times cited: 1},
  number = {6},
  type = {Journal Article}
}

@article{opitz2020,
  title = {{{AMR Quality Rating}} with a {{Lightweight CNN}}},
  author = {Opitz, Juri},
  year = {2020},
  pages = {2005.12187v2},
  abstract = {Structured semantic sentence representations such as Abstract Meaning Representations (AMRs) are potentially useful in various NLP tasks. However, the quality of automatic parses can vary greatly and jeopardizes their usefulness. This can be mitigated by models that can accurately rate AMR quality in the absence of costly gold data, allowing us to inform downstream systems about an incorporated parse’s trustworthiness or select among different candidate parses. In this work, we propose to transfer the AMR graph to the domain of images. This allows us to create a simple convolutional neural network (CNN) that imitates a human judge tasked with rating graph quality. Our experiments show that the method can rate quality more accurately than strong baselines, in several quality dimensions. Moreover, the method proves to be efficient and reduces the incurred energy consumption.},
  journal = {arXiv},
  note = {AACL-IJCNLP 2020},
  type = {Journal Article}
}

@article{ott2019,
  title = {Fairseq: {{A Fast}}, {{Extensible Toolkit}} for {{Sequence Modeling}}},
  author = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
  year = {2019},
  pages = {1904.01038v1},
  abstract = {fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto},
  journal = {arXiv},
  note = {NAACL 2019 Demo paper},
  type = {Journal Article}
}

@article{otter2018,
  title = {A {{Survey}} of the {{Usages}} of {{Deep Learning}} in {{Natural Language Processing}}},
  author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
  year = {2018},
  pages = {1807.10854v3},
  abstract = {Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{pang2008,
  title = {Opinion {{Mining}} and {{Sentiment Analysis}}},
  author = {Pang, Bo and Lee, Lillian},
  year = {2008},
  volume = {2},
  pages = {1--135},
  publisher = {{Now Publishers Inc.}},
  address = {{Hanover, MA, USA}},
  doi = {10.1561/1500000011},
  abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
  isbn = {1554-0669},
  journal = {Found. Trends Inf. Retr.},
  number = {1–2},
  type = {Journal Article}
}

@book{papineni2001,
  title = {Neural {{AMR}}: {{Sequence}}-to-{{Sequence Models}} for {{Parsing}} and {{Generation}}},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2001},
  publisher = {{Association for Computational Linguistics}},
  address = {{Morristown, NJ, USA}}
}

@book{papineni2002,
  title = {{{BLEU}}: A Method for Automatic Evaluation of Machine Translation},
  author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  year = {2002},
  volume = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1}
}

@article{pascanu2013,
  title = {How to {{Construct Deep Recurrent Neural Networks}}},
  author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2013},
  pages = {1312.6026v5},
  abstract = {In this paper, we explore different ways to extend a recurrent neural network (RNN) to a \emph{deep} RNN. We start by arguing that the concept of depth in an RNN is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an RNN, however, we find three points of an RNN which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep RNN which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep RNN (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep RNNs using a novel framework based on neural operators. The proposed deep RNNs are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep RNNs benefit from the depth and outperform the conventional, shallow RNNs.},
  journal = {arXiv},
  note = {Accepted at ICLR 2014 (Conference Track). 10-page text + 3-page references},
  type = {Journal Article}
}

@article{pascanu2013a,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  year = {2013},
  pages = {1310--1318},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al.(1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients …},
  journal = {jmlr.org},
  type = {Journal Article}
}

@article{pasunuru2018,
  title = {Multi-Reward Reinforced Summarization with Saliency and Entailment},
  author = {Pasunuru, Ramakanth and Bansal, Mohit},
  year = {2018},
  abstract = {Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: ROUGESal and Entail, on top of a coverage-based baseline. The ROUGESal reward modifies the ROUGE metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward …},
  archiveprefix = {arXiv},
  eprint = {1804.06451},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1804.06451},
  note = {Times cited: 89},
  type = {Journal Article}
}

@article{paulus2017,
  title = {A Deep Reinforced Model for Abstractive Summarization},
  author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
  year = {2017},
  abstract = {Attentional, RNN-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We …},
  archiveprefix = {arXiv},
  eprint = {1705.04304},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1705.04304},
  note = {Times cited: 723},
  type = {Journal Article}
}

@article{pedregosa2011,
  title = {Scikit-Learn: {{Machine Learning}} in {{Python}}},
  author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and {and Thirion}, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and {and Weiss}, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and {and Cournapeau}, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
  year = {2011},
  volume = {12},
  pages = {2825--2830},
  journal = {Journal of Machine Learning Research},
  type = {Journal Article}
}

@article{peters2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {1802.05365v2},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  journal = {arXiv},
  note = {NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready},
  type = {Journal Article}
}

@book{peyrard2017,
  title = {Learning to {{Score System Summaries}} for {{Better Content Selection Evaluation}}.},
  author = {Peyrard, Maxime and Botschen, Teresa and Gurevych, Iryna},
  year = {2017},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@book{peyrard2019,
  title = {Studying {{Summarization Evaluation Metrics}} in the {{Appropriate Scoring Range}}},
  author = {Peyrard, Maxime},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@book{popovic2015,
  title = {{{chrF}}: Character n-Gram {{F}}-Score for Automatic {{MT}} Evaluation},
  author = {Popović, Maja},
  year = {2015},
  volume = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  abstract = {We propose the use of character n-gram F-score for automatic evaluation of machine translation output. Character ngrams have already been used as a part of more complex metrics, but their individual potential has not been investigated yet. We report system-level correlations with human rankings for 6-gram F1-score (CHRF) on the WMT12, WMT13 and WMT14 data as well as segment-level correlation for 6-gram F1 (CHRF) and F3-scores (CHRF3) on WMT14 data for all available target languages. The results are very promising …}
}

@article{qazvinian2008,
  title = {Scientific {{Paper Summarization Using Citation Summary Networks}}},
  author = {Qazvinian, Vahed and Radev, Dragomir R.},
  year = {2008},
  pages = {0807.1560v1},
  abstract = {Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others’ viewpoint of the target article’s contributions and the study of its citation summary network using a clustering approach.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{radev2002,
  title = {Introduction to the Special Issue on Summarization},
  author = {Radev, Dragomir R and Hovy, Eduard and McKeown, Kathleen},
  year = {2002},
  volume = {28},
  pages = {399--408},
  publisher = {{MIT Press}},
  abstract = {As the amount of on-line information increases, systems that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia …},
  isbn = {0891-2017},
  journal = {Computational linguistics},
  note = {Times cited: 603},
  number = {4},
  type = {Journal Article}
}

@article{raffel2019,
  title = {Exploring the {{Limits}} of {{Transfer Learning}} with a {{Unified Text}}-to-{{Text Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2019},
  pages = {1910.10683v3},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus’’, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  journal = {arXiv},
  note = {Final version as published in JMLR},
  type = {Journal Article}
}

@book{raganato2018,
  title = {An Analysis of Encoder Representations in Transformer-Based Machine Translation},
  author = {Raganato, Alessandro and Tiedemann, Jörg},
  year = {2018},
  volume = {Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  publisher = {{The Association for Computational Linguistics}},
  abstract = {The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the …‏}
}

@article{rankel2012,
  title = {Better {{Metrics}} to {{Automatically Predict}} the {{Quality}} of a {{Text Summary}}},
  author = {Rankel, Peter A. and Conroy, John M. and Schlesinger, Judith D.},
  year = {2012},
  volume = {5},
  pages = {398--420},
  publisher = {{MDPI AG}},
  doi = {10.3390/a5040398},
  abstract = {{$<$}jats:p{$>$}In this paper we demonstrate a family of metrics for estimating the quality of a text summary relative to one or more human-generated summaries. The improved metrics are based on features automatically computed from the summaries to measure content and linguistic quality. The features are combined using one of three methods—robust regression, non-negative least squares, or canonical correlation, an eigenvalue method. The new metrics significantly outperform the previous standard for automatic text summarization evaluation, ROUGE.},
  journal = {Algorithms},
  note = {Times cited: 4},
  number = {4},
  type = {Journal Article}
}

@book{ray2018,
  title = {Opportunistic {{Power Savings}} with {{Coordinated Control}} in {{Data Center Networks}}},
  author = {Ray, Madhurima and Sondur, Sanjeev and Biswas, Joyanta and Pal, Amitangshu and Kant, Krishna},
  year = {2018},
  publisher = {{ACM}},
  address = {{New York, NY, USA}}
}

@article{reiter2018,
  title = {A Structured Review of the Validity of {{BLEU}}},
  author = {Reiter, Ehud},
  year = {2018},
  volume = {44},
  pages = {393--401},
  publisher = {{MIT Press}},
  abstract = {The BLEU metric has been widely used in NLP for over 15 years to evaluate NLP systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique—in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally …‏},
  isbn = {1530-9312},
  journal = {Computational Linguistics},
  number = {3},
  type = {Journal Article}
}

@article{ribeiro2019,
  title = {Enhancing {{AMR}}-to-{{Text Generation}} with {{Dual Graph Representations}}},
  author = {Ribeiro, Leonardo F. R. and Gardent, Claire and Gurevych, Iryna},
  year = {2019},
  pages = {1909.00352v1},
  abstract = {Generating text from graph-based data, such as Abstract Meaning Representation (AMR), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the AMR graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in AMR-to-text generation, achieving state-of-the-art results on two AMR datasets.},
  journal = {arXiv},
  note = {Accepted as a long conference paper to EMNLP 2019},
  type = {Journal Article}
}

@article{rumelhart1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year = {1986},
  volume = {323},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create …},
  isbn = {1476-4687},
  journal = {nature},
  note = {Times cited: 22676},
  number = {6088},
  type = {Journal Article}
}

@article{rush2015,
  title = {A {{Neural Attention Model}} for {{Abstractive Sentence Summarization}}},
  author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
  year = {2015},
  pages = {1509.00685v2},
  abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
  journal = {arXiv},
  note = {Proceedings of EMNLP 2015},
  type = {Journal Article}
}

@incollection{saggion2013,
  title = {Automatic Text Summarization: {{Past}}, Present and Future},
  author = {Saggion, Horacio and Poibeau, Thierry},
  year = {2013},
  pages = {3--21},
  publisher = {{Springer}},
  abstract = {Automatic text summarization, the computer-based production of condensed versions of documents, is an important technology for the information society. Without summaries it would be practically impossible for human beings to get access to the ever growing mass of information available online. Although research in text summarization is over 50 years old, some efforts are still needed given the insufficient quality of automatic summaries and the number of interesting summarization topics being proposed in different contexts by end …}
}

@book{samira2013,
  title = {An {{Evaluation Summary Method Based}} on a {{Combination}} of {{Content}} and {{Linguistic Metrics RANLP}}},
  author = {Samira, Ellouze and M., Jaoua and L., Belguith},
  year = {2013}
}

@article{samira2017,
  title = {Merging {{Multiple Features}} to {{Evaluate}} the {{Content}} of {{Text Summary}}},
  author = {Samira, Ellouze and M., Jaoua and L., Belguith},
  year = {2017},
  volume = {58},
  pages = {69--76},
  journal = {Proces. del Leng. Natural},
  type = {Journal Article}
}

@article{santhanam2019,
  title = {A {{Survey}} of {{Natural Language Generation Techniques}} with a {{Focus}} on {{Dialogue Systems}} - {{Past}}, {{Present}} and {{Future Directions}}},
  author = {Santhanam, Sashank and Shaikh, Samira},
  year = {2019},
  pages = {1906.00500v1},
  abstract = {One of the hardest problems in the area of Natural Language Processing and Artificial Intelligence is automatically generating language that is coherent and understandable to humans. Teaching machines how to converse as humans do falls under the broad umbrella of Natural Language Generation. Recent years have seen unprecedented growth in the number of research articles published on this subject in conferences and journals both by academic and industry researchers. There have also been several workshops organized alongside top-tier NLP conferences dedicated specifically to this problem. All this activity makes it hard to clearly define the state of the field and reason about its future directions. In this work, we provide an overview of this important and thriving area, covering traditional approaches, statistical approaches and also approaches that use deep neural networks. We provide a comprehensive review towards building open domain dialogue systems, an important application of natural language generation. We find that, predominantly, the approaches for building dialogue systems use seq2seq or language models architecture. Notably, we identify three important areas of further research towards building more effective dialogue systems: 1) incorporating larger context, including conversation context and world knowledge; 2) adding personae or personality in the NLG system; and 3) overcoming dull and generic responses that affect the quality of system-produced responses. We provide pointers on how to tackle these open problems through the use of cognitive architectures that mimic human language understanding and generation capabilities.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{schmidhuber2014,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  author = {Schmidhuber, Juergen},
  year = {2014},
  pages = {1404.7828v4},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  journal = {arXivNeural Networks, Vol 61, pp 85-117, Jan 2015},
  note = {88 pages, 888 references},
  type = {Journal Article}
}

@article{sebastian2018,
  title = {{{MLxtend}}: {{Providing}} Machine Learning and Data Science Utilities and Extensions to {{Python}}’s Scientific Computing Stack},
  author = {Sebastian, Raschka},
  year = {2018},
  volume = {3},
  publisher = {{The Open Journal}},
  doi = {10.21105/joss.00638},
  journal = {The Journal of Open Source Software},
  number = {24},
  type = {Journal Article}
}

@article{sedgwick2014,
  title = {Spearman’s Rank Correlation Coefficient},
  author = {Sedgwick, Philip},
  year = {2014},
  volume = {349},
  publisher = {{BMJ Publishing Group Ltd}},
  doi = {10.1136/bmj.g7327},
  isbn = {0959-8138},
  journal = {BMJ},
  type = {Journal Article}
}

@article{see2017,
  title = {Get {{To The Point}}: {{Summarization}} with {{Pointer}}-{{Generator Networks}}},
  author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
  year = {2017},
  pages = {1704.04368v2},
  abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
  journal = {arXiv},
  note = {Add METEOR evaluation results, add some citations, fix some equations (what are now equations 1, 8 and 11 were missing a bias term), fix url to pyrouge package, add acknowledgments},
  type = {Journal Article}
}

@article{sellam2020,
  title = {{{BLEURT}}: {{Learning Robust Metrics}} for {{Text Generation}}},
  author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P.},
  year = {2020},
  pages = {2004.04696v5},
  abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., BLEU and ROUGE) may correlate poorly with human judgments. We propose BLEURT, a learned evaluation metric based on BERT that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. BLEURT provides state-of-the-art results on the last three years of the WMT Metrics shared task and the WebNLG Competition dataset. In contrast to a vanilla BERT-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
  journal = {arXiv},
  note = {Accepted at ACL 2020},
  type = {Journal Article}
}

@book{sevastjanova2018,
  title = {Going beyond Visualization: {{Verbalization}} as Complementary Medium to Explain Machine Learning Models},
  author = {Sevastjanova, Rita and Beck, Fabian and Ell, Basil and Turkay, Cagatay and Henkin, Rafael and Butt, Miriam and Keim, Daniel A and {El-Assady}, Mennatallah},
  year = {2018},
  volume = {Workshop on Visualization for AI Explainability at IEEE VIS},
  abstract = {In this position paper, we argue that a combination of visualization and verbalization techniques is beneficial for creating broad and versatile insights into the structure and decision-making processes of machine learning models. Explainability of machine learning models is emerging as an important area of research. Hence, insights into the inner workings of a trained model allow users and analysts, alike, to understand the models, develop justifications, and gain trust in the systems they inform. Explanations can be …}
}

@article{sharma2019,
  title = {An Entity-Driven Framework for Abstractive Summarization},
  author = {Sharma, Eva and Huang, Luyang and Hu, Zhe and Wang, Lu},
  year = {2019},
  abstract = {Abstractive summarization systems aim to produce more coherent and concise summaries than their extractive counterparts. Popular neural models have achieved impressive results for single-document summarization, yet their outputs are often incoherent and unfaithful to the input. In this paper, we introduce SENECA, a novel System for ENtity-drivEn Coherent Abstractive summarization framework that leverages entity information to generate informative and coherent abstracts. Our framework takes a two-step approach:(1) an entity …},
  archiveprefix = {arXiv},
  eprint = {1909.02059},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1909.02059},
  note = {Times cited: 17},
  type = {Journal Article}
}

@article{shi2018,
  title = {Neural {{Abstractive Text Summarization}} with {{Sequence}}-to-{{Sequence Models}}},
  author = {Shi, Tian and Keneshloo, Yaser and Ramakrishnan, Naren and Reddy, Chandan K.},
  year = {2018},
  pages = {1812.02303v4},
  abstract = {In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efficiency and parallelism for training a model. In this paper, we provide a comprehensive literature survey on different seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were first proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer (NATS) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used CNN/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in NATS on the two recently released datasets, namely, Newsroom and Bytecup.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{shi2019,
  title = {{{LeafNATS}}: {{An Open}}-{{Source Toolkit}} and {{Live Demo System}} for {{Neural Abstractive Text Summarization}}},
  author = {Shi, Tian and Wang, Ping and Reddy, Chandan K.},
  year = {2019},
  pages = {1906.01512v1},
  abstract = {Neural abstractive text summarization (NATS) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely LeafNATS, for training and evaluation of different sequence-to-sequence based models for the NATS task, and for deploying the pre-trained models to real-world applications. The toolkit is modularized and extensible in addition to maintaining competitive performance in the NATS task. A live news blogging system has also been implemented to demonstrate how these models can aid blog/news editors by providing them suggestions of headlines and summaries of their articles.},
  journal = {arXiv},
  note = {Accepted by NAACL-HLT 2019 demo track},
  type = {Journal Article}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc},
  year = {2016},
  volume = {529},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value …},
  isbn = {1476-4687},
  journal = {nature},
  note = {Times cited: 9947},
  number = {7587},
  type = {Journal Article}
}

@book{singh2016,
  title = {Value and {{Energy Aware Adaptive Resource Allocation}} of {{Soft Real}}-{{Time Jobs}} on {{Many}}-{{Core HPC Data Centers}}},
  author = {Singh, Amit Kumar and Dziurzanski, Piotr and Indrusiak, Leandro Soares},
  year = {2016},
  publisher = {{IEEE}}
}

@book{sipos2013,
  title = {Generating Comparative Summaries from Reviews},
  author = {Sipos, Ruben and Joachims, Thorsten},
  year = {2013},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}}
}

@phdthesis{sloman1962,
  title = {Knowing and {{Understanding}}: {{Relations}} between Meaning and Truth, Meaning and Necessary Truth, Meaning and Synthetic Necessary Truth},
  author = {Sloman, Aaron},
  year = {1962},
  abstract = {The avowed aim of the thesis is to show that there are some synthetic necessary truths, or that synthetic apriori knowledge is possible. This is really a pretext for an investigation into the general connection between meaning and truth, or between understanding and …},
  school = {University of Oxford},
  type = {{{PhD Thesis}}}
}

@article{song2018,
  title = {A {{Graph}}-to-{{Sequence Model}} for {{AMR}}-to-{{Text Generation}}},
  author = {Song, Linfeng and Zhang, Yue and Wang, Zhiguo and Gildea, Daniel},
  year = {2018},
  pages = {1805.02473v3},
  abstract = {The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.},
  journal = {arXiv},
  note = {ACL 2018 camera-ready, Proceedings of ACL 2018 with updated performance},
  type = {Journal Article}
}

@article{song2019,
  title = {Controlling the {{Amount}} of {{Verbatim Copying}} in {{Abstractive Summarization}}},
  author = {Song, Kaiqiang and Wang, Bingqing and Feng, Zhe and Ren, Liu and Liu, Fei},
  year = {2019},
  pages = {1911.10390v1},
  abstract = {An abstract must not change the meaning of the original text. A single most effective way to achieve that is to increase the amount of copying while still allowing for text abstraction. Human editors can usually exercise control over copying, resulting in summaries that are more extractive than abstractive, or vice versa. However, it remains poorly understood whether modern neural abstractive summarizers can provide the same flexibility, i.e., learning from single reference summaries to generate multiple summary hypotheses with varying degrees of copying. In this paper, we present a neural summarization model that, by learning from single human abstracts, can produce a broad spectrum of summaries ranging from purely extractive to highly generative ones. We frame the task of summarization as language modeling and exploit alternative mechanisms to generate summary hypotheses. Our method allows for control over copying during both training and decoding stages of a neural summarization model. Through extensive experiments we illustrate the significance of our proposed method on controlling the amount of verbatim copying and achieve competitive results over strong baselines. Our analysis further reveals interesting and unobvious facts.},
  journal = {arXiv},
  note = {AAAI 2020 (Main Technical Track)},
  type = {Journal Article}
}

@article{staudemeyer2019,
  title = {Understanding {{LSTM}} - a Tutorial into {{Long Short}}-{{Term Memory Recurrent Neural Networks}}},
  author = {Staudemeyer, Ralf C. and Morris, Eric Rothstein},
  year = {2019},
  pages = {1909.09586v1},
  abstract = {Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the most powerful dynamic classifiers publicly known. The network itself and the related learning algorithms are reasonably well documented to get an idea how it works. This paper will shed more light into understanding how LSTM-RNNs evolved and why they work impressively well, focusing on the early, ground-breaking publications. We significantly improved documentation and fixed a number of errors and inconsistencies that accumulated in previous publications. To support understanding we as well revised and unified the notation used.},
  journal = {arXiv},
  note = {42 pages, 11 figures, tutorial},
  type = {Journal Article}
}

@article{subramanian2019,
  title = {On {{Extractive}} and {{Abstractive Neural Document Summarization}} with {{Transformer Language Models}}},
  author = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher},
  year = {2019},
  pages = {1909.03186v2},
  abstract = {We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{suleiman2020,
  title = {Deep {{Learning Based Abstractive Text Summarization}}: {{Approaches}}, {{Datasets}}, {{Evaluation Measures}}, and {{Challenges}}},
  author = {Suleiman, Dima and Awajan, Arafat},
  year = {2020},
  volume = {2020},
  pages = {1--29},
  publisher = {{Hindawi Limited}},
  doi = {10.1155/2020/9365340},
  abstract = {In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and analysing information. To retrieve useful knowledge within a reasonable time period, this information must be summarised. This paper reviews recent approaches for abstractive text summarisation using deep learning models. In addition, existing datasets for training and validating these approaches are reviewed, and their features and limitations are presented. The Gigaword dataset is commonly employed for single-sentence summary approaches, while the Cable News Network (CNN)/Daily Mail dataset is commonly employed for multisentence summary approaches. Furthermore, the measures that are utilised to evaluate the quality of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 (ROUGE1), ROUGE2, and ROUGE-L are determined to be the most commonly applied metrics. The challenges that are encountered during the summarisation process and the solutions proposed in each approach are analysed. The analysis of the several approaches shows that recurrent neural networks with an attention mechanism and long short-term memory (LSTM) are the most prevalent techniques for abstractive text summarisation. The experimental results show that text summarisation with a pretrained encoder model achieved the highest values for ROUGE1, ROUGE2, and ROUGE-L (43.85, 20.34, and 39.9, respectively). Furthermore, it was determined that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, out-of-vocabulary (OOV) words, summary sentence repetition, inaccurate sentences, and fake facts.},
  journal = {Mathematical Problems in Engineering},
  keywords = {General Engineering,General Mathematics},
  type = {Journal Article}
}

@book{sutskever2011,
  title = {Generating Text with Recurrent Neural Networks},
  author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
  year = {2011},
  volume = {ICML},
  abstract = {Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks …}
}

@book{sutskever2014,
  title = {Sequence to Sequence Learning with Neural Networks},
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  year = {2014},
  volume = {Advances in neural information processing systems},
  abstract = {Abstract Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then …}
}

@book{sutton2018,
  title = {Reinforcement Learning: {{An}} Introduction},
  author = {Sutton, Richard S and Barto, Andrew G},
  year = {2018},
  publisher = {{MIT press}},
  abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational …}
}

@book{takase2016,
  title = {Neural Headline Generation on Abstract Meaning Representation},
  author = {Takase, Sho and Suzuki, Jun and Okazaki, Naoaki and Hirao, Tsutomu and Nagata, Masaaki},
  year = {2016},
  volume = {Proceedings of the 2016 conference on empirical methods in natural language processing},
  abstract = {Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation (AMR) parser using a modified version of Tree-LSTM. Our proposed attention-based AMR encoder-decoder model improves headline generation benchmarks compared …}
}

@article{tallec2018,
  title = {Can Recurrent Neural Networks Warp Time},
  author = {Tallec, Corentin and Ollivier, Yann},
  year = {2018},
  pages = {1804.11188v1},
  abstract = {Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in LSTMs and GRUs. Ex- perimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.},
  journal = {arXiv},
  type = {Journal Article}
}

@book{tan2017,
  title = {Abstractive {{Document Summarization}} with a {{Graph}}-{{Based Attentional Neural Model}}},
  author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
  year = {2017},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@book{tan2017a,
  title = {An {{Energy}}-Aware {{Virtual Machine Placement Algorithm}} in {{Cloud Data Center}}},
  author = {Tan, Mingzhe and Chi, Ce and Zhang, Jiahao and Zhao, Shichang and Li, Guangli and Lü, Shuai},
  year = {2017},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}}
}

@book{tan2019,
  title = {Introduction to {{Data Mining}}},
  author = {Tan, P.N. and Steinbach, M. and Karpatne, A. and Kumar, V.},
  year = {2019},
  publisher = {{Pearson}}
}

@article{tas2017,
  title = {A Survey Automatic Text Summarization},
  author = {Tas, Oguzhan and Kiyani, Farzad},
  year = {2017},
  volume = {5},
  pages = {205--213},
  publisher = {{Pressacademia}},
  doi = {10.17261/pressacademia.2017.591},
  journal = {Pressacademia},
  note = {Times cited: 3},
  number = {1},
  type = {Journal Article}
}

@article{tian2014,
  title = {{{UM}}-{{Corpus}}: {{A Large English}}-{{Chinese Parallel Corpus}} for {{Statistical Machine Translation}}.},
  author = {Tian, Liang and Wong, Derek F and Chao, Lidia S and Quaresma, Paulo and Oliveira, Francisco and Yi, Lu},
  year = {2014},
  pages = {1837--1842},
  abstract = {Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems, especially for Statistical Machine Translation (SMT). However, most existing parallel corpora to Chinese are subject to in-house use, while others are domain specific and limited in size. To a certain degree, this limits the SMT research. This paper describes the acquisition of a large scale and high quality parallel corpora for English and Chinese. The corpora constructed in this paper contain about 15 …},
  journal = {researchgate.net},
  type = {Journal Article}
}

@book{toutanova2003,
  title = {Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network},
  author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
  year = {2003},
  volume = {Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics},
  abstract = {We present a new part-of-speech tagger that demonstrates the following ideas:(i) explicit use of both preceding and following tag contexts via a dependency network representation,(ii) broad use of lexical features, including jointly conditioning on multiple …}
}

@article{vasilyev2020,
  title = {Is Human Scoring the Best Criteria for Summary Evaluation},
  author = {Vasilyev, Oleg and Bohannon, John},
  year = {2020},
  pages = {2012.14602v1},
  abstract = {Normally, summary quality measures are compared with quality scores produced by human annotators. A higher correlation with human scores is considered to be a fair indicator of a better measure. We discuss observations that cast doubt on this view. We attempt to show a possibility of an alternative indicator. Given a family of measures, we explore a criterion of selecting the best measure not relying on correlations with human scores. Our observations for the BLANC family of measures suggest that the criterion is universal across very different styles of summaries.},
  journal = {arXiv},
  note = {7 pages, 5 figures, 1 table},
  type = {Journal Article}
}

@book{vaswani2017,
  title = {Attention Is All You Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  year = {2017},
  volume = {Advances in neural information processing systems},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm …}
}

@book{vedantam2015,
  title = {{{CIDEr}}: {{Consensus}}-Based Image Description Evaluation},
  author = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
  year = {2015},
  publisher = {{IEEE}}
}

@article{vig2019,
  title = {A {{Multiscale Visualization}} of {{Attention}} in the {{Transformer Model}}},
  author = {Vig, Jesse},
  year = {2019},
  pages = {1906.05714v1},
  abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
  journal = {arXiv},
  note = {To appear in ACL 2019 (System Demonstrations). arXiv admin note: substantial text overlap with arXiv:1904.02679},
  type = {Journal Article}
}

@article{vig2019a,
  title = {Analyzing the {{Structure}} of {{Attention}} in a {{Transformer Language Model}}},
  author = {Vig, Jesse and Belinkov, Yonatan},
  year = {2019},
  pages = {1906.04284v2},
  abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
  journal = {arXiv},
  note = {To appear in ACL BlackboxNLP workshop},
  type = {Journal Article}
}

@book{vinyals2015,
  title = {Pointer Networks},
  author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
  year = {2015},
  volume = {Advances in neural information processing systems},
  abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in eachstep of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorialoptimization problems …}
}

@article{virtanen2020,
  title = {{{SciPy}} 1.0: {{Fundamental Algorithms}} for {{Scientific Computing}} in {{Python}}},
  author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and {and Haberland}, Matt and Reddy, Tyler and Cournapeau, David and {and Burovski}, Evgeni and Peterson, Pearu and Weckesser, Warren and {and Bright}, Jonathan and {van der Walt}, Stefan J. and {and Brett}, Matthew and Wilson, Joshua and Millman, K. Jarrod and {and Mayorov}, Nikolay and Nelson, Andrew R. J. and Jones, Eric and {and Kern}, Robert and Larson, Eric and Carey, C J and {and Polat}, Ilhan and Feng, Yu and Moore, Eric W. and {and VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and {and Cimrman}, Robert and Henriksen, Ian and Quintero, E. A. and {and Harris}, Charles R. and Archibald, Anne M. and {and Ribeiro}, Antonio H. and Pedregosa, Fabian and {and van Mulbregt}, Paul},
  year = {2020},
  volume = {17},
  pages = {261--272},
  doi = {10.1038/s41592-019-0686-2},
  journal = {Nature Methods},
  type = {Journal Article}
}

@article{vu2010,
  title = {Towards Automated Related Work Summarization},
  author = {Vu, HCD},
  year = {2010},
  abstract = {This thesis introduces and describes the novel problem of automated related work summarization. Given multiple articles (eg, conference or journal papers) as input, and a set of keywords that describes a target paper? s topics of interest in a hierarchical fashion, a related work summarization system creates a topic-biased summary of related work specific to the target paper. This thesis has two main contributions. First, I conducted a deep manual analysis on various aspects of related work sections to identify their important characteristics …},
  journal = {scholarbank.nus.edu.sg},
  note = {Times cited: 3},
  type = {Journal Article}
}

@article{w.1958,
  title = {Ordinal {{Measures}} of {{Association}}},
  author = {W., Kruskal},
  year = {1958},
  volume = {53},
  pages = {814--861},
  journal = {Journal of the American Statistical Association},
  type = {Journal Article}
}

@article{wang2020,
  title = {Asking and Answering Questions to Evaluate the Factual Consistency of Summaries},
  author = {Wang, Alex and Cho, Kyunghyun and Lewis, Mike},
  year = {2020},
  abstract = {Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called QAGS (pronounced” kags”) that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is …},
  archiveprefix = {arXiv},
  eprint = {2004.04228},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:2004.04228},
  note = {Times cited: 27},
  type = {Journal Article}
}

@book{wanzheng2020,
  title = {{{GRUEN}} for {{Evaluating Linguistic Quality}} of {{Generated Text EMNLP}}},
  author = {Wanzheng, Zhu and S., Bhat},
  year = {2020}
}

@article{weber2018,
  title = {Controlling {{Decoding}} for {{More Abstractive Summaries}} with {{Copy}}-{{Based Networks}}},
  author = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan and Cho, Kyunghyun},
  year = {2018},
  pages = {1803.07038v2},
  abstract = {Attention-based neural abstractive summarization systems equipped with copy mechanisms have shown promising results. Despite this success, it has been noticed that such a system generates a summary by mostly, if not entirely, copying over phrases, sentences, and sometimes multiple consecutive sentences from an input paragraph, effectively performing extractive summarization. In this paper, we verify this behavior using the latest neural abstractive summarization system - a pointer-generator network. We propose a simple baseline method that allows us to control the amount of copying without retraining. Experiments indicate that the method provides a strong baseline for abstractive systems looking to obtain high ROUGE scores while minimizing overlap with the source article, substantially reducing the n-gram overlap with the original article while keeping within 2 points of the original model’s ROUGE score.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{weissenborn2017,
  title = {Dynamic {{Integration}} of {{Background Knowledge}} in {{Neural NLU Systems}}},
  author = {Weissenborn, Dirk and Kočiský, Tomáš and Dyer, Chris},
  year = {2017},
  pages = {1706.02596v3},
  abstract = {Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding (NLU) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in NLU models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific NLU architecture that reprocesses the task inputs with these representations. Experiments on document question answering (DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{wenbo2019,
  title = {Concept {{Pointer Network}} for {{Abstractive Summarization}}},
  author = {Wenbo, Wang and Yang, Gao and Heyan, Huang and Yuxiang, Zhou},
  year = {2019},
  pages = {1910.08486v1},
  abstract = {A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distantly-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the DUC-2004 and Gigaword datasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework.},
  journal = {arXiv},
  note = {Accepted by EMNLP’2019},
  type = {Journal Article}
}

@article{werbos1990,
  title = {Backpropagation through Time: What It Does and How to Do It},
  author = {Werbos, Paul J},
  year = {1990},
  volume = {78},
  pages = {1550--1560},
  publisher = {{IEEE}},
  doi = {10.1109/5.58337},
  abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with …},
  isbn = {0018-9219},
  journal = {Proceedings of the IEEE},
  note = {Times cited: 4392},
  number = {10},
  type = {Journal Article}
}

@book{wes2010,
  title = {Data {{Structures}} for {{Statistical Computing}} in {{Python Proceedings}} of the 9th {{Python}} in {{Science Conference}}},
  author = {Wes, McKinney},
  editor = {St\textbackslash ’efan, van der Walt and Jarrod, Millman},
  year = {2010}
}

@article{wolf2018,
  title = {Meta-{{Learning}} a {{Dynamical Language Model}}},
  author = {Wolf, Thomas and Chaumond, Julien and Delangue, Clement},
  year = {2018},
  pages = {1803.10631v1},
  abstract = {We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.},
  journal = {arXiv},
  note = {5 pages, 2 figures, accepted at ICLR 2018 workshop track},
  type = {Journal Article}
}

@article{wolf2019,
  title = {{{HuggingFace}}’s {{Transformers}}: {{State}}-of-the-Art {{Natural Language Processing}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2019},
  pages = {1910.03771v5},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \emph{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \emph{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
  journal = {arXiv},
  note = {8 pages, 4 figures, more details at https://github.com/huggingface/transformers},
  type = {Journal Article}
}

@article{wu2008,
  title = {Top 10 Algorithms in Data Mining},
  author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
  year = {2008},
  volume = {14},
  pages = {1--37},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10115-007-0114-2},
  journal = {Knowledge and Information Systems},
  note = {Times cited: 2440},
  number = {1},
  type = {Journal Article}
}

@article{wu2018,
  title = {Learning to {{Extract Coherent Summary}} via {{Deep Reinforcement Learning}}},
  author = {Wu, Yuxiang and Hu, Baotian},
  year = {2018},
  pages = {1804.07036v1},
  abstract = {Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and ROUGE package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization (RNES) model. The RNES model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed RNES outperforms existing baselines and achieves state-of-the-art performance in term of ROUGE on CNN/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by RNES are more coherent and readable.},
  journal = {arXiv},
  note = {8 pages, 1 figure, presented at AAAI-2018},
  type = {Journal Article}
}

@article{xu2019,
  title = {Neural {{Extractive Text Summarization}} with {{Syntactic Compression}}},
  author = {Xu, Jiacheng and Durrett, Greg},
  year = {2019},
  pages = {1902.00863v2},
  abstract = {Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the CNN/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by ROUGE. Moreover, our approach outperforms an off-the-shelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical.},
  journal = {arXiv},
  note = {14 pages, EMNLP 2019},
  type = {Journal Article}
}

@article{xu2020,
  title = {Improving {{AMR Parsing}} with {{Sequence}}-to-{{Sequence Pre}}-Training},
  author = {Xu, Dongqin and Li, Junhui and Zhu, Muhua and Zhang, Min and Zhou, Guodong},
  year = {2020},
  pages = {2010.01771v1},
  abstract = {In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing. However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing. In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https://github.com/xdqkid/S2S-AMR-Parser.},
  journal = {arXiv},
  note = {Accepted by EMNLP 2020},
  type = {Journal Article}
}

@book{xu2020a,
  title = {Fact-Based {{Content Weighting}} for {{Evaluating Abstractive Summarisation}}},
  author = {Xu, Xinnuo and Dušek, Ondřej and Li, Jingyi and Rieser, Verena and Konstas, Ioannis},
  year = {2020},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@book{y.2013,
  title = {An {{Efficient Power}}-{{Aware Resource Scheduling Strategy}} in {{Virtualized Datacenters}}},
  author = {Y., Zu and T., Huang and Y., Zhu},
  year = {2013},
  month = dec,
  abstract = {In the era of cloud computing, data centers are well-known to be bounded by the power wall issue. This issue lowers the profit of service providers and obstructs the expansions of data center’s scale. As virtual machine’s behavior was not explored sufficiently in classic data center’s power-saving strategies, in this paper we address the power consumption issue in the setting of a virtualized data center. We propose an efficient power-aware resource scheduling strategy that reduces data center’s power consumption effectively based on VM live migration which is a key technical feature of cloud computing. Our scheduling algorithm leverages the Xen platform and consolidates VM workloads periodically to reduce the number of running servers. To satisfy each VM’s service level agreements, our strategy keeps adjusting VM placements between scheduling rounds. We developed a power-aware data center simulator to test our algorithm. The simulator runs in time domain and includes server’s segmented linear power model. We validated our simulator using measured server power trace. Our simulation shows that compared with event-driven schedulers, our strategy improves data center power budget by 35\% for random workloads resembling web-requests, and improve data center power budget by 22.7\% for workloads exhibiting stable resource requirements like ScaLAPACK.},
  keywords = {cloud computing,Cloud computing,Computational modeling,computer centres,contracts,datacenter power consumption,datacenter simulator,event-driven schedulers,power aware computing,power consumption,power consumption issue,Power demand,power wall issue,power-aware data center simulator,power-aware resource scheduling strategy,resource allocation,resource provisioning,ScaLAPACK,scheduling,Scheduling algorithms,server power model,server power trace,server segmented linear power model,Servers,Time-domain analysis,virtual machine,virtual machines,virtualisation,virtualized datacenters,VM live migration,VM placements,VM service level agreements,VM workloads,Xen platform}
}

@book{yang2016,
  title = {Peak: {{Pyramid}} Evaluation via Automated Knowledge Extraction},
  author = {Yang, Qian and Passonneau, Rebecca and De Melo, Gerard},
  year = {2016},
  volume = {Proceedings of the AAAI Conference on Artificial Intelligence 30(1)},
  abstract = {Evaluating the selection of content in a summary is important both for human-written summaries, which can be a useful pedagogical tool for reading and writing skills, and machine-generated summaries, which are increasingly being deployed in information …}
}

@article{yang2017,
  title = {Breaking the Softmax Bottleneck: {{A}} High-Rank {{RNN}} Language Model},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W},
  year = {2017},
  abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn …},
  archiveprefix = {arXiv},
  eprint = {1711.03953},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1711.03953},
  type = {Journal Article}
}

@article{yoon2020,
  title = {Learning by {{Semantic Similarity Makes Abstractive Summarization Better}}},
  author = {Yoon, Wonjin and Yeo, Yoon Sun and Jeong, Minbyul and Yi, Bong-Jun and Kang, Jaewoo},
  year = {2020},
  pages = {2002.07767v1},
  abstract = {One of the obstacles of abstractive summarization is the presence of various potentially correct predictions. Widely used objective functions for supervised learning, such as cross-entropy loss, cannot handle alternative answers effectively. Rather, they act as a training noise. In this paper, we propose Semantic Similarity strategy that can consider semantic meanings of generated summaries while training. Our training objective includes maximizing semantic similarity score which is calculated by an additional layer that estimates semantic similarity between generated summary and reference summary. By leveraging pre-trained language models, our model achieves a new state-of-the-art performance, ROUGE-L score of 41.5 on CNN/DM dataset. To support automatic evaluation, we also conducted human evaluation and received higher scores relative to both baseline and reference summaries.},
  journal = {arXiv},
  type = {Journal Article}
}

@book{yu2015,
  title = {{{CASICT}}-{{DCU}} Participation in {{WMT2015}} Metrics Task},
  author = {Yu, Hui and Ma, Qingsong and Wu, Xiaofeng and Liu, Qun},
  year = {2015},
  volume = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
  abstract = {Human-designed sub-structures are required by most of the syntax-based machine translation evaluation metrics. In this paper, we propose a novel evaluation metric based on dependency parsing model, which does not need this human involvement. Experimental results show that the new single metric gets better correlation than METEOR on system level and is comparable with it on sentence level. To introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart …‏}
}

@article{yu2019,
  title = {A {{Review}} of {{Recurrent Neural Networks}}: {{LSTM Cells}} and {{Network Architectures}}.},
  author = {Yu, Y and Si, X and Hu, C and Zhang, J},
  year = {2019},
  volume = {31},
  pages = {1235--1270},
  doi = {10.1162/neco_a_01199},
  abstract = {Recurrent neural networks (RNNs) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, RNNs consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory (LSTM) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on RNNs have been achieved by the LSTM. The LSTM has become the focus of deep learning. We review the LSTM cell and its variants to explore the learning capacity of the LSTM cell. Furthermore, the LSTM networks are divided into two broad categories: LSTM-dominated networks and integrated LSTM networks. In addition, their various applications are discussed. Finally, future research directions are presented for LSTM networks.},
  journal = {Neural Comput},
  number = {7},
  type = {Journal Article}
}

@article{yu2020,
  title = {A {{Survey}} of {{Knowledge}}-{{Enhanced Text Generation}}},
  author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  year = {2020},
  pages = {2010.04389v1},
  abstract = {The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
  journal = {arXiv},
  note = {44 pages; Preprint; A paper and code collection is available at https://github.com/wyu97/KENLG-Reading},
  type = {Journal Article}
}

@article{zacharias2018,
  title = {A {{Survey}} on {{Deep Learning Toolkits}} and {{Libraries}} for {{Intelligent User Interfaces}}},
  author = {Zacharias, Jan and Barz, Michael and Sonntag, Daniel},
  year = {2018},
  pages = {1803.04818v2},
  abstract = {This paper provides an overview of prominent deep learning toolkits and, in particular, reports on recent publications that contributed open source software for implementing tasks that are common in intelligent user interfaces (IUI). We provide a scientific reference for researchers and software engineers who plan to utilise deep learning techniques within their IUI research and development projects.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{zaheer2020,
  title = {Big {{Bird}}: {{Transformers}} for {{Longer Sequences}}},
  author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
  year = {2020},
  pages = {2007.14062v1},
  abstract = {Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.},
  journal = {arXiv},
  type = {Journal Article}
}

@article{zaremba2014,
  title = {Learning to {{Execute}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2014},
  volume = {cs.NE},
  abstract = {Recurrent Neural Networks (RNNs) with Long Short-Term Memory units (LSTM) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of LSTMs in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that LSTMs can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks’ performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an LSTM to add two 9-digit numbers with 99\% accuracy.},
  type = {Journal Article}
}

@article{zeng2016,
  title = {Efficient Summarization with Read-Again and Copy Mechanism},
  author = {Zeng, Wenyuan and Luo, Wenjie and Fidler, Sanja and Urtasun, Raquel},
  year = {2016},
  abstract = {Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a …},
  archiveprefix = {arXiv},
  eprint = {1611.03382},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1611.03382},
  note = {Times cited: 65},
  type = {Journal Article}
}

@article{zhang2017,
  title = {Sentence {{Simplification}} with {{Deep Reinforcement Learning}}},
  author = {Zhang, Xingxing and Lapata, Mirella},
  year = {2017},
  pages = {1703.10931v2},
  abstract = {Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call \textbackslash sc Dress (as shorthand for \textbackslash bf Deep \textbackslash bf REinforcement \textbackslash bf Sentence \textbackslash bf Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.},
  journal = {arXiv},
  note = {to appear in EMNLP 2017},
  type = {Journal Article}
}

@article{zhang2018,
  title = {Neural Latent Extractive Document Summarization},
  author = {Zhang, Xingxing and Lapata, Mirella and Wei, Furu and Zhou, Ming},
  year = {2018},
  abstract = {Extractive summarization models require sentence-level labels, which are usually created heuristically (eg, with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes\textbackslash emph directly from gold summaries. Experiments on the CNN/Dailymail dataset …},
  archiveprefix = {arXiv},
  eprint = {1808.07187},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1808.07187},
  note = {Times cited: 75},
  type = {Journal Article}
}

@article{zhang2019,
  title = {{{PEGASUS}}: {{Pre}}-Training with {{Extracted Gap}}-Sentences for {{Abstractive Summarization}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
  year = {2019},
  pages = {1912.08777v3},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  journal = {arXiv},
  note = {Added results from mixed+stochastic model, test-set overlapping analysis; Code link added; Accepted for ICML 2020. arXiv admin note: text overlap with arXiv:1605.06560, arXiv:1205.2395, arXiv:0902.4351, arXiv:1610.09932, arXiv:nucl-ex/0512029 by other authors},
  type = {Journal Article}
}

@book{zhang2019a,
  title = {Knowledge {{Adaptive Neural Network}} for {{Natural Language Inference}}},
  author = {Zhang, Qi and Yang, Yan and Chen, Chengcai and He, Liang and Yu, Zhou},
  year = {2019},
  publisher = {{IEEE}}
}

@article{zhang2019b,
  title = {{{BERTScore}}: {{Evaluating Text Generation}} with {{BERT}}},
  author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  year = {2019},
  pages = {1904.09675v3},
  abstract = {We propose BERTScore, an automatic evaluation metric for text generation. Analogously to common metrics, BERTScore computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. BERTScore correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that BERTScore is more robust to challenging examples when compared to existing metrics.},
  journal = {arXiv},
  note = {Code available at https://github.com/Tiiiger/bert\_score; To appear in ICLR2020},
  type = {Journal Article}
}

@article{zhang2019c,
  title = {{{HIBERT}}: {{Document}} Level Pre-Training of Hierarchical Bidirectional Transformers for Document Summarization},
  author = {Zhang, Xingxing and Wei, Furu and Zhou, Ming},
  year = {2019},
  abstract = {Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these\textbackslash emph inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders\textbackslash cite devlin: 2018: arxiv, we propose \textbackslash sc Hibert(as shorthand for \textbackslash bf HI erachical \textbackslash bf B idirectional \textbackslash bf E ncoder \textbackslash bf R epresentations from \textbackslash bf T ransformers) for …},
  archiveprefix = {arXiv},
  eprint = {1905.06566},
  eprinttype = {arxiv},
  journal = {arXiv preprint arXiv:1905.06566},
  note = {Times cited: 101},
  type = {Journal Article}
}

@article{zhang2020,
  title = {Lightweight, {{Dynamic Graph Convolutional Networks}} for {{AMR}}-to-{{Text Generation}}},
  author = {Zhang, Yan and Guo, Zhijiang and Teng, Zhiyang and Lu, Wei and Cohen, Shay B. and Liu, Zuozhu and Bing, Lidong},
  year = {2020},
  pages = {2010.04383v1},
  abstract = {AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper GCN models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.},
  journal = {arXiv},
  note = {Accepted to EMNLP 2020, long paper},
  type = {Journal Article}
}

@book{zhao2019,
  title = {{{MoverScore}}: {{Text Generation Evaluating}} with {{Contextualized Embeddings}} and {{Earth Mover Distance}}},
  author = {Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  address = {{Stroudsburg, PA, USA}}
}

@article{zheng2015,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Systems}}},
  author = {Zheng, Martin Abadi and Agarwal, Ashish and Barham, P. and Brevdo, E. and Chen, Z. and Citro, C. and Corrado, G. and Davis, Andy and Dean, J. and Devin, M. and Ghemawat, Sanjay and Goodfellow, I. and Harp, Andrew and Irving, Geoffrey and Isard, M. and Jia, Y. and Jozefowicz, R. and Kaiser, Lukasz and Kudlur, M. and Levenberg, Josh and Mane, Dandelion and Monga, Rajat and Moore, Sherry and Murray, D. and Olah, Christopher and Schuster, M. and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, P. and Vanhoucke, V. and Vasudevan, Vijay and Viegas, Fernanda B. and Vinyals, Oriol and Warden, Pete and Wattenberg, M. and Wicke, Martin and Yu, Y. and {X.}},
  year = {2015},
  note = {Software available from tensorflow.org},
  type = {Journal Article}
}

@book{zhou2006,
  title = {Paraeval: {{Using}} Paraphrases to Evaluate Summaries Automatically},
  author = {Zhou, Liang and Lin, Chin-Yew and Munteanu, Dragos Stefan and Hovy, Eduard},
  year = {2006},
  volume = {Proceedings of the human language technology conference of the NAACL, main conference},
  abstract = {ParaEval is an automated evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation (MT). We show that the quality of ParaE-val’s evaluations, measured by correlating with human judgments, closely resembles that of …}
}

@article{zhou2017,
  title = {Selective {{Encoding}} for {{Abstractive Sentence Summarization}}},
  author = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Zhou, Ming},
  year = {2017},
  pages = {1704.07073v1},
  abstract = {We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.},
  journal = {arXiv},
  note = {10 pages; To appear in ACL 2017},
  type = {Journal Article}
}

@article{zhou2018,
  title = {Neural {{Document Summarization}} by {{Jointly Learning}} to {{Score}} and {{Select Sentences}}},
  author = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Huang, Shaohan and Zhou, Ming and Zhao, Tiejun},
  year = {2018},
  pages = {1807.02305v1},
  abstract = {Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.},
  journal = {arXiv},
  note = {In ACL 2018},
  type = {Journal Article}
}

@article{zhu2020,
  title = {Boosting {{Factual Correctness}} of {{Abstractive Summarization}} with {{Knowledge Graph}}},
  author = {Zhu, Chenguang and Hinthorn, William and Xu, Ruochen and Zeng, Qingkai and Zeng, Michael and Huang, Xuedong and Jiang, Meng},
  year = {2020},
  pages = {2003.08612v5},
  abstract = {A commonly observed problem with abstractive summarization is the distortion or fabrication of factual information in the article. This inconsistency between summary and original text has led to various concerns over its applicability. In this paper, we propose a Fact-Aware Summarization model, FASum, which extracts factual relations from the article to build a knowledge graph and integrates it into the neural decoding process. Then, we propose a Factual Corrector model, FC, that can modify abstractive summaries generated by any summarization model to improve factual correctness. Empirical results show that FASum can generate summaries with higher factual correctness compared with state-of-the-art abstractive summarization systems. And FC improves the factual correctness of summaries generated by various models via only modifying several entity tokens.},
  journal = {arXiv},
  note = {15 pages, 3 figures},
  type = {Journal Article}
}

@article{ziegler2019,
  title = {Fine-{{Tuning Language Models}} from {{Human Preferences}}},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = {2019},
  pages = {1909.08593v2},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  journal = {arXiv},
  type = {Journal Article}
}



@article{agarwal_neural_2020,
	title = {Neural additive models: Interpretable machine learning with neural nets},
	url = {https://arxiv.org/pdf/2004.13912},
	abstract = {Deep neural networks ({DNNs}) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models ({NAMs}) which combine some of the expressivity of {DNNs} with the inherent intelligibility of generalized additive models. {NAMs} learn a linear combination of …},
	journaltitle = {{arXiv} preprint {arXiv}:2004.13912},
	author = {Agarwal, Rishabh and Frosst, Nicholas and Zhang, Xuezhou and Caruana, Rich and Hinton, Geoffrey E},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Times cited: 22}
}

@article{alguliev_multiple_2013,
	title = {Multiple documents summarization based on evolutionary optimization algorithm},
	volume = {40},
	url = {http://dx.doi.org/10.1016/j.eswa.2012.09.014},
	doi = {10.1016/j.eswa.2012.09.014},
	pages = {1675--1689},
	number = {5},
	journaltitle = {Expert Systems with Applications},
	author = {Alguliev, Rasim M. and Aliguliyev, Ramiz M. and Isazade, Nijat R.},
	date = {2013},
	note = {Publisher: Elsevier {BV}
	Type: Journal article},
	annotation = {Times cited: 46}
}

@article{allahyari_text_2017,
	title = {Text Summarization Techniques: A Brief Survey},
	url = {http://arxiv.org/abs/1707.02268v3},
	abstract = {In recent years, there has been a explosion in the amount of text data from a variety of sources. This volume of text is an invaluable source of information and knowledge which needs to be effectively summarized to be useful. In this review, the main approaches to automatic text summarization are described. We review the different processes for summarization and describe the effectiveness and shortcomings of the different methods.},
	pages = {1707.02268v3},
	journaltitle = {{arXiv}},
	author = {Allahyari, Mehdi and Pouriyeh, Seyedamin and Assefi, Mehdi and Safaei, Saeid and Trippe, Elizabeth D. and Gutierrez, Juan B. and Kochut, Krys},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Some of references format have updated}
}

@article{allen_analogies_2019,
	title = {Analogies Explained: Towards Understanding Word Embeddings},
	url = {http://arxiv.org/abs/1901.09813v2},
	abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy “woman is to queen as man is to king” approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of “\$w\_x\$ is to \$w\_y\$”. From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
	pages = {1901.09813v2},
	journaltitle = {{arXiv}},
	author = {Allen, Carl and Hospedales, Timothy},
	date = {2019},
	note = {Type: Journal article}
}

@article{anchieta_sema_2019,
	title = {{SEMA}: an Extended Semantic Evaluation Metric for {AMR}},
	url = {http://arxiv.org/abs/1905.12069v1},
	abstract = {Abstract Meaning Representation ({AMR}) is a recently designed semantic representation language intended to capture the meaning of a sentence, which may be represented as a single-rooted directed acyclic graph with labeled nodes and edges. The automatic evaluation of this structure plays an important role in the development of better systems, as well as for semantic annotation. Despite there is one available metric, smatch, it has some drawbacks. For instance, smatch creates a self-relation on the root of the graph, has weights for different error types, and does not take into account the dependence of the elements in the {AMR} structure. With these drawbacks, smatch masks several problems of the {AMR} parsers and distorts the evaluation of the {AMRs}. In view of this, in this paper, we introduce an extended metric to evaluate {AMR} parsers, which deals with the drawbacks of the smatch metric. Finally, we compare both metrics, using four well-known {AMR} parsers, and we argue that our metric is more refined, robust, fairer, and faster than smatch.},
	pages = {1905.12069v1},
	journaltitle = {{arXiv}},
	author = {Anchieta, Rafael T. and Cabezudo, Marco A. S. and Pardo, Thiago A. S.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted by {CICLing} 2019}
}

@book{aston_dive_2020,
	title = {Dive into Deep Learning},
	author = {Aston, Zhang and Zachary, C. Lipton and Mu, Li and Alexander, J. Smola},
	date = {2020}
}

@book{b_nice_2014,
	title = {{NICE}: Network-aware {VM} Consolidation scheme for Energy Conservation in Data Centers},
	abstract = {Energy conservation and network performance have become two of the most important issues in data center as the scale of cloud services continues growing. Recent researches usually consider these two issues separately. Energy conservation mainly deals with hosts, which reduces total energy consumption by consolidating virtual machine({VM})s to fewer hosts, and network performance mainly deals with network scalability and energy efficiency, which improves data center network({DCN}) scalability by applying new network topologies or routing schemes and improves {DCN} energy efficiency by consolidating trafile. In this paper, we jointly consider these two issues and define Combined {VM} Consolidation ({CVC}) problem. We prove that {CVC} is {NP}-complete and is inapproximable by a factor of 3/2 ε unless P = {NP}. Next, we propose {NICE}: Network-aware {VM} Consolidation scheme for Energy Conservation in Data {CEnter} to solve {CVC}. Instead of taking the unrealistic hypothesis that migration cost is negligible, a common assumption in most literatures, we precisely analyze {VM} migration cost according to real-trace experiments in a 6-server testbed via {VMware}. Massive simulations validate the efficiency of {NICE}, In all, to the best of our knowledge, we arc the first work to combine {VM} consolidation with network optimization and migration cost.},
	pagetotal = {166-173},
	author = {B., Cao and X., Gao and G., Chen and Y., Jin},
	date = {2014-12},
	keywords = {cloud computing, cloud service, combined {VM} consolidation, computational complexity, computer centres, {CVC}, data center, data center network, {DCN}, energy saving, migration cost, network optimization, network-aware {VM} consolidation scheme for energy conservation in data centers, {NICE}, Nonvolatile memory, {NP}-complete problem, optimisation, power aware computing, Silicon, virtual machine, virtual machines, {VM} consolidation, {VM} migration cost}
}

@article{bahdanau_neural_2014,
	title = {Neural machine translation by jointly learning to align and translate},
	url = {https://arxiv.org/pdf/1409.0473)},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance …},
	journaltitle = {{arXiv} preprint {arXiv}:1409.0473},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	date = {2014},
	note = {Type: Journal article},
	annotation = {Times cited: 14895}
}

@article{bamler_dynamic_2017,
	title = {Dynamic Word Embeddings},
	url = {http://arxiv.org/abs/1702.08359v2},
	abstract = {We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time. The model represents words and contexts by latent trajectories in an embedding space. At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec [Mikolov et al., 2013]. These embedding vectors are connected in time through a latent diffusion process. We describe two scalable variational inference algorithms–skip-gram smoothing and skip-gram filtering–that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift. Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.},
	pages = {1702.08359v2},
	journaltitle = {{arXiv}},
	author = {Bamler, Robert and Mandt, Stephan},
	date = {2017},
	note = {Type: Journal article},
	annotation = {In the proceedings of the International Conference on Machine Learning ({ICML} 2017); 8 pages + references and supplement}
}

@book{banarescu_abstract_2013,
	title = {Abstract meaning representation for sembanking},
	volume = {Proceedings of the 7th linguistic annotation workshop and interoperability with discourse},
	abstract = {{\textless}span dir=ltr{\textgreater}Abstract We describe Abstract Meaning Representation ({AMR}), a semantic representation language in which we are writing down the meanings of thousands of English sentences. We hope that a sembank of simple, whole-sentence semantic structures will spur new work in statistical natural language understanding and generation, like the Penn Treebank encouraged work on statistical parsing. This paper gives an overview of {AMR} and tools associated with it.{\textless}/span{\textgreater}‏},
	pagetotal = {178-186},
	author = {Banarescu, Laura and Bonial, Claire and Cai, Shu and Georgescu, Madalina and Griffitt, Kira and Hermjakob, Ulf and Knight, Kevin and Koehn, Philipp and Palmer, Martha and Schneider, Nathan},
	date = {2013}
}

@article{barros_natsum_2019,
	title = {{NATSUM}: Narrative abstractive summarization through cross-document timeline generation},
	volume = {56},
	url = {http://dx.doi.org/10.1016/j.ipm.2019.02.010},
	doi = {10.1016/j.ipm.2019.02.010},
	pages = {1775--1793},
	number = {5},
	journaltitle = {Information Processing \& Management},
	author = {Barros, Cristina and Lloret, Elena and Saquete, Estela and Navarro-Colorado, Borja},
	date = {2019},
	note = {Publisher: Elsevier {BV}
	Type: Journal article},
	annotation = {Times cited: 7}
}

@article{barzilay_sentence_2005,
	title = {Sentence fusion for multidocument news summarization},
	volume = {31},
	url = {https://www.mitpressjournals.org/doi/pdfplus/10.1162/089120105774321091},
	abstract = {A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence …},
	pages = {297--328},
	number = {3},
	journaltitle = {Computational Linguistics},
	author = {Barzilay, Regina and {McKeown}, Kathleen R},
	date = {2005},
	note = {{ISBN}: 0891-2017
	Publisher: {MIT} Press
	Type: Journal article},
	annotation = {Times cited: 441}
}

@article{bauer_empirical_1999,
	title = {An Empirical Comparison of Voting Classification Algorithms: Bagging, Boosting, and Variants},
	volume = {36},
	url = {http://dx.doi.org/10.1023/a:1007515423169},
	doi = {10.1023/a:1007515423169},
	pages = {105--139},
	number = {1},
	journaltitle = {Machine Learning},
	author = {Bauer, Eric and Kohavi, Ron},
	date = {1999},
	note = {Publisher: Springer Science and Business Media {LLC}
	Type: Journal article},
	annotation = {Times cited: 1328}
}

@thesis{bayer_learning_2015,
	title = {Learning sequence representations},
	abstract = {This work contributes to learning representations of data with Neural Networks ({NNs}), and {RNNs} in particular, in three ways. First, we will show how {NNs} can be augmented with additional calculations to allow the propagation of not only points, but random variables summarised by their expectation and variance through an {NN}. This generalises Fast Dropout ({FD}), which we show to be an outstanding regularisation method for {RNNs}. It further allows us to obtain approximations of the marginal likelihood and the predictive distribution of {NNs}, which we will use to implement Variational Bayes ({VB}) and related methods for the estimation of parameters. Second, we will introduce the framework of sequence reduction. It consists of using {RNNs} in conjunction with pooling operators to reduce sequences of arbitrary length to ﬁxed-length points, enabling further analysis. Third, we will leverage advances in Variational Inference ({VI}) to learn latent state representations of sequences. These are obtained by stochastic Recurrent Networks ({STORNs}), where a standard {RNN} is augmented with stochastic units, making it able to represent arbitrarily complex distributions. The model is trained by means of Stochastic Gradient Variational Bayes ({SGVB}), making it probabilistic and paving the way for applications such as denoising, missing value imputation, synthesis and more.},
	institution = {Technische Universität München},
	type = {phdthesis},
	author = {Bayer, Justin Simon},
	date = {2015}
}

@article{belinkov_analysis_2019,
	title = {Analysis methods in neural language processing: A survey},
	volume = {7},
	pages = {49--72},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Glass, James},
	date = {2019},
	note = {{ISBN}: 2307-387X
	Publisher: {MIT} Press
	Type: Journal article}
}

@article{beltagy_longformer_2020,
	title = {Longformer: The long-document transformer},
	url = {https://arxiv.org/pdf/2004.05150},
	abstract = {Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence …},
	journaltitle = {{arXiv} preprint {arXiv}:2004.05150},
	author = {Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Times cited: 62}
}

@book{bengio_learning_2009,
	title = {Learning deep architectures for {AI}},
	abstract = {Can machine learning deliver {AI}? Theoretical results, inspiration from the brain and cognition, as well as machine learning experiments suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (eg in vision, language, and other {AI}-level tasks), one would need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers, graphical models with many levels of latent variables, or in complicated propositional …},
	publisher = {Now Publishers Inc},
	author = {Bengio, Yoshua},
	date = {2009}
}

@article{bengio_representation_2012,
	title = {Representation Learning: A Review and New Perspectives},
	url = {http://arxiv.org/abs/1206.5538v3},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for {AI} is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	pages = {1206.5538v3},
	journaltitle = {{arXiv}},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	date = {2012},
	note = {Type: Journal article}
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/279181},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between …},
	pages = {157--166},
	number = {2},
	journaltitle = {{IEEE} transactions on neural networks},
	author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
	date = {1994},
	note = {{ISBN}: 1045-9227
	Publisher: {IEEE}
	Type: Journal article},
	annotation = {Times cited: 5746}
}

@book{benikova_bridging_2016,
	title = {Bridging the gap between extractive and abstractive summaries: Creation and evaluation of coherent extracts from heterogeneous sources},
	volume = {Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers},
	abstract = {Coherent extracts are a novel type of summary combining the advantages of manually created abstractive summaries, which are fluent but difficult to evaluate, and low-quality automatically created extractive summaries, which lack coherence and structure. We use a corpus of heterogeneous documents to address the issue that information seekers usually face–a variety of different types of information sources. We directly extract information from these, but minimally redact and meaningfully order it to form a coherent text. Our qualitative …},
	pagetotal = {1039-1050},
	author = {Benikova, Darina and Mieskes, Margot and Meyer, Christian M and Gurevych, Iryna},
	date = {2016}
}

@article{berger_maximum_1996,
	title = {A maximum entropy approach to natural language processing},
	volume = {22},
	url = {https://dl.acm.org/doi/abs/10.5555/234285.234289},
	abstract = {The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper, we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach …},
	pages = {39--71},
	number = {1},
	journaltitle = {Computational linguistics},
	author = {Berger, Adam and Della Pietra, Stephen A and Della Pietra, Vincent J},
	date = {1996},
	note = {Type: Journal article},
	annotation = {Times cited: 4276}
}

@article{bhandari_metrics_2020,
	title = {Metrics also Disagree in the Low Scoring Range: Revisiting Summarization Evaluation Metrics},
	url = {http://arxiv.org/abs/2011.04096v1},
	abstract = {In text summarization, evaluating the efficacy of automatic metrics without human judgments has become recently popular. One exemplar work concludes that automatic metrics strongly disagree when ranking high-scoring summaries. In this paper, we revisit their experiments and find that their observations stem from the fact that metrics disagree in ranking summaries from any narrow scoring range. We hypothesize that this may be because summaries are similar to each other in a narrow scoring range and are thus, difficult to rank. Apart from the width of the scoring range of summaries, we analyze three other properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.},
	pages = {2011.04096v1},
	journaltitle = {{arXiv}},
	author = {Bhandari, Manik and Gour, Pranav and Ashfaq, Atabak and Liu, Pengfei},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted at {COLING} 2020}
}

@article{bhandari_re-evaluating_2020,
	title = {Re-evaluating Evaluation in Text Summarization},
	url = {http://arxiv.org/abs/2010.07100v1 http://arxiv.org/pdf/2010.07100v1},
	abstract = {Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not – for nearly 20 years {ROUGE} has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.},
	pages = {2010.07100v1},
	journaltitle = {{arXiv}},
	author = {Bhandari, Manik and Gour, Pranav and Ashfaq, Atabak and Liu, Pengfei and Neubig, Graham},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted at {EMNLP} 2020}
}

@article{bi_incorporating_2019,
	title = {Incorporating External Knowledge into Machine Reading for Generative Question Answering},
	url = {http://arxiv.org/abs/1909.02745v1 http://arxiv.org/pdf/1909.02745v1},
	abstract = {Commonsense and background knowledge is required for a {QA} model to answer many nontrivial questions. Different from existing work on knowledge-aware {QA}, we focus on a more challenging task of leveraging external knowledge to generate answers in natural language for a given question with context. In this paper, we propose a new neural model, Knowledge-Enriched Answer Generator ({KEAG}), which is able to compose a natural answer by exploiting and aggregating evidence from all four information sources available: question, passage, vocabulary and knowledge. During the process of answer generation, {KEAG} adaptively determines when to utilize symbolic knowledge and which fact from the knowledge is useful. This allows the model to exploit external knowledge that is not explicitly stated in the given text, but that is relevant for generating an answer. The empirical study on public benchmark of answer generation demonstrates that {KEAG} improves answer quality over models without knowledge and existing knowledge-aware models, confirming its effectiveness in leveraging knowledge.},
	pages = {1909.02745v1},
	journaltitle = {{arXiv}},
	author = {Bi, Bin and Wu, Chen and Yan, Ming and Wang, Wei and Xia, Jiangnan and Li, Chenliang},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted at {EMNLP} 2019}
}

@article{bitzer_recognizing_2012,
	title = {Recognizing recurrent neural networks ({rRNN}): Bayesian inference for recurrent neural networks.},
	volume = {106},
	url = {https://pubmed.ncbi.nlm.nih.gov/22581026},
	doi = {10.1007/s00422-012-0490-x},
	abstract = {Recurrent neural networks ({RNNs}) are widely used in computational neuroscience and machine learning applications. In an {RNN}, each neuron computes its output as a nonlinear function of its integrated input. While the importance of {RNNs}, especially as models of brain processing, is undisputed, it is also widely acknowledged that the computations in standard {RNN} models may be an over-simplification of what real neuronal networks compute. Here, we suggest that the {RNN} approach may be made computationally more powerful by its fusion with Bayesian inference techniques for nonlinear dynamical systems. In this scheme, we use an {RNN} as a generative model of dynamic input caused by the environment, e.g. of speech or kinematics. Given this generative {RNN} model, we derive Bayesian update equations that can decode its output. Critically, these updates define a ‘recognizing {RNN}’ ({rRNN}), in which neurons compute and exchange prediction and prediction error messages. The {rRNN} has several desirable features that a conventional {RNN} does not have, e.g. fast decoding of dynamic stimuli and robustness to initial conditions and noise. Furthermore, it implements a predictive coding scheme for dynamic inputs. We suggest that the Bayesian inversion of {RNNs} may be useful both as a model of brain function and as a machine learning tool. We illustrate the use of the {rRNN} by an application to the online decoding (i.e. recognition) of human kinematics.},
	pages = {201--217},
	number = {4},
	journaltitle = {Biol Cybern},
	author = {Bitzer, S and Kiebel, {SJ}},
	date = {2012},
	note = {Place: {MPI} for Human Cognitive and Brain Sciences, Stephanstr. 1a, 04107, Leipzig, Germany. bitzer@cbs.mpg.de
	Type: Journal article}
}

@article{bohm_better_2019,
	title = {Better rewards yield better summaries: Learning to summarise without references},
	url = {https://arxiv.org/pdf/1909.01214},
	abstract = {Reinforcement Learning ({RL}) based document summarisation systems yield state-of-the-art performance in terms of {ROUGE} scores, because they directly use {ROUGE} as the rewards during training. However, summaries with high {ROUGE} scores often receive low human judgement. To find a better reward function that can guide {RL} to generate human-appealing summaries, we learn a reward function from human ratings on 2,500 summaries. Our reward function only takes the document and system summary as input. Hence, once trained, it can …},
	journaltitle = {{arXiv} preprint {arXiv}:1909.01214},
	author = {Böhm, Florian and Gao, Yang and Meyer, Christian M and Shapira, Ori and Dagan, Ido and Gurevych, Iryna},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Times cited: 18}
}

@article{bos_separating_2019,
	title = {Separating Argument Structure from Logical Structure in {AMR}},
	url = {http://arxiv.org/abs/1908.01355v2},
	abstract = {The {AMR} (Abstract Meaning Representation) formalism for representing meaning of natural language sentences was not designed to deal with scope and quantifiers. By extending {AMR} with indices for contexts and formulating constraints on these contexts, a formalism is derived that makes correct prediction for inferences involving negation and bound variables. The attractive core predicate-argument structure of {AMR} is preserved. The resulting framework is similar to that of Discourse Representation Theory.},
	pages = {1908.01355v2},
	journaltitle = {{arXiv}},
	author = {Bos, Johan},
	date = {2019},
	note = {Type: Journal article}
}

@article{bottou_optimization_2016,
	title = {Optimization Methods for Large-Scale Machine Learning},
	url = {http://arxiv.org/abs/1606.04838v3},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient ({SG}) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile {SG} algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	pages = {1606.04838v3},
	journaltitle = {{arXiv}},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	date = {2016},
	note = {Type: Journal article}
}

@article{boutkan_point-less_2019,
	title = {Point-less: More Abstractive Summarization with Pointer-Generator Networks},
	url = {http://arxiv.org/abs/1905.01975v1},
	abstract = {The Pointer-Generator architecture has shown to be a big improvement for abstractive summarization seq2seq models. However, the summaries produced by this model are largely extractive as over 30\% of the generated sentences are copied from the source text. This work proposes a multihead attention mechanism, pointer dropout, and two new loss functions to promote more abstractive summaries while maintaining similar {ROUGE} scores. Both the multihead attention and dropout do not improve N-gram novelty, however, the dropout acts as a regularizer which improves the {ROUGE} score. The new loss function achieves significantly higher novel N-grams and sentences, at the cost of a slightly lower {ROUGE} score.},
	pages = {1905.01975v1},
	journaltitle = {{arXiv}},
	author = {Boutkan, Freek and Ranzijn, Jorn and Rau, David and Wel, Eelco van der},
	date = {2019},
	note = {Type: Journal article},
	annotation = {7 pages}
}

@book{breiman_classification_1984,
	title = {Classification and Regression Trees},
	publisher = {Taylor \& Francis},
	author = {Breiman, L. and Friedman, J. and Stone, C.J. and Olshen, R.A.},
	date = {1984}
}

@article{breiman_bagging_1996,
	title = {Bagging predictors},
	volume = {24},
	url = {http://dx.doi.org/10.1007/bf00058655},
	doi = {10.1007/bf00058655},
	pages = {123--140},
	number = {2},
	journaltitle = {Machine Learning},
	author = {Breiman, Leo},
	date = {1996},
	note = {Publisher: Springer Science and Business Media {LLC}
	Type: Journal article},
	annotation = {Times cited: 8399}
}

@article{breiman_arcing_1997,
	title = {Arcing the edge},
	abstract = {Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman [1996] …},
	author = {Breiman, Leo},
	date = {1997},
	note = {Publisher: Technical Report 486, Statistics Department, University of California at …
	Type: Journal article},
	annotation = {Times cited: 349}
}

@article{breiman_pasting_1999,
	title = {Pasting small votes for classification in large databases and on-line},
	volume = {36},
	url = {https://link.springer.com/article/10.1023/A:1007563306331},
	abstract = {Many databases have grown to the point where they cannot fit into the fast memory of even large memory machines, to say nothing of current workstations. If what we want to do is to use these data bases to construct predictions of various characteristics, then since the usual methods require that all data be held in fast memory, various work-arounds have to be used. This paper studies one such class of methods which give accuracy comparable to that which could have been obtained if all data could have been held in core and which are …},
	pages = {85--103},
	number = {1},
	journaltitle = {Machine learning},
	author = {Breiman, Leo},
	date = {1999},
	note = {{ISBN}: 1573-0565
	Publisher: Springer
	Type: Journal article},
	annotation = {Times cited: 307}
}

@article{britz_massive_2017,
	title = {Massive exploration of neural machine translation architectures},
	url = {https://arxiv.org/pdf/1703.03906},
	abstract = {Neural Machine Translation ({NMT}) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of {GPU} time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of {NMT} architecture hyperparameters. We report empirical results and …},
	journaltitle = {{arXiv} preprint {arXiv}:1703.03906},
	author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Times cited: 343}
}

@article{cachola_tldr_2020,
	title = {{TLDR}: Extreme Summarization of Scientific Documents},
	url = {https://arxiv.org/pdf/2004.15011},
	abstract = {We introduce {TLDR} generation for scientific papers, a new automatic summarization task with high source compression requiring expert background knowledge and complex language understanding. To facilitate research on this task, we introduce {SciTLDR}, a …},
	journaltitle = {{arXiv} preprint {arXiv}:2004.15011},
	author = {Cachola, I and Lo, K and Cohan, A and Weld, {DS}},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Times cited: 2}
}

@book{calheiros_energy-efficient_2014,
	title = {Energy-Efficient Scheduling of Urgent Bag-of-Tasks Applications in Clouds through {DVFS}},
	publisher = {{IEEE}},
	author = {Calheiros, Rodrigo N. and Buyya, Rajkumar},
	date = {2014}
}

@article{camacho-collados_word_2018,
	title = {From Word to Sense Embeddings: A Survey on Vector Representations of Meaning},
	url = {http://arxiv.org/abs/1805.04032v3 http://arxiv.org/pdf/1805.04032v3},
	abstract = {Over the past years, distributed semantic representations have proved to be effective and flexible keepers of prior knowledge to be integrated into downstream applications. This survey focuses on the representation of meaning. We start from the theoretical background behind word vector space models and highlight one of their major limitations: the meaning conflation deficiency, which arises from representing a word with all its possible meanings as a single vector. Then, we explain how this deficiency can be addressed through a transition from the word level to the more fine-grained level of word senses (in its broader acceptation) as a method for modelling unambiguous lexical meaning. We present a comprehensive overview of the wide range of techniques in the two main branches of sense representation, i.e., unsupervised and knowledge-based. Finally, this survey covers the main evaluation procedures and applications for this type of representation, and provides an analysis of four of its important aspects: interpretability, sense granularity, adaptability to different domains and compositionality.},
	pages = {1805.04032v3},
	journaltitle = {{arXiv}},
	author = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
	date = {2018},
	note = {Type: Journal article},
	annotation = {46 pages, 8 figures. Published in Journal of Artificial Intelligence Research}
}

@article{cambria_jumping_2014,
	title = {Jumping {NLP} Curves: A Review of Natural Language Processing Research [Review Article]},
	volume = {9},
	url = {http://dx.doi.org/10.1109/mci.2014.2307227},
	doi = {10.1109/mci.2014.2307227},
	pages = {48--57},
	number = {2},
	journaltitle = {{IEEE} Computational Intelligence Magazine},
	author = {Cambria, Erik and White, Bebo},
	date = {2014},
	note = {Publisher: Institute of Electrical and Electronics Engineers ({IEEE})
	Type: Journal article},
	annotation = {Times cited: 317}
}

@article{cao_faithful_2017,
	title = {Faithful to the original: Fact aware neural abstractive summarization},
	url = {https://arxiv.org/abs/1711.04434},
	abstract = {Unlike extractive summarization, abstractive summarization has to fuse different parts of the source text, which inclines to create fake facts. Our preliminary study reveals nearly 30\% of the outputs from a state-of-the-art neural summarization system suffer from this problem. While previous abstractive summarization approaches usually focus on the improvement of informativeness, we argue that faithfulness is also a vital prerequisite for a practical abstractive summarization system. To avoid generating fake facts in a summary, we …},
	journaltitle = {{arXiv} preprint {arXiv}:1711.04434},
	author = {Cao, Ziqiang and Wei, Furu and Li, Wenjie and Li, Sujian},
	date = {2017},
	note = {Type: Journal article}
}

@article{celikyilmaz_deep_2018,
	title = {Deep Communicating Agents for Abstractive Summarization},
	url = {http://arxiv.org/abs/1803.10357v3},
	abstract = {We present deep communicating agents in an encoder-decoder architecture to address the challenges of representing a long document for abstractive summarization. With deep communicating agents, the task of encoding a long text is divided across multiple collaborating agents, each in charge of a subsection of the input text. These encoders are connected to a single decoder, trained end-to-end using reinforcement learning to generate a focused and coherent summary. Empirical results demonstrate that multiple communicating encoders lead to a higher quality summary compared to several strong baselines, including those based on a single encoder or multiple non-communicating encoders.},
	pages = {1803.10357v3},
	journaltitle = {{arXiv}},
	author = {Celikyilmaz, Asli and Bosselut, Antoine and He, Xiaodong and Choi, Yejin},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Accepted for publication at {NAACL} 2018}
}

@article{celikyilmaz_evaluation_2020,
	title = {Evaluation of Text Generation: A Survey},
	url = {http://arxiv.org/abs/2006.14799v1},
	abstract = {The paper surveys evaluation methods of natural language generation ({NLG}) systems that have been developed in the last few years. We group {NLG} evaluation methods into three categories: (1) human-centric evaluation metrics, (2) automatic metrics that require no training, and (3) machine-learned metrics. For each category, we discuss the progress that has been made and the challenges still being faced, with a focus on the evaluation of recently proposed {NLG} tasks and neural {NLG} models. We then present two case studies of automatic text summarization and long text generation, and conclude the paper by proposing future research directions.},
	pages = {2006.14799v1},
	journaltitle = {{arXiv}},
	author = {Celikyilmaz, Asli and Clark, Elizabeth and Gao, Jianfeng},
	date = {2020},
	note = {Type: Journal article},
	annotation = {42 pages}
}

@book{chaganty_price_2018,
	location = {Stroudsburg, {PA}, {USA}},
	title = {The price of debiasing automatic metrics in natural language evalaution},
	publisher = {Association for Computational Linguistics},
	author = {Chaganty, Arun and Mussmann, Stephen and Liang, Percy},
	date = {2018}
}

@article{chandrasekaran_evolution_2020,
	title = {Evolution of Semantic Similarity – A Survey},
	url = {http://arxiv.org/abs/2004.13820v1 http://arxiv.org/pdf/2004.13820v1},
	abstract = {Estimating the semantic similarity between text data is one of the challenging and open research problems in the field of Natural Language Processing ({NLP}). The versatility of natural language makes it difficult to define rule-based methods for determining semantic similarity measures. In order to address this issue, various semantic similarity methods have been proposed over the years. This survey article traces the evolution of such methods, categorizing them based on their underlying principles as knowledge-based, corpus-based, deep neural network-based methods, and hybrid methods. Discussing the strengths and weaknesses of each method, this survey provides a comprehensive view of existing systems in place, for new researchers to experiment and develop innovative ideas to address the issue of semantic similarity.},
	pages = {2004.13820v1},
	journaltitle = {{arXiv}},
	author = {Chandrasekaran, Dhivya and Mago, Vijay},
	date = {2020},
	note = {Type: Journal article},
	annotation = {29 pages, 5 figures, submitted to “{ACM} Computing Survey”}
}

@article{chawla_improving_2019,
	title = {Improving generation quality of pointer networks via guided attention},
	url = {https://arxiv.org/pdf/1901.11492},
	abstract = {Pointer generator networks have been used successfully for abstractive summarization. Along with the capability to generate novel words, it also allows the model to copy from the input text to handle out-of-vocabulary words. In this paper, we point out two key shortcomings of the summaries generated with this framework via manual inspection, statistical analysis and human evaluation. The first shortcoming is the extractive nature of the generated summaries, since the network eventually learns to copy from the input article most …},
	journaltitle = {{arXiv} preprint {arXiv}:1901.11492},
	author = {Chawla, K and Krishna, K and Srinivasan, {BV}},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Times cited: 1}
}

@article{chen_mocha_2020,
	title = {{MOCHA}: A Dataset for Training and Evaluating Generative Reading Comprehension Metrics},
	url = {http://arxiv.org/abs/2010.03636v2},
	abstract = {Posing reading comprehension as a generation problem provides a great deal of flexibility, allowing for open-ended questions with few restrictions on possible answers. However, progress is impeded by existing generation metrics, which rely on token overlap and are agnostic to the nuances of reading comprehension. To address this, we introduce a benchmark for training and evaluating generative reading comprehension metrics: {MOdeling} Correctness with Human Annotations. {MOCHA} contains 40K human judgement scores on model outputs from 6 diverse question answering datasets and an additional set of minimal pairs for evaluation. Using {MOCHA}, we train a Learned Evaluation metric for Reading Comprehension, {LERC}, to mimic human judgement scores. {LERC} outperforms baseline metrics by 10 to 36 absolute Pearson points on held-out annotations. When we evaluate robustness on minimal pairs, {LERC} achieves 80\% accuracy, outperforming baselines by 14 to 26 absolute percentage points while leaving significant room for improvement. {MOCHA} presents a challenging problem for developing accurate and robust generative reading comprehension metrics.},
	pages = {2010.03636v2},
	journaltitle = {{arXiv}},
	author = {Chen, Anthony and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	date = {2020},
	note = {Type: Journal article}
}

@article{chen_thorough_2016,
	title = {A Thorough Examination of the {CNN}/Daily Mail Reading Comprehension Task},
	url = {http://arxiv.org/abs/1606.02858v2},
	abstract = {Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of {NLP}. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing {CNN} and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6\% and 76.6\% on these two datasets, exceeding current state-of-the-art results by 7-10\% and approaching what we believe is the ceiling for performance on this task.},
	pages = {1606.02858v2},
	journaltitle = {{arXiv}},
	author = {Chen, Danqi and Bolton, Jason and Manning, Christopher D.},
	date = {2016},
	note = {Type: Journal article},
	annotation = {{ACL} 2016, updated results}
}

@article{chen_neural_2017,
	title = {Neural Natural Language Inference Models Enhanced with External Knowledge},
	volume = {cs.{CL}},
	url = {http://arxiv.org/abs/1711.04289v3},
	doi = {10.18653/v1/P18-1224},
	abstract = {Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference ({NLI}) from these data? If not, how can neural-network-based {NLI} models benefit from external knowledge and how to build {NLI} models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural {NLI} models to achieve the state-of-the-art performance on the {SNLI} and {MultiNLI} datasets.},
	author = {Chen, Qian and Zhu, Xiaodan and Ling, Zhen-Hua and Inkpen, Diana and Wei, Si},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Accepted by {ACL} 2018}
}

@article{chen_fast_2018,
	title = {Fast abstractive summarization with reinforce-selected sentence rewriting},
	url = {https://arxiv.org/pdf/1805.11080},
	abstract = {Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (ie, compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) …},
	journaltitle = {{arXiv} preprint {arXiv}:1805.11080},
	author = {Chen, Yen-Chun and Bansal, Mohit},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 212}
}

@book{chin-yew_looking_2004,
	title = {Looking for a Few Good Metrics: Automatic Summarization Evaluation - How Many Samples Are Enough? {NTCIR}},
	author = {Chin-Yew, Lin},
	date = {2004}
}

@article{cho_natural_2015,
	title = {Natural Language Understanding with Distributed Representation},
	volume = {cs.{CL}},
	url = {http://arxiv.org/abs/1511.07916v1},
	abstract = {This is a lecture note for the course {DS}-{GA} 3001 {\textless}Natural Language Understanding with Distributed Representation{\textgreater} at the Center for Data Science, New York University in Fall, 2015. As the name of the course suggests, this lecture note introduces readers to a neural network based approach to natural language understanding/processing. In order to make it as self-contained as possible, I spend much time on describing basics of machine learning and neural networks, only after which how they are used for natural languages is introduced. On the language front, I almost solely focus on language modelling and machine translation, two of which I personally find most fascinating and most fundamental to natural language understanding.},
	author = {Cho, Kyunghyun},
	date = {2015},
	note = {Type: Journal article}
}

@article{cho_learning_2014,
	title = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical Machine Translation},
	url = {http://arxiv.org/abs/1406.1078v3},
	abstract = {In this paper, we propose a novel neural network model called {RNN} Encoder-Decoder that consists of two recurrent neural networks ({RNN}). One {RNN} encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the {RNN} Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	pages = {1406.1078v3},
	journaltitle = {{arXiv}},
	author = {Cho, Kyunghyun and Merrienboer, Bart van and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	date = {2014},
	note = {Type: Journal article},
	annotation = {{EMNLP} 2014}
}

@article{cho_properties_2014,
	title = {On the properties of neural machine translation: Encoder-decoder approaches},
	url = {https://arxiv.org/pdf/1409.1259},
	abstract = {… A number of recent papers have proposed to use neural networks to directly learnthe condi- tional distribution from a bilingual, parallel cor- pus (Kalchbrenner {andBlunsom}, 2013; Cho et al., 2014; Sutskever et al., 2014). For …},
	journaltitle = {{arXiv} preprint {arXiv}:1409.1259},
	author = {Cho, Kyunghyun and Van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	date = {2014},
	note = {Type: Journal article},
	annotation = {Times cited: 3088}
}

@book{choi_vae-pgn_2019,
	title = {{VAE}-{PGN} based Abstractive Model in Multi-stage Architecture for Text Summarization},
	volume = {Proceedings of the 12th International Conference on Natural Language Generation},
	abstract = {This paper describes our submission to the {TL}; {DR} challenge. Neural abstractive summarization models have been successful in generating fluent and consistent summaries with advancements like the copy (Pointer-generator) and coverage mechanisms. However, these models suffer from their extractive nature as they learn to copy words from the source text. In this paper, we propose a novel abstractive model based on Variational Autoencoder ({VAE}) to address this issue. We also propose a Unified Summarization Framework for the …},
	pagetotal = {510-515},
	author = {Choi, Hyungtak and Ravuru, Lohith and Dryjanski, Tomasz and Rye, Sunghan and Lee, Donghyun and Lee, Hojung and Hwang, Inchul},
	date = {2019}
}

@article{chollet_measure_2019,
	title = {On the Measure of Intelligence},
	url = {http://arxiv.org/abs/1911.01547v2},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and {AI}. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary {AI} community still gravitates towards benchmarking intelligence by comparing the skill exhibited by {AIs} and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to “buy” arbitrary levels of skills for a system, in a way that masks the system’s own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general {AI} benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus ({ARC}), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that {ARC} can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between {AI} systems and humans.},
	pages = {1911.01547v2},
	journaltitle = {{arXiv}},
	author = {Chollet, François},
	date = {2019},
	note = {Type: Journal article}
}

@book{chopra_abstractive_2016,
	title = {Abstractive sentence summarization with attentive recurrent neural networks},
	volume = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	abstract = {Abstract Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network ({RNN}) which generates a summary of an input sentence. The conditioning is provided by a novel convolutional attention-based encoder which ensures that the decoder focuses on the appropriate input words at each step of generation. Our model relies only on learned features and is easy to train in an end-to-end fashion on large data sets. Our …},
	pagetotal = {93-98},
	author = {Chopra, Sumit and Auli, Michael and Rush, Alexander M},
	date = {2016}
}

@article{chung_empirical_2014,
	title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
	volume = {cs.{NE}},
	url = {http://arxiv.org/abs/1412.3555v1},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks ({RNNs}). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory ({LSTM}) unit and a recently proposed gated recurrent unit ({GRU}). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found {GRU} to be comparable to {LSTM}.},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, {KyungHyun} and Bengio, Yoshua},
	date = {2014},
	note = {Type: Journal article},
	annotation = {Presented in {NIPS} 2014 Deep Learning and Representation Learning Workshop}
}

@book{clark_sentence_2019,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Sentence Mover’s Similarity: Automatic Evaluation for Multi-Sentence Texts},
	publisher = {Association for Computational Linguistics},
	author = {Clark, Elizabeth and Celikyilmaz, Asli and Smith, Noah A.},
	date = {2019}
}

@article{cohan_discourse-aware_2018,
	title = {A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents},
	url = {http://arxiv.org/abs/1804.05685v2},
	abstract = {Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.},
	pages = {1804.05685v2},
	journaltitle = {{arXiv}},
	author = {Cohan, Arman and Dernoncourt, Franck and Kim, Doo Soon and Bui, Trung and Kim, Seokhwan and Chang, Walter and Goharian, Nazli},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{NAACL} {HLT} 2018}
}

@book{conroy_mind_2008,
	location = {Morristown, {NJ}, {USA}},
	title = {Mind the Gap Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality},
	publisher = {Association for Computational Linguistics},
	author = {Conroy, John M. and Dang, Hoa Trang},
	date = {2008}
}

@book{d_learning_1986,
	title = {Learning internal representations by error propagation},
	author = {D., Rumelhart and Geoffrey, E. Hinton and R., J. Williams},
	date = {1986}
}

@article{david_stacked_1992,
	title = {Stacked generalization},
	volume = {5},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608005800231},
	doi = {10.1016/S0893-6080(05)80023-1},
	abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation’s crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the {NETtalk} task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
	pages = {241--259},
	number = {2},
	journaltitle = {Neural Networks},
	author = {David, H. Wolpert},
	date = {1992},
	note = {{ISBN}: 0893-6080
	Type: Journal article},
	keywords = {Combining generalizers, cross-validation, Error estimation and correction, Generalization and induction, Learning set preprocessing}
}

@article{day_knowledge-based_2005,
	title = {A knowledge-based approach to citation extraction},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.124.5155&rep=rep1&type=pdf},
	abstract = {Integration of the bibliographical information of scholarly publications available on the Internet is an important task in academic research. To accomplish this task, accurate reference metadata extraction for scholarly publications is essential for the integration of …},
	journaltitle = {{IRI}-2005 {IEEE} …},
	author = {Day, {MY} and Tsai, {TH} and Sung, {CL} and Lee…, {CW}},
	date = {2005},
	note = {Type: Journal article},
	annotation = {Times cited: 57}
}

@book{del_corro_clausie_2013,
	title = {Clausie: clause-based open information extraction},
	volume = {Proceedings of the 22nd international conference on World Wide Web},
	abstract = {We propose {ClausIE}, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. {ClausIE} fundamentally differs from previous approaches in that it separates the detection of“useful’’pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, {ClausIE} exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of …},
	pagetotal = {355-366},
	author = {Del Corro, Luciano and Gemulla, Rainer},
	date = {2013}
}

@book{dernoncourt_repository_2018,
	title = {A repository of corpora for summarization},
	volume = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)},
	abstract = {Summarization corpora are numerous but fragmented, making it challenging for researchers to efficiently pinpoint corpora most suited to a given summarization task. In this paper, we introduce a repository containing corpora available to train and evaluate automatic summarization systems. We also present an overview of the main corpora with respect to the different summarization tasks, and identify various corpus parameters that researchers may want to consider when choosing a corpus. Lastly, as the recent successes of artificial neural …},
	author = {Dernoncourt, Franck and Ghassemi, Mohammad and Chang, Walter},
	date = {2018}
}

@article{deutsch_understanding_2020,
	title = {Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries},
	url = {http://arxiv.org/abs/2010.12495v1 http://arxiv.org/pdf/2010.12495v1},
	abstract = {Reference-based metrics such as {ROUGE} or {BERTScore} evaluate the content quality of a summary by comparing the summary to a reference. Ideally, this comparison should measure the summary’s information quality by calculating how much information the summaries have in common. In this work, we analyze the token alignments used by {ROUGE} and {BERTScore} to compare summaries and argue that their scores largely cannot be interpreted as measuring information overlap, but rather the extent to which they discuss the same topics. Further, we provide evidence that this result holds true for many other summarization evaluation metrics. The consequence of this result is that it means the summarization community has not yet found a reliable automatic metric that aligns with its research goal, to generate summaries with high-quality information. Then, we propose a simple and interpretable method of evaluating summaries which does directly measure information overlap and demonstrate how it can be used to gain insights into model behavior that could not be provided by other methods alone.},
	pages = {2010.12495v1},
	journaltitle = {{arXiv}},
	author = {Deutsch, Daniel and Roth, Dan},
	date = {2020},
	note = {Type: Journal article}
}

@article{devlin_bert_2018,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805v2},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	pages = {1810.04805v2},
	journaltitle = {{arXiv}},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	date = {2018},
	note = {Type: Journal article}
}

@article{dinan_wizard_2018,
	title = {Wizard of Wikipedia: Knowledge-Powered Conversational agents},
	url = {http://arxiv.org/abs/1811.01241v2},
	abstract = {In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically “generate and hope” generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction.},
	pages = {1811.01241v2},
	journaltitle = {{arXiv}},
	author = {Dinan, Emily and Roller, Stephen and Shuster, Kurt and Fan, Angela and Auli, Michael and Weston, Jason},
	date = {2018},
	note = {Type: Journal article}
}

@article{dipietro_analyzing_2017,
	title = {Analyzing and Exploiting {NARX} Recurrent Neural Networks for Long-Term Dependencies},
	url = {http://arxiv.org/abs/1702.07805v4},
	abstract = {Recurrent neural networks ({RNNs}) have achieved state-of-the-art performance on many diverse tasks, from machine translation to surgical activity recognition, yet training {RNNs} to capture long-term dependencies remains difficult. To date, the vast majority of successful {RNN} architectures alleviate this problem using nearly-additive connections between states, as introduced by long short-term memory ({LSTM}). We take an orthogonal approach and introduce {MIST} {RNNs}, a {NARX} {RNN} architecture that allows direct connections from the very distant past. We show that {MIST} {RNNs} 1) exhibit superior vanishing-gradient properties in comparison to {LSTM} and previously-proposed {NARX} {RNNs}; 2) are far more efficient than previously-proposed {NARX} {RNN} architectures, requiring even fewer computations than {LSTM}; and 3) improve performance substantially over {LSTM} and Clockwork {RNNs} on tasks requiring very long-term dependencies.},
	pages = {1702.07805v4},
	journaltitle = {{arXiv}},
	author = {{DiPietro}, Robert and Rupprecht, Christian and Navab, Nassir and Hager, Gregory D.},
	date = {2017},
	note = {Type: Journal article}
}

@article{dohare_text_2017,
	title = {Text Summarization using Abstract Meaning Representation},
	url = {http://arxiv.org/abs/1706.01678v3 http://arxiv.org/pdf/1706.01678v3},
	abstract = {With an ever increasing size of text present on the Internet, automatic summary generation remains an important problem for natural language understanding. In this work we explore a novel full-fledged pipeline for text summarization with an intermediate step of Abstract Meaning Representation ({AMR}). The pipeline proposed by us first generates an {AMR} graph of an input story, through which it extracts a summary graph and finally, generate summary sentences from this summary graph. Our proposed method achieves state-of-the-art results compared to the other text summarization routines based on {AMR}. We also point out some significant problems in the existing evaluation methods, which make them unsuitable for evaluating summary quality.},
	pages = {1706.01678v3},
	journaltitle = {{arXiv}},
	author = {Dohare, Shibhansh and Karnick, Harish and Gupta, Vivek},
	date = {2017},
	note = {Type: Journal article},
	annotation = {10 pages, 4 figures, Update: Added more results, corrected figures and tables}
}

@article{dong_unified_2019,
	title = {Unified Language Model Pre-training for Natural Language Understanding and Generation},
	url = {http://arxiv.org/abs/1905.03197v3 http://arxiv.org/pdf/1905.03197v3},
	abstract = {This paper presents a new Unified pre-trained Language Model ({UniLM}) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. {UniLM} compares favorably with {BERT} on the {GLUE} benchmark, and the {SQuAD} 2.0 and {CoQA} question answering tasks. Moreover, {UniLM} achieves new state-of-the-art results on five natural language generation datasets, including improving the {CNN}/{DailyMail} abstractive summarization {ROUGE}-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization {ROUGE}-L to 35.75 (0.86 absolute improvement), the {CoQA} generative question answering F1 score to 82.5 (37.1 absolute improvement), the {SQuAD} question generation {BLEU}-4 to 22.12 (3.75 absolute improvement), and the {DSTC}7 document-grounded dialog response generation {NIST}-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
	pages = {1905.03197v3},
	journaltitle = {{arXiv}},
	author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted by {NeurIPS}-19. Code and pre-trained models: https://github.com/microsoft/unilm}
}

@article{dong_survey_2018,
	title = {A survey on neural network-based summarization methods},
	url = {https://arxiv.org/pdf/1804.04589},
	abstract = {Automatic text summarization, the automated process of shortening a text while reserving the main ideas of the document (s), is a critical research area in natural language processing. The aim of this literature review is to survey the recent work on neural-based …},
	journaltitle = {{arXiv} preprint {arXiv}:1804.04589},
	author = {Dong, Y},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 15}
}

@article{dong_banditsum_2018,
	title = {Banditsum: Extractive summarization as a contextual bandit},
	url = {https://arxiv.org/pdf/1809.09672},
	abstract = {In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach {BanditSum} as it treats extractive summarization as a contextual bandit ({CB}) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences …},
	journaltitle = {{arXiv} preprint {arXiv}:1809.09672},
	author = {Dong, Yue and Shen, Yikang and Crawford, Eric and van Hoof, Herke and Cheung, Jackie Chi Kit},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 66}
}

@article{durmus_feqa_2020,
	title = {{FEQA}: A Question Answering Evaluation Framework for Faithfulness Assessment in Abstractive Summarization},
	url = {http://arxiv.org/abs/2005.03754v1},
	abstract = {Neural abstractive summarization models are prone to generate content inconsistent with the source document, i.e. unfaithful. Existing automatic metrics do not capture such mistakes effectively. We tackle the problem of evaluating faithfulness of a generated summary given its source document. We first collected human annotations of faithfulness for outputs from numerous models on two datasets. We find that current models exhibit a trade-off between abstractiveness and faithfulness: outputs with less word overlap with the source document are more likely to be unfaithful. Next, we propose an automatic question answering ({QA}) based metric for faithfulness, {FEQA}, which leverages recent advances in reading comprehension. Given question-answer pairs generated from the summary, a {QA} model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our {QA}-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries.},
	pages = {2005.03754v1},
	journaltitle = {{arXiv}},
	author = {Durmus, Esin and He, He and Diab, Mona},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted to {ACL} 2020}
}

@article{dusek_evaluating_2019,
	title = {Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E {NLG} Challenge},
	url = {http://arxiv.org/abs/1901.07931v3 http://arxiv.org/pdf/1901.07931v3},
	abstract = {This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation ({NLG}) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end {NLG} systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for {NLG} in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning {SLUG} system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper.},
	pages = {1901.07931v3},
	journaltitle = {{arXiv}},
	author = {Dušek, Ondřej and Novikova, Jekaterina and Rieser, Verena},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Computer Speech and Language, final accepted manuscript (in press)}
}

@article{edunov_classical_2017,
	title = {Classical Structured Prediction Losses for Sequence to Sequence Learning},
	url = {http://arxiv.org/abs/1711.04956v5},
	abstract = {There has been much recent work on training neural attention models at the sequence-level using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to sequence models. Our experiments show that these losses can perform surprisingly well by slightly outperforming beam search optimization in a like for like setup. We also report new state of the art results on both {IWSLT}’14 German-English translation as well as Gigaword abstractive summarization. On the larger {WMT}’14 English-French translation task, sequence-level training achieves 41.5 {BLEU} which is on par with the state of the art.},
	pages = {1711.04956v5},
	journaltitle = {{arXiv}},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David and Ranzato, Marc’Aurelio},
	date = {2017},
	note = {Type: Journal article},
	annotation = {10 pages, {NAACL} 2018}
}

@article{ellouze_mix_2017,
	title = {Mix Multiple Features to Evaluate the Content and the Linguistic Quality of Text Summaries},
	volume = {25},
	url = {http://dx.doi.org/10.20532/cit.2017.1003398},
	doi = {10.20532/cit.2017.1003398},
	pages = {149--166},
	number = {2},
	journaltitle = {Journal of Computing and Information Technology},
	author = {Ellouze, Samira and Jaoua, Maher and Hadrich Belguith, Lamia},
	date = {2017},
	note = {Publisher: Faculty of Electrical Engineering and Computing, Univ. of Zagreb
	Type: Journal article}
}

@article{elman_finding_1990,
	title = {Finding structure in time},
	volume = {14},
	url = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
	doi = {https://doi.org/10.1207/s15516709cog1402_1},
	abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current …},
	pages = {179--211},
	number = {2},
	journaltitle = {Cognitive science},
	author = {Elman, Jeffrey L},
	date = {1990},
	note = {{ISBN}: 0364-0213
	Publisher: Wiley Online Library
	Type: Journal article},
	annotation = {Times cited: 11804}
}

@book{enarvi_generating_2020,
	title = {Generating medical reports from patient-doctor conversations using sequence-to-sequence models},
	volume = {Proceedings of the first workshop on natural language processing for medical conversations},
	abstract = {We discuss automatic creation of medical reports from {ASR}-generated patient-doctor conversational transcripts using an end-to-end neural summarization approach. We explore both recurrent neural network ({RNN}) and Transformer-based sequence-to-sequence architectures for summarizing medical conversations. We have incorporated enhancements to these architectures, such as the pointer-generator network that facilitates copying parts of the conversations to the reports, and a hierarchical {RNN} encoder that makes {RNN} training …},
	pagetotal = {22-30},
	author = {Enarvi, Seppo and Amoia, Marilisa and Teba, Miguel Del-Agua and Delaney, Brian and Diehl, Frank and Hahn, Stefan and Harris, Kristina and {McGrath}, Liam and Pan, Yue and Pinto, Joel},
	date = {2020}
}

@article{ermakova_survey_2019,
	title = {A survey on evaluation of summarization methods},
	volume = {56},
	url = {http://dx.doi.org/10.1016/j.ipm.2019.04.001},
	doi = {10.1016/j.ipm.2019.04.001},
	pages = {1794--1814},
	number = {5},
	journaltitle = {Information Processing \& Management},
	author = {Ermakova, Liana and Cossu, Jean Valère and Mothe, Josiane},
	date = {2019},
	note = {Publisher: Elsevier {BV}
	Type: Journal article},
	annotation = {Times cited: 8}
}

@book{evans_straightforward_1996,
	title = {Straightforward statistics for the behavioral sciences.},
	publisher = {Thomson Brooks/Cole Publishing Co},
	author = {Evans, James D},
	date = {1996}
}

@article{fabbri_summeval_2020,
	title = {{SummEval}: Re-evaluating Summarization Evaluation},
	url = {http://arxiv.org/abs/2007.12626v3},
	abstract = {The scarcity of comprehensive up-to-date studies on evaluation metrics for text summarization and the lack of consensus regarding evaluation protocols continues to inhibit progress. We address the existing shortcomings of summarization evaluation methods along five dimensions: 1) we re-evaluate 12 automatic evaluation metrics in a comprehensive and consistent fashion using neural summarization model outputs along with expert and crowd-sourced human annotations, 2) we consistently benchmark 23 recent summarization models using the aforementioned automatic evaluation metrics, 3) we assemble the largest collection of summaries generated by models trained on the {CNN}/{DailyMail} news dataset and share it in a unified format, 4) we implement and share a toolkit that provides an extensible and unified {API} for evaluating summarization models across a broad range of automatic metrics, 5) we assemble and share the largest and most diverse, in terms of model types, collection of human judgments of model-generated summaries on the {CNN}/Daily Mail dataset annotated by both expert judges and crowd source workers. We hope that this work will help promote a more complete evaluation protocol for text summarization as well as advance research in developing evaluation metrics that better correlate with human judgements.},
	pages = {2007.12626v3},
	journaltitle = {{arXiv}},
	author = {Fabbri, Alexander R. and Kryscinski, Wojciech and {McCann}, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
	date = {2020},
	note = {Type: Journal article},
	annotation = {10 pages, 4 tables, 1 figure}
}

@article{fan_controllable_2017,
	title = {Controllable Abstractive Summarization},
	url = {http://arxiv.org/abs/1711.05217v2},
	abstract = {Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how much of the document the user has already read. We present a neural summarization model with a simple but effective mechanism to enable users to specify these high level attributes in order to control the shape of the final summaries to better suit their needs. With user input, our system can produce high quality summaries that follow user preferences. Without user input, we set the control variables automatically. On the full text {CNN}-Dailymail dataset, we outperform state of the art abstractive systems (both in terms of F1-{ROUGE}1 40.38 vs. 39.53 and human evaluation).},
	pages = {1711.05217v2},
	journaltitle = {{arXiv}},
	author = {Fan, Angela and Grangier, David and Auli, Michael},
	date = {2017},
	note = {Type: Journal article},
	annotation = {{ACL}2018 Workshop on Neural Machine Translation and Generation ({NMT}@{ACL})}
}

@article{fendji_web_2020,
	title = {From web to {SMS}: A text summarization of Wikipedia pages with character limitation},
	volume = {7},
	doi = {10.4108/eai.11-6-2020.165277},
	pages = {165277},
	number = {24},
	journaltitle = {{EAI} Endorsed Transactions on Creative Technologies},
	author = {Fendji, {JLEK} and Aminatou, {BAH}},
	date = {2020},
	note = {Publisher: European Alliance for Innovation n.o.
	Type: Journal article}
}

@article{ferreira_neural_2019,
	title = {Neural data-to-text generation: A comparison between pipeline and end-to-end architectures},
	url = {http://arxiv.org/abs/1908.09022v2},
	abstract = {Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. In contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in-between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from {RDF} triples. Both architectures were implemented making use of state-of-the art deep learning methods as the encoder-decoder Gated-Recurrent Units ({GRU}) and Transformer. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.},
	pages = {1908.09022v2},
	journaltitle = {{arXiv}},
	author = {Ferreira, Thiago Castro and Lee, Chris van der and Miltenburg, Emiel van and Krahmer, Emiel},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Preprint version of the {EMNLP} 2019 article}
}

@article{ferreto_server_2011,
	title = {Server consolidation with migration control for virtualized data centers},
	volume = {27},
	url = {http://dx.doi.org/10.1016/j.future.2011.04.016},
	doi = {10.1016/j.future.2011.04.016},
	pages = {1027--1034},
	number = {8},
	journaltitle = {Future Generation Computer Systems},
	author = {Ferreto, Tiago C. and Netto, Marco A.S. and Calheiros, Rodrigo N. and De Rose, César A.F.},
	date = {2011},
	note = {Publisher: Elsevier {BV}
	Type: Journal article},
	annotation = {Times cited: 182}
}

@book{finkel_incorporating_2005,
	title = {Incorporating non-local information into information extraction systems by gibbs sampling},
	volume = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}’05)},
	abstract = {Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this …},
	pagetotal = {363-370},
	author = {Finkel, Jenny Rose and Grenager, Trond and Manning, Christopher D},
	date = {2005}
}

@book{flanigan_discriminative_2014,
	title = {A discriminative graph-based parser for the abstract meaning representation},
	volume = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	abstract = {{\textless}span dir=ltr{\textgreater}Abstract Abstract Meaning Representation ({AMR}) is a semantic formalism for which a growing set of annotated examples is available. We introduce the first approach to parse sentences into this representation, providing a strong baseline for future improvement. The method is based on a novel algorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general …{\textless}/span{\textgreater}‏},
	pagetotal = {1426-1436},
	author = {Flanigan, Jeffrey and Thomson, Sam and Carbonell, Jaime G and Dyer, Chris and Smith, Noah A},
	date = {2014}
}

@book{foland_abstract_2017,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Abstract Meaning Representation Parsing using {LSTM} Recurrent Neural Networks},
	publisher = {Association for Computational Linguistics},
	author = {Foland, William and Martin, James H.},
	date = {2017}
}

@article{fortunato_bayesian_2017,
	title = {Bayesian Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1704.02798v4},
	abstract = {In this work we explore a straightforward variational Bayes scheme for Recurrent Neural Networks. Firstly, we show that a simple adaptation of truncated backpropagation through time can yield good quality uncertainty estimates and superior regularisation at only a small extra computational cost during training, also reducing the amount of parameters by 80\%. Secondly, we demonstrate how a novel kind of posterior approximation yields further improvements to the performance of Bayesian {RNNs}. We incorporate local gradient information into the approximate posterior to sharpen it around the current batch statistics. We show how this technique is not exclusive to recurrent neural networks and can be applied more widely to train Bayesian neural networks. We also empirically demonstrate how Bayesian {RNNs} are superior to traditional {RNNs} on a language modelling benchmark and an image captioning task, as well as showing how each of these methods improve our model over a variety of other schemes for training them. We also introduce a new benchmark for studying uncertainty for language models so future methods can be easily compared.},
	pages = {1704.02798v4},
	journaltitle = {{arXiv}},
	author = {Fortunato, Meire and Blundell, Charles and Vinyals, Oriol},
	date = {2017},
	note = {Type: Journal article},
	annotation = {12th Women in Machine Learning Workshop ({WiML} 2017), co-located with the 31st Conference on Neural Information Processing Systems ({NeurIPS} 2017), Long Beach, {CA}, {USA}}
}

@article{francois_keras_2015,
	title = {keras},
	journaltitle = {{GitHub} repository},
	author = {François, Chollet},
	date = {2015},
	note = {Publisher: {GitHub}
	Type: Journal article},
	annotation = {https://github.com/fchollet/keras}
}

@article{freund_decision-theoretic_1997,
	title = {A decision-theoretic generalization of on-line learning and an application to boosting},
	volume = {55},
	url = {https://www.sciencedirect.com/science/article/pii/S002200009791504X/pdf?md5=e471323f84c2764746c94ba206a9bc47&pid=1-s2.0-S002200009791504X-main.pdf&_valck=1},
	abstract = {In the first part of the paper we consider the problem of dynamically apportioning resources among a set of options in a worst-case on-line framework. The model we study can be interpreted as a broad, abstract extension of the well-studied on-line prediction model to a general decision-theoretic setting. We show that the multiplicative weight-update Littlestone–Warmuth rule can be adapted to this model, yielding bounds that are slightly weaker in some cases, but applicable to a considerably more general class of learning problems. We show …},
	pages = {119--139},
	number = {1},
	journaltitle = {Journal of computer and system sciences},
	author = {Freund, Yoav and Schapire, Robert E},
	date = {1997},
	note = {{ISBN}: 0022-0000
	Publisher: Elsevier
	Type: Journal article},
	annotation = {Times cited: 21149}
}

@book{friedman_elements_2001,
	title = {The elements of statistical learning},
	volume = {1(10)},
	publisher = {Springer series in statistics New York},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert and {others}},
	date = {2001}
}

@article{friedman_greedy_2001,
	title = {Greedy function approximation: a gradient boosting machine},
	url = {https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent” boosting” paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special …},
	pages = {1189--1232},
	journaltitle = {Annals of statistics},
	author = {Friedman, Jerome H},
	date = {2001},
	note = {{ISBN}: 0090-5364
	Publisher: {JSTOR}
	Type: Journal article},
	annotation = {Times cited: 12904}
}

@article{gabriel_go_2020,
	title = {Go Figure! A Meta Evaluation of Factuality in Summarization},
	url = {http://arxiv.org/abs/2010.12834v1},
	abstract = {Text generation models can generate factually inconsistent text containing distorted or fabricated facts about the source text. Recent work has focused on building evaluation models to verify the factual correctness of semantically constrained text generation tasks such as document summarization. While the field of factuality evaluation is growing fast, we don’t have well-defined criteria for measuring the effectiveness, generalizability, reliability, or sensitivity of the factuality metrics. Focusing on these aspects, in this paper, we introduce a meta-evaluation framework for evaluating factual consistency metrics. We introduce five necessary, common-sense conditions for effective factuality metrics and experiment with nine recent factuality metrics using synthetic and human-labeled factuality data from short news, long news and dialogue summarization domains. Our framework enables assessing the efficiency of any new factual consistency metric on a variety of dimensions over multiple summarization domains and can be easily extended with new meta-evaluation criteria. We also present our conclusions towards standardizing the factuality evaluation metrics.},
	pages = {2010.12834v1},
	journaltitle = {{arXiv}},
	author = {Gabriel, Saadia and Celikyilmaz, Asli and Jha, Rahul and Choi, Yejin and Gao, Jianfeng},
	date = {2020},
	note = {Type: Journal article}
}

@article{galassi_attention_2020,
	title = {Attention in Natural Language Processing},
	url = {https://ieeexplore.ieee.org/abstract/document/9194070},
	doi = {10.1109/TNNLS.2020.3019893},
	abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions …},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
	date = {2020},
	note = {{ISBN}: 2162-237X
	Publisher: {IEEE}
	Type: Journal article}
}

@article{ganesan_rouge_2018,
	title = {{ROUGE} 2.0: Updated and Improved Measures for Evaluation of Summarization Tasks},
	url = {http://arxiv.org/abs/1803.01937v1},
	abstract = {Evaluation of summarization tasks is extremely crucial to determining the quality of machine generated summaries. Over the last decade, {ROUGE} has become the standard automatic evaluation measure for evaluating summarization tasks. While {ROUGE} has been shown to be effective in capturing n-gram overlap between system and human composed summaries, there are several limitations with the existing {ROUGE} measures in terms of capturing synonymous concepts and coverage of topics. Thus, often times {ROUGE} scores do not reflect the true quality of summaries and prevents multi-faceted evaluation of summaries (i.e. by topics, by overall content coverage and etc). In this paper, we introduce {ROUGE} 2.0, which has several updated measures of {ROUGE}: {ROUGE}-N+Synonyms, {ROUGE}-Topic, {ROUGE}-Topic+Synonyms, {ROUGE}-{TopicUniq} and {ROUGE}-{TopicUniq}+Synonyms; all of which are improvements over the core {ROUGE} measures.},
	pages = {1803.01937v1},
	journaltitle = {{arXiv}},
	author = {Ganesan, Kavita},
	date = {2018},
	note = {Type: Journal article}
}

@article{gao_supert_2020,
	title = {{SUPERT}: Towards New Frontiers in Unsupervised Evaluation Metrics for Multi-Document Summarization},
	url = {http://arxiv.org/abs/2005.03724v1},
	abstract = {We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.). We propose {SUPERT}, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques. Compared to the state-of-the-art unsupervised evaluation metrics, {SUPERT} correlates better with human ratings by 18-39\%. Furthermore, we use {SUPERT} as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers. All source code is available at https://github.com/yg211/acl20-ref-free-eval.},
	pages = {2005.03724v1},
	journaltitle = {{arXiv}},
	author = {Gao, Yang and Zhao, Wei and Eger, Steffen},
	date = {2020},
	note = {Type: Journal article},
	annotation = {{ACL} 2020}
}

@article{garbacea_neural_2020,
	title = {Neural Language Generation: Formulation, Methods, and Evaluation},
	url = {http://arxiv.org/abs/2007.15780v1 http://arxiv.org/pdf/2007.15780v1},
	abstract = {Recent advances in neural network-based generative modeling have reignited the hopes in having computer systems capable of seamlessly conversing with humans and able to understand natural language. Neural architectures have been employed to generate text excerpts to various degrees of success, in a multitude of contexts and tasks that fulfil various user needs. Notably, high capacity deep learning models trained on large scale datasets demonstrate unparalleled abilities to learn patterns in the data even in the lack of explicit supervision signals, opening up a plethora of new possibilities regarding producing realistic and coherent texts. While the field of natural language generation is evolving rapidly, there are still many open challenges to address. In this survey we formally define and categorize the problem of natural language generation. We review particular application tasks that are instantiations of these general formulations, in which generating natural language is of practical importance. Next we include a comprehensive outline of methods and neural architectures employed for generating diverse texts. Nevertheless, there is no standard way to assess the quality of text produced by these generative models, which constitutes a serious bottleneck towards the progress of the field. To this end, we also review current approaches to evaluating natural language generation systems. We hope this survey will provide an informative overview of formulations, methods, and assessments of neural natural language generation.},
	pages = {2007.15780v1},
	journaltitle = {{arXiv}},
	author = {Garbacea, Cristina and Mei, Qiaozhu},
	date = {2020},
	note = {Type: Journal article},
	annotation = {70 pages}
}

@article{gatt_survey_2017,
	title = {Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation},
	url = {http://arxiv.org/abs/1703.09902v1},
	abstract = {This paper surveys the current state of the art in Natural Language Generation ({NLG}), defined as the task of generating text or speech from non-linguistic input. A survey of {NLG} is timely in view of the changes that the field has undergone over the past decade or so, especially in relation to new (usually data-driven) methods, as well as new applications of {NLG} technology. This survey therefore aims to (a) give an up-to-date synthesis of research on the core tasks in {NLG} and the architectures adopted in which such tasks are organised; (b) highlight a number of relatively recent research topics that have arisen partly as a result of growing synergies between {NLG} and other areas of artificial intelligence; (c) draw attention to the challenges in {NLG} evaluation, relating them to similar challenges faced in other areas of Natural Language Processing, with an emphasis on different evaluation methods and the relationships between them.},
	pages = {1703.09902v1},
	journaltitle = {{arXiv}},
	author = {Gatt, Albert and Krahmer, Emiel},
	date = {2017},
	note = {Type: Journal article},
	annotation = {111 pages, 8 figures, 2 tables}
}

@article{gehrmann_bottom-up_2018,
	title = {Bottom-Up Abstractive Summarization},
	url = {http://arxiv.org/abs/1808.10792v2},
	abstract = {Neural network-based methods for abstractive summarization produce outputs that are more fluent than other techniques, but which can be poor at content selection. This work proposes a simple technique for addressing this issue: use a data-efficient content selector to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on {ROUGE} for both the {CNN}-{DM} and {NYT} corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences, making it easy to transfer a trained summarizer to a new domain.},
	pages = {1808.10792v2},
	journaltitle = {{arXiv}},
	author = {Gehrmann, Sebastian and Deng, Yuntian and Rush, Alexander M.},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{EMNLP} 2018}
}

@article{geoffrey_connectionist_1989,
	title = {Connectionist Learning Procedures},
	volume = {40},
	pages = {185--234},
	journaltitle = {Artif. Intell.},
	author = {Geoffrey, E. Hinton},
	date = {1989},
	note = {Type: Journal article}
}

@book{geron_hands-machine_2019,
	location = {Sebastopol, {CA}},
	title = {Hands-on machine learning with scikit-learn, keras, and {TensorFlow}: Concepts, tools, and techniques to build intelligent systems},
	publisher = {O’Reilly Media},
	author = {Geron, Aurelien},
	date = {2019}
}

@article{gers_recurrent_nodate,
	title = {Recurrent nets that time and count},
	url = {https://ieeexplore.ieee.org/abstract/document/861302/},
	abstract = {The size of the time intervals between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While hidden Markov models tend to ignore this information, recurrent neural networks ({RNN}) can in principle learn to make use of it. We focus on long short-term memory ({LSTM}) because it usually outperforms other {RNN}. Surprisingly, {LSTM} augmented by” peephole connections” from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes …},
	journaltitle = {ieeexplore.ieee.org},
	author = {Gers, FA and Schmidhuber - Proceedings of the IEEE-INNS, J and {2000}},
	note = {Type: Journal article}
}

@article{gers_learning_1999,
	title = {Learning to forget: Continual prediction with {LSTM}},
	url = {https://digital-library.theiet.org/content/conferences/10.1049/cp_19991218},
	abstract = {Long short-term memory ({LSTM}) can solve many tasks not solvable by previous learning algorithms for recurrent neural networks ({RNNs}). We identify a weakness of {LSTM} networks processing continual input streams without explicitly marked sequence ends. Without resets, the internal state values may grow indefinitely and eventually cause the network to break down. Our remedy is an adaptive “forget gate” that enables an {LSTM} cell to learn to reset itself at appropriate times, thus releasing internal resources. We review an illustrative …},
	author = {Gers, Felix A and Schmidhuber, Jürgen and Cummins, Fred},
	date = {1999},
	note = {Publisher: {IET}
	Type: Journal article},
	annotation = {Times cited: 3432}
}

@article{gers_learning_2002,
	title = {Learning precise timing with {LSTM} recurrent networks},
	volume = {3},
	url = {http://www.jmlr.org/papers/v3/gers02a.html},
	abstract = {The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks ({RNNs}) can in principle learn to make use of it. We focus on Long Short-Term Memory ({LSTM}) because it has been shown to outperform other {RNNs} on tasks involving long time lags. We find that {LSTM} augmented by” peephole connections” from its internal cells to its multiplicative gates can learn the fine …},
	pages = {115--143},
	issue = {Aug},
	journaltitle = {Journal of machine learning research},
	author = {Gers, Felix A and Schraudolph, Nicol N and Schmidhuber, Jürgen},
	date = {2002},
	note = {Type: Journal article},
	annotation = {Times cited: 1192}
}

@article{giannakopoulos_summarization_2008,
	title = {Summarization system evaluation revisited},
	volume = {5},
	url = {http://dx.doi.org/10.1145/1410358.1410359},
	doi = {10.1145/1410358.1410359},
	abstract = {{\textless}jats:p{\textgreater}This article presents a novel automatic method ({AutoSummENG}) for the evaluation of summarization systems, based on comparing the character n-gram graphs representation of the extracted summaries and a number of model summaries. The presented approach is language neutral, due to its statistical nature, and appears to hold a level of evaluation performance that matches and even exceeds other contemporary evaluation methods. Within this study, we measure the effectiveness of different representation methods, namely, word and character n-gram graph and histogram, different n-gram neighborhood indication methods as well as different comparison methods between the supplied representations. A theory for the a priori determination of the methods’ parameters along with supporting experiments concludes the study to provide a complete alternative to existing methods concerning the automatic summary system evaluation process.{\textless}/jats:p},
	pages = {1--39},
	number = {3},
	journaltitle = {{ACM} Transactions on Speech and Language Processing},
	author = {Giannakopoulos, George and Karkaletsis, Vangelis and Vouros, George and Stamatopoulos, Panagiotis},
	date = {2008},
	note = {Publisher: Association for Computing Machinery ({ACM})
	Type: Journal article},
	annotation = {Times cited: 49}
}

@article{giles_dynamic_1994,
	title = {Dynamic recurrent neural networks: Theory and applications},
	volume = {5},
	url = {https://ieeexplore.ieee.org/abstract/document/8753425},
	abstract = {This special issue illustrates both the scientific trends of the early work in recurrent neural networks, and the mathematics of training when at least some recurrent terms of the network derivatives can be non-zero. Herein is a brief description of each of the papers. We have organized this description into two parts. The first part contains the papers that are mainly theoretical, and the second part contains the papers that are mainly applications. The order of papers is alphabetical by first author.},
	pages = {153--156},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Giles, C Lee and Kuhn, Gary M and Williams, Ronald J},
	date = {1994},
	note = {{ISBN}: 1045-9227
	Publisher: {IEEE}
	Type: Journal article},
	annotation = {Times cited: 127}
}

@book{gilpin_explaining_2018,
	title = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
	publisher = {{IEEE}},
	author = {Gilpin, Leilani H and Bau, David and Yuan, Ben Z and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	date = {2018}
}

@article{goldberg_primer_2015,
	title = {A Primer on Neural Network Models for Natural Language Processing},
	url = {http://arxiv.org/abs/1510.00726v1},
	abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
	pages = {1510.00726v1},
	journaltitle = {{arXiv}},
	author = {Goldberg, Yoav},
	date = {2015},
	note = {Type: Journal article}
}

@article{goldberg_neural_2017,
	title = {Neural network methods for natural language processing},
	volume = {10},
	url = {https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037},
	doi = {10.2200/S00762ED1V01Y201703HLT037},
	abstract = {Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and {II}) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is …},
	pages = {1--309},
	number = {1},
	journaltitle = {Synthesis Lectures on Human Language Technologies},
	author = {Goldberg, Yoav},
	date = {2017},
	note = {{ISBN}: 1947-4040
	Publisher: Morgan \& Claypool Publishers
	Type: Journal article}
}

@article{gomaa_survey_2013,
	title = {A survey of text similarity approaches},
	volume = {68},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.403.5446&rep=rep1&type=pdf},
	abstract = {Measuring the similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. This survey discusses the existing works on text similarity through partitioning them into three approaches; String-based, Corpus-based and Knowledgebased similarities. Furthermore, samples of combination between these …},
	pages = {13--18},
	number = {13},
	journaltitle = {International Journal of Computer Applications},
	author = {Gomaa, Wael H and Fahmy, Aly A},
	date = {2013},
	note = {Publisher: Citeseer
	Type: Journal article},
	annotation = {Times cited: 667}
}

@book{gonzalez_ipa_2014,
	title = {{IPA} and {STOUT}: Leveraging linguistic and source-based features for machine translation evaluation},
	volume = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
	abstract = {{\textless}span dir=ltr{\textgreater}This paper describes the {UPC} submissions to the {WMT}14 Metrics Shared Task: {UPCIPA} and {UPC}-{STOUT}. These metrics use a collection of evaluation measures integrated in {ASIYA}, a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than …{\textless}/span{\textgreater}‏},
	pagetotal = {394-401},
	author = {Gonzalez, Meritxell and Barrón-Cedeno, Alberto and Màrquez, Lluís},
	date = {2014}
}

@book{goodrich_assessing_2019,
	location = {New York, {NY}, {USA}},
	title = {Assessing The Factual Accuracy of Generated Text},
	publisher = {{ACM}},
	author = {Goodrich, Ben and Rao, Vinay and Liu, Peter J. and Saleh, Mohammad},
	date = {2019}
}

@book{grandl_multi-resource_2014,
	location = {New York, {NY}, {USA}},
	title = {Multi-resource packing for cluster schedulers},
	publisher = {{ACM}},
	author = {Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya},
	date = {2014}
}

@incollection{graves_supervised_2012,
	location = {Berlin, Heidelberg},
	title = {Supervised Sequence Labelling},
	pages = {5--13},
	publisher = {Springer Berlin Heidelberg},
	author = {Graves, Alex},
	date = {2012}
}

@article{graves_generating_2013,
	title = {Generating sequences with recurrent neural networks},
	url = {https://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online …},
	journaltitle = {{arXiv} preprint {arXiv}:1308.0850},
	author = {Graves, Alex},
	date = {2013},
	note = {Type: Journal article}
}

@article{graves_neural_2014,
	title = {Neural Turing Machines},
	url = {http://arxiv.org/abs/1410.5401v2 http://arxiv.org/pdf/1410.5401v2},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	pages = {1410.5401v2},
	journaltitle = {{arXiv}},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	date = {2014},
	note = {Type: Journal article}
}

@article{groschwitz_amr_2018,
	title = {{AMR} Dependency Parsing with a Typed Semantic Algebra},
	url = {http://arxiv.org/abs/1805.11465v1 http://arxiv.org/pdf/1805.11465v1},
	abstract = {We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an {AMR} graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.},
	pages = {1805.11465v1},
	journaltitle = {{arXiv}},
	author = {Groschwitz, Jonas and Lindemann, Matthias and Fowlie, Meaghan and Johnson, Mark and Koller, Alexander},
	date = {2018},
	note = {Type: Journal article},
	annotation = {This paper will be presented at {ACL} 2018 (see https://acl2018.org/programme/papers/)}
}

@article{grusky_newsroom_2018,
	title = {Newsroom: A Dataset of 1.3 Million Summaries with Diverse Extractive Strategies},
	url = {http://arxiv.org/abs/1804.11283v2},
	abstract = {We present {NEWSROOM}, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in {NEWSROOM} summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges.},
	pages = {1804.11283v2},
	journaltitle = {{arXiv}},
	author = {Grusky, Max and Naaman, Mor and Artzi, Yoav},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Proceedings of {NAACL}-{HLT} 2018 (Long Paper)}
}

@article{gu_incorporating_2016,
	title = {Incorporating Copying Mechanism in Sequence-to-Sequence Learning},
	url = {http://arxiv.org/abs/1603.06393v3},
	abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called {CopyNet} with encoder-decoder structure. {CopyNet} can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of {CopyNet}. For example, {CopyNet} can outperform regular {RNN}-based model with remarkable margins on text summarization tasks.},
	pages = {1603.06393v3},
	journaltitle = {{arXiv}},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.},
	date = {2016},
	note = {Type: Journal article},
	annotation = {10 pages, 5 figures, accepted by {ACL}2016}
}

@article{gu_perception_2020,
	title = {Perception Score, A Learned Metric for Open-ended Text Generation Evaluation},
	url = {http://arxiv.org/abs/2008.03082v2},
	abstract = {Automatic evaluation for open-ended natural language generation tasks remains a challenge. Existing metrics such as {BLEU} show a low correlation with human judgment. We propose a novel and powerful learning-based evaluation metric: Perception Score. The method measures the overall quality of the generation and scores holistically instead of only focusing on one evaluation criteria, such as word overlapping. Moreover, it also shows the amount of uncertainty about its evaluation result. By connecting the uncertainty, Perception Score gives a more accurate evaluation for the generation system. Perception Score provides state-of-the-art results on two conditional generation tasks and two unconditional generation tasks.},
	pages = {2008.03082v2},
	journaltitle = {{arXiv}},
	author = {Gu, Jing and Wu, Qingyang and Yu, Zhou},
	date = {2020},
	note = {Type: Journal article},
	annotation = {8 pages, 2 figures}
}

@article{gulcehre_pointing_2016,
	title = {Pointing the Unknown Words},
	url = {http://arxiv.org/abs/1603.08148v3},
	abstract = {The problem of rare and unknown words is an important issue that can potentially influence the performance of many {NLP} systems, including both the traditional count-based and the deep learning models. We propose a novel way to deal with the rare and unseen words for the neural network models using attention. Our model uses two softmax layers in order to predict the next word in conditional language models: one predicts the location of a word in the source sentence, and the other predicts a word in the shortlist vocabulary. At each time-step, the decision of which softmax layer to use choose adaptively made by an {MLP} which is conditioned on the context. We motivate our work from a psychological evidence that humans naturally have a tendency to point towards objects in the context or the environment when the name of an object is not known. We observe improvements on two tasks, neural machine translation on the Europarl English to French parallel corpora and text summarization on the Gigaword dataset using our proposed model.},
	pages = {1603.08148v3},
	journaltitle = {{arXiv}},
	author = {Gulcehre, Caglar and Ahn, Sungjin and Nallapati, Ramesh and Zhou, Bowen and Bengio, Yoshua},
	date = {2016},
	note = {Type: Journal article},
	annotation = {{ACL} 2016 Oral Paper}
}

@article{gunel_mind_2020,
	title = {Mind The Facts: Knowledge-Boosted Coherent Abstractive Text Summarization},
	url = {http://arxiv.org/abs/2006.15435v1 http://arxiv.org/pdf/2006.15435v1},
	abstract = {Neural models have become successful at producing abstractive summaries that are human-readable and fluent. However, these models have two critical shortcomings: they often don’t respect the facts that are either included in the source article or are known to humans as commonsense knowledge, and they don’t produce coherent summaries when the source article is long. In this work, we propose a novel architecture that extends Transformer encoder-decoder architecture in order to improve on these shortcomings. First, we incorporate entity-level knowledge from the Wikidata knowledge graph into the encoder-decoder architecture. Injecting structural world knowledge from Wikidata helps our abstractive summarization model to be more fact-aware. Second, we utilize the ideas used in Transformer-{XL} language model in our proposed encoder-decoder architecture. This helps our model with producing coherent summaries even when the source article is long. We test our model on {CNN}/Daily Mail summarization dataset and show improvements on {ROUGE} scores over the baseline Transformer model. We also include model predictions for which our model accurately conveys the facts, while the baseline Transformer model doesn’t.},
	pages = {2006.15435v1},
	journaltitle = {{arXiv}},
	author = {Gunel, Beliz and Zhu, Chenguang and Zeng, Michael and Huang, Xuedong},
	date = {2020},
	note = {Type: Journal article},
	annotation = {{NeurIPS} 2019, Knowledge Representation \& Reasoning Meets Machine Learning ({KR}2ML workshop)}
}

@article{guo_soft_2018,
	title = {Soft layer-specific multi-task summarization with entailment and question generation},
	url = {https://arxiv.org/pdf/1805.11004},
	abstract = {An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also …},
	journaltitle = {{arXiv} preprint {arXiv}:1805.11004},
	author = {Guo, H and Pasunuru, R and Bansal, M},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 73}
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	url = {https://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf?ref=driverlayer.com/web},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The …},
	pages = {1157--1182},
	issue = {Mar},
	journaltitle = {Journal of machine learning research},
	author = {Guyon, Isabelle and Elisseeff, André},
	date = {2003},
	note = {Type: Journal article},
	annotation = {Times cited: 16233}
}

@book{hall_feature_1999,
	title = {Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper.},
	volume = {{FLAIRS} conference 1999},
	abstract = {Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a filter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs …},
	pagetotal = {235-239},
	author = {Hall, Mark A and Smith, Lloyd A},
	date = {1999}
}

@article{haonan_exploring_2020,
	title = {Exploring Explainable Selection to Control Abstractive Summarization},
	url = {http://arxiv.org/abs/2004.11779v2 http://arxiv.org/pdf/2004.11779v2},
	abstract = {Like humans, document summarization models can interpret a document’s contents in a number of ways. Unfortunately, the neural models of today are largely black boxes that provide little explanation of how or why they generated a summary in the way they did. Therefore, to begin prying open the black box and to inject a level of control into the substance of the final summary, we developed a novel select-and-generate framework that focuses on explainability. By revealing the latent centrality and interactions between sentences, along with scores for sentence novelty and relevance, users are given a window into the choices a model is making and an opportunity to guide those choices in a more desirable direction. A novel pair-wise matrix captures the sentence interactions, centrality, and attribute scores, and a mask with tunable attribute thresholds allows the user to control which sentences are likely to be included in the extraction. A sentence-deployed attention mechanism in the abstractor ensures the final summary emphasizes the desired content. Additionally, the encoder is adaptable, supporting both Transformer- and {BERT}-based configurations. In a series of experiments assessed with {ROUGE} metrics and two human evaluations, {ESCA} outperformed eight state-of-the-art models on the {CNN}/{DailyMail} and {NYT}50 benchmark datasets.},
	pages = {2004.11779v2},
	journaltitle = {{arXiv}},
	author = {Haonan, Wang and Yang, Gao and Yu, Bai and Lapata, Mirella and Heyan, Huang},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted by {AAAI}’2021. Include all Appendices}
}

@article{hardy_highres_2019,
	title = {{HighRES}: Highlight-based Reference-less Evaluation of Summarization},
	url = {http://arxiv.org/abs/1906.01361v1 http://arxiv.org/pdf/1906.01361v1},
	abstract = {There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization ({HighRES}), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that {HighRES} improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.},
	pages = {1906.01361v1},
	journaltitle = {{arXiv}},
	author = {{Hardy} and Narayan, Shashi and Vlachos, Andreas},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted for {ACL} 2019}
}

@article{hardy_guided_2018,
	title = {Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation},
	url = {http://arxiv.org/abs/1808.09160v1 http://arxiv.org/pdf/1808.09160v1},
	abstract = {Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation ({AMR}) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in {ROUGE}-2 using gold standard {AMR} parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance using the latter is 2 {ROUGE}-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset. Code is available at https://github.com/sheffieldnlp/{AMR}2Text-summ},
	pages = {1808.09160v1},
	journaltitle = {{arXiv}},
	author = {{Hardy} and Vlachos, Andreas},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Accepted in {EMNLP} 2018}
}

@book{harnly_automation_2005,
	title = {Automation of summary evaluation by the pyramid method},
	volume = {Recent Advances in Natural Language Processing ({RANLP})},
	abstract = {The manual Pyramid method for summary evaluation, which focuses on the task of determining if a summary expresses the same content as a set of manual models, has shown sufficient promise that the Document Understanding Conference 2005 effort will make use of it. However, an automated approach would make the method far more useful for developers and evaluators of automated summarization systems. We present an experimental environment for testing automated evaluation of summaries, pre-annotated for …},
	pagetotal = {226-232},
	author = {Harnly, Aaron and Nenkova, Ani and Passonneau, Rebecca and Rambow, Owen},
	date = {2005}
}

@article{hashimoto_unifying_2019,
	title = {Unifying Human and Statistical Evaluation for Natural Language Generation},
	url = {http://arxiv.org/abs/1904.02792v1 http://arxiv.org/pdf/1904.02792v1},
	abstract = {How can we measure whether a natural language generation system produces both high quality and diverse outputs? Human evaluation captures quality but not diversity, as it does not catch models that simply plagiarize from the training set. On the other hand, statistical evaluation (i.e., perplexity) captures diversity but not quality, as models that occasionally emit low quality samples would be insufficiently penalized. In this paper, we propose a unified framework which evaluates both diversity and quality, based on the optimal error rate of predicting whether a sentence is human- or machine-generated. We demonstrate that this error rate can be efficiently estimated by combining human and statistical evaluation, using an evaluation metric which we call {HUSE}. On summarization and chit-chat dialogue, we show that (i) {HUSE} detects diversity defects which fool pure human evaluation and that (ii) techniques such as annealing for improving quality actually decrease {HUSE} due to decreased diversity.},
	pages = {1904.02792v1},
	journaltitle = {{arXiv}},
	author = {Hashimoto, Tatsunori B. and Zhang, Hugh and Liang, Percy},
	date = {2019},
	note = {Type: Journal article},
	annotation = {{NAACL} Camera Ready Submission}
}

@article{hastings_monte_1970,
	title = {Monte Carlo sampling methods using Markov chains and their applications},
	volume = {57},
	url = {http://dx.doi.org/10.1093/biomet/57.1.97},
	doi = {10.1093/biomet/57.1.97},
	pages = {97--109},
	number = {1},
	journaltitle = {Biometrika},
	author = {Hastings, W. K.},
	date = {1970},
	note = {Publisher: Oxford University Press ({OUP})
	Type: Journal article},
	annotation = {Times cited: 7297}
}

@article{haviv_understanding_2019,
	title = {Understanding and Controlling Memory in Recurrent Neural Networks},
	volume = {cs.{LG}},
	url = {http://arxiv.org/abs/1902.07275v3},
	abstract = {To be effective in sequential data processing, Recurrent Neural Networks ({RNNs}) are required to keep track of past events by creating memories. While the relation between memories and the network’s hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network’s hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a ‘slow point’. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
	author = {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
	date = {2019},
	note = {Type: Journal article},
	annotation = {The link to the code was changed due to technical issues with the original repository. We no longer refer to the process shown in Figure 5C as a bifurcation diagram, but describe it in a more precise manner. We thank an anonymous reviewer for pointing this out}
}

@article{he_jointly_2018,
	title = {Jointly predicting predicates and arguments in neural semantic role labeling},
	journaltitle = {{arXiv} preprint {arXiv}:1805.04787},
	author = {He, Luheng and Lee, Kenton and Levy, Omer and Zettlemoyer, Luke},
	date = {2018},
	note = {Type: Journal article}
}

@article{hermann_teaching_2015,
	title = {Teaching Machines to Read and Comprehend},
	url = {http://arxiv.org/abs/1506.03340v3 http://arxiv.org/pdf/1506.03340v3},
	abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
	pages = {1506.03340v3},
	journaltitle = {{arXiv}},
	author = {Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	date = {2015},
	note = {Type: Journal article},
	annotation = {Appears in: Advances in Neural Information Processing Systems 28 ({NIPS} 2015). 14 pages, 13 figures}
}

@article{hinton_fast_2006,
	title = {A fast learning algorithm for deep belief nets},
	volume = {18},
	url = {https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.2006.18.7.1527},
	abstract = {We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed …},
	pages = {1527--1554},
	number = {7},
	journaltitle = {Neural computation},
	author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	date = {2006},
	note = {{ISBN}: 0899-7667
	Publisher: {MIT} Press
	Type: Journal article},
	annotation = {Times cited: 14070}
}

@article{hinton_improving_2012,
	title = {Improving neural networks by preventing co-adaptation of feature detectors},
	url = {https://arxiv.org/pdf/1207.0580.pdf)},
	abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This” overfitting” is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in …},
	journaltitle = {{arXiv} preprint {arXiv}:1207.0580},
	author = {Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R},
	date = {2012},
	note = {Type: Journal article},
	annotation = {Times cited: 5635}
}

@book{ho_random_1995,
	location = {{USA}},
	title = {Random Decision Forests},
	abstract = {Decision trees are attractive classifiers due to their high execution speed. But trees derived with traditional methods often cannot be grown to arbitrary complexity for possible loss of generalization accuracy on unseen data. The limitation on complexity usually means suboptimal accuracy on training data. Following the principles of stochastic modeling, we propose a method to construct tree-based classifiers whose capacity can be arbitrarily expanded for increases in accuracy for both training and unseen data. The essence of the method is to build multiple trees in randomly selected subspaces of the feature space. Trees in, different subspaces generalize their classification in complementary ways, and their combined classification can be monotonically improved. The validity of the method is demonstrated through experiments on the recognition of handwritten digits.},
	pagetotal = {278},
	publisher = {{IEEE} Computer Society},
	author = {Ho, Tin Kam},
	date = {1995},
	keywords = {complexity, decision theory, decision trees, generalization accuracy, handwriting recognition, handwritten digits, optical character recognition, random decision forests, stochastic modeling, suboptimal accuracy, tree-based classifiers}
}

@book{dang_overview_2005,
	title = {Overview of {DUC} 2005},
	author = {Dang, Hoa Trang},
	date = {2005}
}

@article{hochreiter_gradient_2001,
	title = {Gradient flow in recurrent nets: the difficulty of learning long-term dependencies},
	abstract = {Recurrent networks (crossreference Chapter 12) can, in principle, use their feedback connections to store representations of recent input events in the form of activations. The most widely used algorithms for learning what to put in short-term memory, however, take …},
	author = {Hochreiter, Sepp and Bengio, Yoshua and Frasconi, Paolo and Schmidhuber, Jürgen},
	date = {2001},
	note = {Publisher: A field guide to dynamical recurrent neural networks. {IEEE} Press
	Type: Journal article},
	annotation = {Times cited: 1312}
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf},
	abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter’s (1991) analysis of this problem, then address it by introducing a novel …},
	pages = {1735--1780},
	number = {8},
	journaltitle = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	date = {1997},
	note = {{ISBN}: 0899-7667
	Publisher: {MIT} Press
	Type: Journal article},
	annotation = {Times cited: 38015}
}

@article{hovy_automated_1999,
	title = {Automated text summarization in {SUMMARIST}},
	volume = {14},
	url = {https://books.google.com/books?hl=en&lr=&id=YtUZQaKDmzEC&oi=fnd&pg=PA81&dq=Automated+Text+Summarization+in+SUMMARIST&ots=ZqrCtoD-hA&sig=lj0018wrdK1OHnRDMKVfWF21QYI},
	abstract = {{SUMMARIST} is an attempt to create a robust automated text summarization system, based on the’equation’: summarization= topic identification+ interpretation+ generation. Each of these stages contains several independent modules, many of them trained on large corpora …},
	pages = {81--94},
	journaltitle = {Advances in automatic text summarization},
	author = {Hovy, Eduard and Lin, Chin-Yew},
	date = {1999},
	note = {Publisher: {MIT} press Cambridge, {MA}
	Type: Journal article},
	annotation = {Times cited: 922}
}

@article{hsu_unified_2018,
	title = {A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss},
	url = {http://arxiv.org/abs/1805.06266v2},
	abstract = {We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high {ROUGE} scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art {ROUGE} scores while being the most informative and readable summarization on the {CNN}/Daily Mail dataset in a solid human evaluation.},
	pages = {1805.06266v2},
	journaltitle = {{arXiv}},
	author = {Hsu, Wan-Ting and Lin, Chieh-Kai and Lee, Ming-Ying and Min, Kerui and Tang, Jing and Sun, Min},
	date = {2018},
	note = {Type: Journal article},
	annotation = {9 pages, {ACL} 2018 oral. Project page: https://hsuwanting.github.io/unified\_summ/. Code: https://github.com/{HsuWanTing}/unified-summarization}
}

@article{hu_lcsts_2015,
	title = {{LCSTS}: A Large Scale Chinese Short Text Summarization Dataset},
	volume = {cs.{CL}},
	url = {http://arxiv.org/abs/1506.05865v4},
	abstract = {Automatic text summarization is widely regarded as the highly difficult problem, partially because of the lack of large text summarization data set. Due to the great challenge of constructing the large scale summaries for full text, in this paper, we introduce a large corpus of Chinese short text summarization dataset constructed from the Chinese microblogging website Sina Weibo, which is released to the public http://icrc.hitsz.edu.cn/Article/show/139.html. This corpus consists of over 2 million real Chinese short texts with short summaries given by the author of each text. We also manually tagged the relevance of 10,666 short summaries with their corresponding short texts. Based on the corpus, we introduce recurrent neural network for the summary generation and achieve promising results, which not only shows the usefulness of the proposed corpus for short text summarization research, but also provides a baseline for further research on this topic.},
	author = {Hu, Baotian and Chen, Qingcai and Zhu, Fangze},
	date = {2015},
	note = {Type: Journal article},
	annotation = {Recently, we received feedbacks from Yuya Taguchi from {NAIST} in Japan and Qian Chen from {USTC} of China, that the results in the {EMNLP}2015 version seem to be underrated. So we carefully checked our results and find out that we made a mistake while using the standard {ROUGE}. Then we re-evaluate all methods in the paper and get corrected results listed in Table 2 of this version}
}

@article{hu_introductory_2018,
	title = {An Introductory Survey on Attention Mechanisms in {NLP} Problems},
	url = {http://arxiv.org/abs/1811.05544v1},
	abstract = {First derived from human intuition, later adapted to machine translation for automatic token alignment, attention mechanism, a simple method that can be used for encoding sequence data based on the importance score each element is assigned, has been widely applied to and attained significant improvement in various tasks in natural language processing, including sentiment classification, text summarization, question answering, dependency parsing, etc. In this paper, we survey through recent works and conduct an introductory summary of the attention mechanism in different {NLP} problems, aiming to provide our readers with basic knowledge on this widely used method, discuss its different variants for different tasks, explore its association with other techniques in machine learning, and examine methods for evaluating its performance.},
	pages = {1811.05544v1},
	journaltitle = {{arXiv}},
	author = {Hu, Dichao},
	date = {2018},
	note = {Type: Journal article},
	annotation = {9 pages}
}

@book{hu_automatic_2014,
	title = {Automatic generation of related work sections in scientific papers: an optimization approach},
	volume = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	abstract = {In this paper, we investigate a challenging task of automatic related work generation. Given multiple reference papers as input, the task aims to generate a related work section for a target paper. The generated related work section can be used as a draft for the author to complete his or her final related work section. We propose our Automatic Related Work Generation system called {ARWG} to address this task. It first exploits a {PLSA} model to split the sentence set of the given papers into different topic-biased parts, and then applies …},
	pagetotal = {1624-1633},
	author = {Hu, Yue and Wan, Xiaojun},
	date = {2014}
}

@article{huang_what_2020,
	title = {What Have We Achieved on Text Summarization},
	url = {http://arxiv.org/abs/2010.04529v1 http://arxiv.org/pdf/2010.04529v1},
	abstract = {Deep learning has led to significant improvement in text summarization with various methods investigated and improved {ROUGE} scores reported over the years. However, gaps still exist between summaries produced by automatic summarizers and human professionals. Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric({MQM}) and quantify 8 major sources of errors on 10 representative summarization models manually. Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with {BART} giving the best results.},
	pages = {2010.04529v1},
	journaltitle = {{arXiv}},
	author = {Huang, Dandan and Cui, Leyang and Yang, Sen and Bao, Guangsheng and Wang, Kun and Xie, Jun and Zhang, Yue},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted by {EMNLP} 2020}
}

@article{huang_knowledge_2020,
	title = {Knowledge Graph-Augmented Abstractive Summarization with Semantic-Driven Cloze Reward},
	url = {http://arxiv.org/abs/2005.01159v1 http://arxiv.org/pdf/2005.01159v1},
	abstract = {Sequence-to-sequence models for abstractive summarization have been studied extensively, yet the generated summaries commonly suffer from fabricated content, and are often found to be near-extractive. We argue that, to address these issues, the summarizer should acquire semantic interpretation over input, e.g., via structured representation, to allow the generation of more informative summaries. In this paper, we present {ASGARD}, a novel framework for Abstractive Summarization with Graph-Augmentation and semantic-driven {RewarD}. We propose the use of dual encoders—a sequential document encoder and a graph-structured encoder—to maintain the global context and local characteristics of entities, complementing each other. We further design a reward based on a multiple choice cloze test to drive the model to better capture entity interactions. Results show that our models produce significantly higher {ROUGE} scores than a variant without knowledge graph as input on both New York Times and {CNN}/Daily Mail datasets. We also obtain better or comparable performance compared to systems that are fine-tuned from large pretrained language models. Human judges further rate our model outputs as more informative and containing fewer unfaithful errors.},
	pages = {2005.01159v1},
	journaltitle = {{arXiv}},
	author = {Huang, Luyang and Wu, Lingfei and Wang, Lu},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted as a long paper to {ACL} 2020}
}

@book{hussin_efficient_2011,
	title = {Efficient Energy Management Using Adaptive Reinforcement Learning-Based Scheduling in Large-Scale Distributed Systems},
	publisher = {{IEEE}},
	author = {Hussin, Masnida and Lee, Young Choon and Zomaya, Albert Y.},
	date = {2011}
}

@book{i_energy-efficient_2010,
	title = {Energy-efficient application-aware online provisioning for virtualized clouds and data centers},
	abstract = {As energy efficiency and associated costs become key concerns, consolidated and virtualized data centers and clouds are attractive computing platforms for data- and compute- intensive applications. These platforms provide an abstraction of nearly-unlimited computing resources through the elastic use of pools of consolidated resources, and provide opportunities for higher utilization and energy savings. Recently, these platforms are also being considered for more traditional high-performance computing ({HPC}) applications that have typically targeted Grids and similar conventional {HPC} platforms. However, maximizing energy efficiency, cost-effectiveness, and utilization for these applications while ensuring performance and other Quality of Service ({QoS}) guarantees, requires leveraging important and extremely challenging tradeoffs. These include, for example, the tradeoff between the need to efficiently create and provision Virtual Machines ({VMs}) on data center resources and the need to accommodate the heterogeneous resource demands and runtimes of these applications. In this paper we present an energy-aware online provisioning approach for {HPC} applications on consolidated and virtualized computing platforms. Energy efficiency is achieved using a workload-aware, just-right dynamic provisioning mechanism and the ability to power down subsystems of a host system that are not required by the {VMs} mapped to it. We evaluate the presented approach using real {HPC} workload traces from widely distributed production systems. The results presented demonstrated that compared to typical reactive or predefined provisioning, our approach achieves significant improvements in energy efficiency with an acceptable {QoS} penalty.},
	pagetotal = {31-45},
	author = {I., Rodero and J., Jaramillo and A., Quiroz and M., Parashar and F., Guim and S., Poole},
	date = {2010-08},
	keywords = {Algorithm design and analysis, Autonomic Computing, Cloud Computing, Clouds, Clustering algorithms, computer centres, Data Center, data center resources, distributed production systems, dynamic provisioning mechanism, Efficiency, Energy, energy efficiency, energy-aware online provisioning approach, energy-efficient application-aware online provisioning, grid computing, heterogeneous resource demands, high-performance computing, {HPC} applications, Internet, {QoS} guarantees, quality of service, Quality of service, Resource management, Resource Provisioning, Runtime, Servers, virtual machines, Virtualization, virtualized clouds, virtualized computing platforms, virtualized data centers}
}

@book{ian_deep_2016,
	title = {Deep Learning},
	publisher = {{MIT} Press},
	author = {Ian, Goodfellow and Yoshua, Bengio and Aaron, Courville},
	date = {2016}
}

@article{ibrahim_altmami_automatic_2020,
	title = {Automatic summarization of scientific articles: A survey},
	url = {http://dx.doi.org/10.1016/j.jksuci.2020.04.020},
	doi = {10.1016/j.jksuci.2020.04.020},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	author = {Ibrahim Altmami, Nouf and El Bachir Menai, Mohamed},
	date = {2020},
	note = {Publisher: Elsevier {BV}
	Type: Journal article}
}

@book{james_introduction_2014,
	title = {An Introduction to Statistical Learning: With Applications in R},
	abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	date = {2014}
}

@article{jiang_closed-book_2018,
	title = {Closed-Book Training to Improve Summarization Encoder Memory},
	url = {http://arxiv.org/abs/1809.04585v1},
	abstract = {A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder’s memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional ‘closed-book’ decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can’t rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the {CNN}/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of {ROUGE} and {METEOR} metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only {DUC}-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.},
	pages = {1809.04585v1},
	journaltitle = {{arXiv}},
	author = {Jiang, Yichen and Bansal, Mohit},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{EMNLP} 2018 (16 pages)}
}

@book{jozefowicz_empirical_2015,
	title = {An empirical exploration of recurrent network architectures},
	volume = {International conference on machine learning},
	abstract = {Abstract The Recurrent Neural Network ({RNN}) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory ({LSTM}) is a specific {RNN} architecture whose design makes it much easier to train. While wildly successful in practice, the {LSTM}’s architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. In this work, we aim to determine whether the {LSTM} architecture is optimal or whether much better architectures exist. We …},
	pagetotal = {2342-2350},
	author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
	date = {2015}
}

@book{jung_mistral_2010,
	title = {Mistral: Dynamically Managing Power, Performance, and Adaptation Cost in Cloud Infrastructures},
	publisher = {{IEEE}},
	author = {Jung, Gueyoung and Hiltunen, Matti A. and Joshi, Kaustubh R. and Schlichting, Richard D. and Pu, Calton},
	date = {2010}
}

@book{k_computing_2011,
	title = {Computing Krippendorff’s Alpha-Reliability},
	author = {K., Krippendorff},
	date = {2011}
}

@article{k_high-performance_2005,
	title = {High-performance, power-aware distributed computing for scientific applications},
	volume = {38},
	doi = {10.1109/MC.2005.380},
	abstract = {The {PowerPack} framework enables distributed systems to profile, analyze, and conserve energy in scientific applications using dynamic voltage scaling. For one common benchmark, the framework achieves more than 30 percent energy savings with minimal performance impact.},
	pages = {40--47},
	number = {11},
	journaltitle = {Computer},
	author = {K., W. Cameron and Rong, Ge and Xizhou, Feng},
	date = {2005-11},
	note = {{ISBN}: 1558-0814
	Type: Journal article},
	keywords = {Biological system modeling, Computational modeling, Costs, Distributed computing, Distributed systems, {DNA}, dynamic voltage scaling, Dynamic voltage scaling, Energy consumption, Large-scale systems, natural sciences computing, parallel processing, parallel systems, power consumption, Power system reliability, Power-aware computing, power-aware distributed computing, {PowerPack} framework, scientific applications, Temperature, Weather forecasting}
}

@article{kedzie_content_2018,
	title = {Content Selection in Deep Learning Models of Summarization},
	url = {http://arxiv.org/abs/1810.12343v2},
	abstract = {We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the summarization task.},
	pages = {1810.12343v2},
	journaltitle = {{arXiv}},
	author = {Kedzie, Chris and {McKeown}, Kathleen and {III}, Hal Daume},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Revised to correct for error in {AMI} oracle results. Originally published at {EMNLP} 2018}
}

@article{khemka_utility_2015,
	title = {Utility maximizing dynamic resource management in an oversubscribed energy-constrained heterogeneous computing system},
	volume = {5},
	url = {http://dx.doi.org/10.1016/j.suscom.2014.08.001},
	doi = {10.1016/j.suscom.2014.08.001},
	pages = {14--30},
	journaltitle = {Sustainable Computing: Informatics and Systems},
	author = {Khemka, Bhavesh and Friese, Ryan and Pasricha, Sudeep and Maciejewski, Anthony A. and Siegel, Howard Jay and Koenig, Gregory A. and Powers, Sarah and Hilton, Marcia and Rambharos, Rajendra and Poole, Steve},
	date = {2015},
	note = {Publisher: Elsevier {BV}
	Type: Journal article},
	annotation = {Times cited: 19}
}

@article{kim_abstractive_2018,
	title = {Abstractive Summarization of Reddit Posts with Multi-level Memory Networks},
	url = {http://arxiv.org/abs/1811.00783v2},
	abstract = {We address the problem of abstractive summarization in two directions: proposing a novel dataset and a new model. First, we collect Reddit {TIFU} dataset, consisting of 120K posts from the online discussion forum Reddit. We use such informal crowd-generated posts as text source, in contrast with existing datasets that mostly use formal documents as source such as news articles. Thus, our dataset could less suffer from some biases that key sentences usually locate at the beginning of the text and favorable summary candidates are already inside the text in similar forms. Second, we propose a novel abstractive summarization model named multi-level memory networks ({MMN}), equipped with multi-level memory to store the information of text from different levels of abstraction. With quantitative evaluation and user studies via Amazon Mechanical Turk, we show the Reddit {TIFU} dataset is highly abstractive and the {MMN} outperforms the state-of-the-art summarization models.},
	pages = {1811.00783v2},
	journaltitle = {{arXiv}},
	author = {Kim, Byeongchang and Kim, Hyunwoo and Kim, Gunhee},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Published in {NAACL}-{HLT} 2019 (Oral)}
}

@book{kim_application-specific_2012,
	title = {Application-specific Cloud Provisioning Model Using Job Profiles Analysis},
	publisher = {{IEEE}},
	author = {Kim, Seoyoung and Kim, Yoonhee},
	date = {2012}
}

@article{kingma_adam_2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980v9 http://arxiv.org/pdf/1412.6980v9},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	pages = {1412.6980v9},
	journaltitle = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	date = {2014},
	note = {Type: Journal article},
	annotation = {Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015}
}

@book{klein_accurate_2003,
	title = {Accurate unlexicalized parsing},
	volume = {Proceedings of the 41st annual meeting of the association for computational linguistics},
	abstract = {We demonstrate that an unlexicalized {PCFG} can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its …},
	pagetotal = {423-430},
	author = {Klein, Dan and Manning, Christopher D},
	date = {2003}
}

@article{kliazovich_dens_2013,
	title = {{DENS}: data center energy-efficient network-aware scheduling},
	volume = {16},
	url = {http://dx.doi.org/10.1007/s10586-011-0177-4},
	doi = {10.1007/s10586-011-0177-4},
	pages = {65--75},
	number = {1},
	journaltitle = {Cluster Computing},
	author = {Kliazovich, Dzmitry and Bouvry, Pascal and Khan, Samee Ullah},
	date = {2013},
	note = {Publisher: Springer Science and Business Media {LLC}
	Type: Journal article},
	annotation = {Times cited: 125}
}

@article{kohavi_wrappers_1997,
	title = {Wrappers for feature subset selection},
	volume = {97},
	url = {https://www.sciencedirect.com/science/article/pii/S000437029700043X},
	abstract = {In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a …},
	pages = {273--324},
	number = {1},
	journaltitle = {Artificial intelligence},
	author = {Kohavi, Ron and John, George H},
	date = {1997},
	note = {{ISBN}: 0004-3702
	Publisher: Elsevier
	Type: Journal article},
	annotation = {Times cited: 9862}
}

@article{krantz_abstractive_2018,
	title = {Abstractive Summarization Using Attentive Neural Techniques},
	url = {http://arxiv.org/abs/1810.08838v1},
	abstract = {In a world of proliferating data, the ability to rapidly summarize text is growing in importance. Automatic summarization of text can be thought of as a sequence to sequence problem. Another area of natural language processing that solves a sequence to sequence problem is machine translation, which is rapidly evolving due to the development of attention-based encoder-decoder networks. This work applies these modern techniques to abstractive summarization. We perform analysis on various attention mechanisms for summarization with the goal of developing an approach and architecture aimed at improving the state of the art. In particular, we modify and optimize a translation model with self-attention for generating abstractive sentence summaries. The effectiveness of this base model along with attention variants is compared and analyzed in the context of standardized evaluation sets and test metrics. However, we show that these metrics are limited in their ability to effectively score abstractive summaries, and propose a new approach based on the intuition that an abstractive model requires an abstractive evaluation.},
	pages = {1810.08838v1},
	journaltitle = {{arXiv}},
	author = {Krantz, Jacob and Kalita, Jugal},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Accepted for oral presentation at the 15th International Conference on Natural Language Processing ({ICON} 2018)}
}

@article{kreutzer_joey_2019,
	title = {Joey {NMT}: A minimalist {NMT} toolkit for novices},
	url = {https://arxiv.org/pdf/1907.12484},
	abstract = {We present Joey {NMT}, a minimalist neural machine translation toolkit based on {PyTorch} that is specifically designed for novices. Joey {NMT} provides many popular {NMT} features in a small and simple code base, so that novices can easily and quickly learn to use it and adapt it to their needs. Despite its focus on simplicity, Joey {NMT} supports classic architectures ({RNNs}, transformers), fast beam search, weight tying, and more, and achieves performance comparable to more complex toolkits on standard benchmarks. We evaluate the accessibility …},
	journaltitle = {{arXiv} preprint {arXiv}:1907.12484},
	author = {Kreutzer, Julia and Bastings, Joost and Riezler, Stefan},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Times cited: 28}
}

@article{krippendorff_computing_2011,
	title = {Computing Krippendorff’s alpha-reliability},
	url = {https://repository.upenn.edu/cgi/viewcontent.cgi?article=1043&context=asc_papers},
	abstract = {Krippendorff’s alpha (α) is a reliability coefficient developed to measure the agreement among observers, coders, judges, raters, or measuring instruments drawing distinctions among typically unstructured phenomena or assign computable values to them. α emerged in content analysis but is widely applicable wherever two or more methods of generating data are applied to the same set of objects, units of analysis, or items and the question is how much the resulting data can be trusted to represent something real.},
	author = {Krippendorff, Klaus},
	date = {2011},
	note = {Type: Journal article},
	annotation = {Times cited: 638}
}

@article{kryscinski_evaluating_2019,
	title = {Evaluating the Factual Consistency of Abstractive Text Summarization},
	url = {http://arxiv.org/abs/1910.12840v1 http://arxiv.org/pdf/1910.12840v1},
	abstract = {Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.},
	pages = {1910.12840v1},
	journaltitle = {{arXiv}},
	author = {Kryściński, Wojciech and {McCann}, Bryan and Xiong, Caiming and Socher, Richard},
	date = {2019},
	note = {Type: Journal article},
	annotation = {11 pages, 7 tables, 1 algorithm}
}

@article{kryscinski_improving_2018,
	title = {Improving abstraction in text summarization},
	url = {https://arxiv.org/pdf/1808.07913},
	abstract = {Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document …},
	journaltitle = {{arXiv} preprint {arXiv}:1808.07913},
	author = {Kryściński, Wojciech and Paulus, Romain and Xiong, Caiming and Socher, Richard},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 71}
}

@book{kumar_ask_2016,
	title = {Ask me anything: Dynamic memory networks for natural language processing},
	volume = {International conference on machine learning},
	abstract = {Most tasks in natural language processing can be cast into question answering ({QA}) problems over language input. We introduce the dynamic memory network ({DMN}), a neural network architecture which processes input sequences and questions, forms episodic …},
	pagetotal = {1378-1387},
	author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
	date = {2016}
}

@article{laban_summary_2020,
	title = {The summary loop: Learning to write abstractive summaries without examples},
	url = {https://www.aclweb.org/anthology/2020.acl-main.460},
	abstract = {This work presents a new approach to unsupervised abstractive summarization based on maximizing a combination of coverage and fluency for a given length constraint. It introduces a novel method that encourages the inclusion of key terms from the original document into the summary: key terms are masked out of the original document and must be filled in by a coverage model using the current generated summary. A novel unsupervised training procedure leverages this coverage model along with a fluency model to generate and score …},
	journaltitle = {… of the 58th Annual Meeting of the …},
	author = {Laban, P and Hsi, A and Canny, J and Hearst, {MA}},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Times cited: 1}
}

@book{lavie_meteor_2007,
	title = {{METEOR}: An automatic metric for {MT} evaluation with high levels of correlation with human judgments},
	volume = {Proceedings of the second workshop on statistical machine translation},
	abstract = {Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year’s shared task within the {ACL} {WMT}-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric …},
	pagetotal = {228-231},
	author = {Lavie, Alon and Agarwal, Abhaya},
	date = {2007}
}

@article{lee_rodgers_thirteen_1988,
	title = {Thirteen Ways to Look at the Correlation Coefficient},
	volume = {42},
	url = {http://dx.doi.org/10.1080/00031305.1988.10475524},
	doi = {10.1080/00031305.1988.10475524},
	pages = {59--66},
	number = {1},
	journaltitle = {The American Statistician},
	author = {Lee Rodgers, Joseph and Nicewander, W. Alan},
	date = {1988},
	note = {Publisher: Informa {UK} Limited
	Type: Journal article},
	annotation = {Times cited: 351}
}

@book{lee_stanfords_2011,
	title = {Stanford’s multi-pass sieve coreference resolution system at the conll-2011 shared task},
	volume = {Proceedings of the 15th conference on computational natural language learning: Shared task},
	abstract = {This paper details the coreference resolution system submitted by Stanford at the {CoNLL}-2011 shared task. Our system is a collection of deterministic coreference resolution models that incorporate lexical, syntactic, semantic, and discourse information. All these models use global document-level information by sharing mention attributes, such as gender and number, across mentions in the same cluster. We participated in both the open and closed tracks and submitted results using both predicted and gold mentions. Our system was …},
	pagetotal = {28-34},
	publisher = {Association for Computational Linguistics},
	author = {Lee, Heeyoung and Peirsman, Yves and Chang, Angel and Chambers, Nathanael and Surdeanu, Mihai and Jurafsky, Dan},
	date = {2011}
}

@article{lei_opening_2018,
	title = {Opening the black box of deep learning},
	volume = {cs.{LG}},
	url = {http://arxiv.org/abs/1805.08355v1},
	abstract = {The great success of deep learning shows that its technology contains profound truth, and understanding its internal mechanism not only has important implications for the development of its technology and effective application in various fields, but also provides meaningful insights into the understanding of human brain mechanism. At present, most of the theoretical research on deep learning is based on mathematics. This dissertation proposes that the neural network of deep learning is a physical system, examines deep learning from three different perspectives: microscopic, macroscopic, and physical world views, answers multiple theoretical puzzles in deep learning by using physics principles. For example, from the perspective of quantum mechanics and statistical physics, this dissertation presents the calculation methods for convolution calculation, pooling, normalization, and Restricted Boltzmann Machine, as well as the selection of cost functions, explains why deep learning must be deep, what characteristics are learned in deep learning, why Convolutional Neural Networks do not have to be trained layer by layer, and the limitations of deep learning, etc., and proposes the theoretical direction and basis for the further development of deep learning now and in the future. The brilliance of physics flashes in deep learning, we try to establish the deep learning technology based on the scientific theory of physics.},
	author = {Lei, Dian and Chen, Xiaoxiao and Zhao, Jianfei},
	date = {2018},
	note = {Type: Journal article}
}

@article{lewis_bart_2019,
	title = {{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension},
	url = {http://arxiv.org/abs/1910.13461v1 http://arxiv.org/pdf/1910.13461v1},
	abstract = {We present {BART}, a denoising autoencoder for pretraining sequence-to-sequence models. {BART} is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing {BERT} (due to the bidirectional encoder), {GPT} (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. {BART} is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of {RoBERTa} with comparable training resources on {GLUE} and {SQuAD}, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 {ROUGE}. {BART} also provides a 1.1 {BLEU} increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the {BART} framework, to better measure which factors most influence end-task performance.},
	pages = {1910.13461v1},
	journaltitle = {{arXiv}},
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	date = {2019},
	note = {Type: Journal article}
}

@article{li_diversity-promoting_2015,
	title = {A Diversity-Promoting Objective Function for Neural Conversation Models},
	url = {http://arxiv.org/abs/1510.03055v3},
	abstract = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., “I don’t know”) regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information ({MMI}) as the objective function in neural models. Experimental results demonstrate that the proposed {MMI} models produce more diverse, interesting, and appropriate responses, yielding substantive gains in {BLEU} scores on two conversational datasets and in human evaluations.},
	pages = {1510.03055v3},
	journaltitle = {{arXiv}},
	author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
	date = {2015},
	note = {Type: Journal article},
	annotation = {In. Proc of {NAACL} 2016}
}

@article{li_simple_2016,
	title = {A Simple, Fast Diverse Decoding Algorithm for Neural Generation},
	url = {http://arxiv.org/abs/1611.08562v2},
	abstract = {In this paper, we propose a simple, fast decoding algorithm that fosters diversity in neural generation. The algorithm modifies the standard beam search algorithm by adding an inter-sibling ranking penalty, favoring choosing hypotheses from diverse parents. We evaluate the proposed model on the tasks of dialogue response generation, abstractive summarization and machine translation. We find that diverse decoding helps across all tasks, especially those for which reranking is needed. We further propose a variation that is capable of automatically adjusting its diversity decoding rates for different inputs using reinforcement learning ({RL}). We observe a further performance boost from this {RL} technique. This paper includes material from the unpublished script “Mutual Information and Diverse Decoding Improve Neural Machine Translation” (Li and Jurafsky, 2016).},
	pages = {1611.08562v2},
	journaltitle = {{arXiv}},
	author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
	date = {2016},
	note = {Type: Journal article}
}

@article{li_deep_2017,
	title = {Deep Recurrent Generative Decoder for Abstractive Text Summarization},
	url = {http://arxiv.org/abs/1708.00625v1},
	abstract = {We propose a new framework for abstractive text summarization based on a sequence-to-sequence oriented encoder-decoder model equipped with a deep recurrent generative decoder ({DRGN}). Latent structure information implied in the target summaries is learned based on a recurrent latent random model for improving the summarization quality. Neural variational inference is employed to address the intractable posterior inference for the recurrent latent variables. Abstractive summaries are generated based on both the generative latent variables and the discriminative deterministic states. Extensive experiments on some benchmark datasets in different languages show that {DRGN} achieves improvements over the state-of-the-art methods.},
	pages = {1708.00625v1},
	journaltitle = {{arXiv}},
	author = {Li, Piji and Lam, Wai and Bing, Lidong and Wang, Zihao},
	date = {2017},
	note = {Type: Journal article},
	annotation = {10 pages, {EMNLP} 2017}
}

@article{li_abstractive_2021,
	title = {Abstractive Multi-Document Summarization Based on Semantic Link Network},
	volume = {33},
	url = {http://dx.doi.org/10.1109/tkde.2019.2922957},
	doi = {10.1109/tkde.2019.2922957},
	pages = {43--54},
	number = {1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Li, Wei and Zhuge, Hai},
	date = {2021},
	note = {Publisher: Institute of Electrical and Electronics Engineers ({IEEE})
	Type: Journal article},
	annotation = {Times cited: 2}
}

@article{li_text_2020,
	title = {Text Summarization Method Based on Double Attention Pointer Network},
	volume = {8},
	url = {http://dx.doi.org/10.1109/access.2020.2965575},
	doi = {10.1109/access.2020.2965575},
	pages = {11279--11288},
	journaltitle = {{IEEE} Access},
	author = {Li, Zhixin and Peng, Zhi and Tang, Suqin and Zhang, Canlong and Ma, Huifang},
	date = {2020},
	note = {Publisher: Institute of Electrical and Electronics Engineers ({IEEE})
	Type: Journal article},
	annotation = {Times cited: 3}
}

@article{liao_abstract_2018,
	title = {Abstract Meaning Representation for Multi-Document Summarization},
	url = {http://arxiv.org/abs/1806.05655v1},
	abstract = {Generating an abstract from a collection of documents is a desirable capability for many real-world applications. However, abstractive approaches to multi-document summarization have not been thoroughly investigated. This paper studies the feasibility of using Abstract Meaning Representation ({AMR}), a semantic representation of natural language grounded in linguistic theory, as a form of content representation. Our approach condenses source documents to a set of summary graphs following the {AMR} formalism. The summary graphs are then transformed to a set of summary sentences in a surface realization step. The framework is fully data-driven and flexible. Each component can be optimized independently using small-scale, in-domain training data. We perform experiments on benchmark summarization datasets and report promising results. We also describe opportunities and challenges for advancing this line of research.},
	pages = {1806.05655v1},
	journaltitle = {{arXiv}},
	author = {Liao, Kexin and Lebanoff, Logan and Liu, Fei},
	date = {2018},
	note = {Type: Journal article},
	annotation = {13 pages}
}

@article{lim_i_2020,
	title = {I Know What You Asked: Graph Path Learning using {AMR} for Commonsense Reasoning},
	url = {http://arxiv.org/abs/2011.00766v2 http://arxiv.org/pdf/2011.00766v2},
	abstract = {{CommonsenseQA} is a task in which a correct answer is predicted through commonsense reasoning with pre-defined knowledge. Most previous works have aimed to improve the performance with distributed representation without considering the process of predicting the answer from the semantic representation of the question. To shed light upon the semantic interpretation of the question, we propose an {AMR}-{ConceptNet}-Pruned ({ACP}) graph. The {ACP} graph is pruned from a full integrated graph encompassing Abstract Meaning Representation ({AMR}) graph generated from input questions and an external commonsense knowledge graph, {ConceptNet} ({CN}). Then the {ACP} graph is exploited to interpret the reasoning path as well as to predict the correct answer on the {CommonsenseQA} task. This paper presents the manner in which the commonsense reasoning process can be interpreted with the relations and concepts provided by the {ACP} graph. Moreover, {ACP}-based models are shown to outperform the baselines.},
	pages = {2011.00766v2},
	journaltitle = {{arXiv}},
	author = {Lim, Jungwoo and Oh, Dongsuk and Jang, Yoonna and Yang, Kisu and Lim, Heuiseok},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted to {COLING} 2020}
}

@book{lin_information_2006,
	title = {An information theoretic approach to automatic evaluation of summaries},
	volume = {Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference},
	abstract = {Until recently there are no common, convenient, and repeatable evaluation methods that could be easily applied to support fast turn-around development of automatic text summarization systems. In this paper, we introduce an informationtheoretic approach to automatic evaluation of summaries based on the Jensen-Shannon divergence of distributions between an automatic summary and a set of reference summaries. Several variants of the approach are also considered and compared. The results indicate that {JS} …},
	pagetotal = {463-470},
	author = {Lin, Chin Yew and Cao, Guihong and Gao, Jianfeng and Nie, Jian Yun},
	date = {2006}
}

@book{lin_rouge_2004,
	title = {Rouge: A package for automatic evaluation of summaries},
	volume = {Text summarization branches out},
	abstract = {{ROUGE} stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different {ROUGE} measures: {ROUGE}-N, {ROUGE}-L, {ROUGE}-W, and {ROUGE}-S included in …},
	pagetotal = {74-81},
	author = {Lin, Chin-Yew},
	date = {2004}
}

@book{lin_automatic_2003,
	location = {Morristown, {NJ}, {USA}},
	title = {Automatic evaluation of summaries using N-gram co-occurrence statistics},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew and Hovy, Eduard},
	date = {2003}
}

@book{lin_reinforcement_2016,
	title = {A Reinforcement Learning-Based Power Management Framework for Green Computing Data Centers},
	publisher = {{IEEE}},
	author = {Lin, Xue and Wang, Yanzhi and Pedram, Massoud},
	date = {2016}
}

@book{lin_combining_2012,
	title = {Combining coherence models and machine translation evaluation metrics for summarization evaluation},
	volume = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	abstract = {An ideal summarization system should produce summaries that have high content coverage and linguistic quality. Many state-ofthe-art summarization systems focus on content coverage by extracting content-dense sentences from source articles. A current research focus is to process these sentences so that they read fluently as a whole. The current {AESOP} task encourages research on evaluating summaries on content, readability, and overall responsiveness. In this work, we adapt a machine translation metric to measure …},
	pagetotal = {1006-1014},
	author = {Lin, Ziheng and Liu, Chang and Ng, Hwee Tou and Kan, Min-Yen},
	date = {2012}
}

@article{lipton_critical_2015,
	title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	volume = {cs.{LG}},
	url = {http://arxiv.org/abs/1506.00019v4},
	abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks ({RNNs}) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory ({LSTM}) and bidirectional ({BRNN}) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	date = {2015},
	note = {Type: Journal article}
}

@article{liu_how_2016,
	title = {How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation},
	url = {https://arxiv.org/pdf/1603.08023.pdf?__hstc=36392319.57feae9086cbe66baa94bf32ef453412.1482451200081.1482451200082.1482451200083.1&__hssc=36392319.1.1482451200084&__hsfp=528229161},
	abstract = {We investigate evaluation metrics for dialogue response generation systems where supervised labels, such as task completion, are not available. Recent works in response generation have adopted metrics from machine translation to compare a model’s generated …},
	journaltitle = {{arXiv} preprint {arXiv}:1603.08023},
	author = {Liu, Chia-Wei and Lowe, Ryan and Serban, Iulian V and Noseworthy, Michael and Charlin, Laurent and Pineau, Joelle},
	date = {2016},
	note = {Type: Journal article},
	annotation = {Times cited: 787}
}

@article{liu_generating_2018,
	title = {Generating Wikipedia by Summarizing Long Sequences},
	url = {http://arxiv.org/abs/1801.10198v1},
	abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, {ROUGE} scores and human evaluations.},
	pages = {1801.10198v1},
	journaltitle = {{arXiv}},
	author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Published as a conference paper at {ICLR} 2018}
}

@article{liu_text_2019,
	title = {Text Summarization with Pretrained Encoders},
	url = {http://arxiv.org/abs/1908.08345v2},
	abstract = {Bidirectional Encoder Representations from Transformers ({BERT}) represents the latest incarnation of pretrained language models which have recently advanced a wide range of natural language processing tasks. In this paper, we showcase how {BERT} can be usefully applied in text summarization and propose a general framework for both extractive and abstractive models. We introduce a novel document-level encoder based on {BERT} which is able to express the semantics of a document and obtain representations for its sentences. Our extractive model is built on top of this encoder by stacking several inter-sentence Transformer layers. For abstractive summarization, we propose a new fine-tuning schedule which adopts different optimizers for the encoder and the decoder as a means of alleviating the mismatch between the two (the former is pretrained while the latter is not). We also demonstrate that a two-staged fine-tuning approach can further boost the quality of the generated summaries. Experiments on three datasets show that our model achieves state-of-the-art results across the board in both extractive and abstractive settings. Our code is available at https://github.com/nlpyang/{PreSumm}},
	pages = {1908.08345v2},
	journaltitle = {{arXiv}},
	author = {Liu, Yang and Lapata, Mirella},
	date = {2019},
	note = {Type: Journal article},
	annotation = {fix typos}
}

@article{liu_amr_2018,
	title = {An {AMR} Aligner Tuned by Transition-based Parser},
	url = {http://arxiv.org/abs/1810.03541v1 http://arxiv.org/pdf/1810.03541v1},
	abstract = {In this paper, we propose a new rich resource enhanced {AMR} aligner which produces multiple alignments and a new transition system for {AMR} parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable {AMR} graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced {AMR} parsers. Based on our aligner and transition system, we develop a transition-based {AMR} parser that parses a sentence into its {AMR} graph directly. An ensemble of our parsers with only words and {POS} tags as input leads to 68.4 Smatch F1 score.},
	pages = {1810.03541v1},
	journaltitle = {{arXiv}},
	author = {Liu, Yijia and Che, Wanxiang and Zheng, Bo and Qin, Bing and Liu, Ting},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{EMNLP}2018}
}

@book{liu_controlling_2018,
	title = {Controlling length in abstractive summarization using a convolutional neural network},
	volume = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	abstract = {Convolutional neural networks ({CNNs}) have met great success in abstractive summarization, but they cannot effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in abstractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this …},
	pagetotal = {4110-4119},
	author = {Liu, Yizhu and Luo, Zhiyi and Zhu, Kenny},
	date = {2018}
}

@article{liu_topic-aware_2019,
	title = {Topic-aware Pointer-Generator Networks for Summarizing Spoken Conversations},
	url = {http://arxiv.org/abs/1910.01335v1},
	abstract = {Due to the lack of publicly available resources, conversation summarization has received far less attention than text summarization. As the purpose of conversations is to exchange information between at least two interlocutors, key information about a certain topic is often scattered and spanned across multiple utterances and turns from different speakers. This phenomenon is more pronounced during spoken conversations, where speech characteristics such as backchanneling and false-starts might interrupt the topical flow. Moreover, topic diffusion and (intra-utterance) topic drift are also more common in human-to-human conversations. Such linguistic characteristics of dialogue topics make sentence-level extractive summarization approaches used in spoken documents ill-suited for summarizing conversations. Pointer-generator networks have effectively demonstrated its strength at integrating extractive and abstractive capabilities through neural modeling in text summarization. To the best of our knowledge, to date no one has adopted it for summarizing conversations. In this work, we propose a topic-aware architecture to exploit the inherent hierarchical structure in conversations to further adapt the pointer-generator model. Our approach significantly outperforms competitive baselines, achieves more efficient learning outcomes, and attains more robust performance.},
	pages = {1910.01335v1},
	journaltitle = {{arXiv}},
	author = {Liu, Zhengyuan and Ng, Angela and Lee, Sheldon and Aw, Ai Ti and Chen, Nancy F.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {To appear in {ASRU}2019}
}

@article{louis_automatically_2013,
	title = {Automatically Assessing Machine Summary Content Without a Gold Standard},
	volume = {39},
	url = {http://dx.doi.org/10.1162/coli_a_00123},
	doi = {10.1162/coli_a_00123},
	abstract = {{\textless}jats:p{\textgreater} The most widely adopted approaches for evaluation of summary content follow some protocol for comparing a summary with gold-standard human summaries, which are traditionally called model summaries. This evaluation paradigm falls short when human summaries are not available and becomes less accurate when only a single model is available. We propose three novel evaluation techniques. Two of them are model-free and do not rely on a gold standard for the assessment. The third technique improves standard automatic evaluations by expanding the set of available model summaries with chosen system summaries. {\textless}/jats:p{\textgreater}{\textless}jats:p{\textgreater} We show that quantifying the similarity between the source text and its summary with appropriately chosen measures produces summary scores which replicate human assessments accurately. We also explore ways of increasing evaluation quality when only one human model summary is available as a gold standard. We introduce pseudomodels, which are system summaries deemed to contain good content according to automatic evaluation. Combining the pseudomodels with the single human model to form the gold-standard leads to higher correlations with human judgments compared to using only the one available model. Finally, we explore the feasibility of another measure—similarity between a system summary and the pool of all other system summaries for the same input. This method of comparison with the consensus of systems produces impressively accurate rankings of system summaries, achieving correlation with human rankings above 0.9. {\textless}/jats:p},
	pages = {267--300},
	number = {2},
	journaltitle = {Computational Linguistics},
	author = {Louis, Annie and Nenkova, Ani},
	date = {2013},
	note = {Publisher: {MIT} Press - Journals
	Type: Journal article},
	annotation = {Times cited: 29}
}

@book{louppe_ensembles_2012,
	title = {Ensembles on random patches},
	volume = {Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
	abstract = {In this paper, we consider supervised learning under the assumption that the available memory is small compared to the dataset size. This general framework is relevant in the context of big data, distributed databases and embedded systems. We investigate a very …},
	pagetotal = {346-361},
	publisher = {Springer},
	author = {Louppe, Gilles and Geurts, Pierre},
	date = {2012}
}

@article{luong_effective_2015,
	title = {Effective Approaches to Attention-based Neural Machine Translation},
	url = {http://arxiv.org/abs/1508.04025v5},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation ({NMT}) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based {NMT}. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the {WMT} translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 {BLEU} points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the {WMT}’15 English to German translation task with 25.9 {BLEU} points, an improvement of 1.0 {BLEU} points over the existing best system backed by {NMT} and an n-gram reranker.},
	pages = {1508.04025v5},
	journaltitle = {{arXiv}},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	date = {2015},
	note = {Type: Journal article},
	annotation = {11 pages, 7 figures, {EMNLP} 2015 camera-ready version, more training details}
}

@book{ma_blend_2017,
	title = {Blend: a novel combined {MT} metric based on direct assessment—casict-dcu submission to {WMT}17 metrics task},
	volume = {Proceedings of the second conference on machine translation},
	abstract = {{\textless}span dir=ltr{\textgreater}Existing metrics to evaluate the quality of Machine Translation hypotheses take different perspectives into account. {DPM}-Fcomb, a metric combining the merits of a range of metrics, achieved the best performance for evaluation of to-English language pairs in the previous two years of {WMT} Metrics Shared Tasks. This year, we submit a novel combined metric, Blend, to {WMT}17 Metrics task. Compared to {DPMFcomb}, Blend includes the following adaptations: i) We use {DA} human evaluation to guide the training process with a vast …{\textless}/span{\textgreater}‏},
	pagetotal = {598-603},
	author = {Ma, Qingsong and Graham, Yvette and Wang, Shugen and Liu, Qun},
	date = {2017}
}

@book{mairesse_phrase-based_2010,
	title = {Phrase-based statistical language generation using graphical models and active learning},
	volume = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
	abstract = {Most previous work on trainable language generation has focused on two paradigms:(a) using a statistical model to rank a set of generated utterances, or (b) using statistics to inform the generation decision process. Both approaches rely on the existence of a handcrafted generator, which limits their scalability to new domains. This paper presents {BAGEL}, a statistical language generator which uses dynamic Bayesian networks to learn from semantically-aligned data produced by 42 untrained annotators. A human evaluation shows …},
	pagetotal = {1552-1561},
	author = {Mairesse, François and Gasic, Milica and Jurcicek, Filip and Keizer, Simon and Thomson, Blaise and Yu, Kai and Young, Steve},
	date = {2010}
}

@book{malvino_digital_1977,
	location = {New York},
	title = {Digital computer electronics},
	publisher = {Gregg Division, {McGraw}-Hill},
	author = {Malvino, Albert Paul.},
	date = {1977}
}

@article{mamun_intra-_2020,
	title = {Intra- and Inter-Server Smart Task Scheduling for Profit and Energy Optimization of {HPC} Data Centers},
	volume = {10},
	url = {http://dx.doi.org/10.3390/jlpea10040032},
	doi = {10.3390/jlpea10040032},
	abstract = {{\textless}jats:p{\textgreater}Servers in a data center are underutilized due to over-provisioning, which contributes heavily toward the high-power consumption of the data centers. Recent research in optimizing the energy consumption of High Performance Computing ({HPC}) data centers mostly focuses on consolidation of Virtual Machines ({VMs}) and using dynamic voltage and frequency scaling ({DVFS}). These approaches are inherently hardware-based, are frequently unique to individual systems, and often use simulation due to lack of access to {HPC} data centers. Other approaches require profiling information on the jobs in the {HPC} system to be available before run-time. In this paper, we propose a reinforcement learning based approach, which jointly optimizes profit and energy in the allocation of jobs to available resources, without the need for such prior information. The approach is implemented in a software scheduler used to allocate real applications from the Princeton Application Repository for Shared-Memory Computers ({PARSEC}) benchmark suite to a number of hardware nodes realized with Odroid-{XU}3 boards. Experiments show that the proposed approach increases the profit earned by 40\% while simultaneously reducing energy consumption by 20\% when compared to a heuristic-based approach. We also present a network-aware server consolidation algorithm called Bandwidth-Constrained Consolidation ({BCC}), for {HPC} data centers which can address the under-utilization problem of the servers. Our experiments show that the {BCC} consolidation technique can reduce the power consumption of a data center by up-to 37\%.{\textless}/jats:p},
	pages = {32},
	number = {4},
	journaltitle = {Journal of Low Power Electronics and Applications},
	author = {Mamun, Sayed Ashraf and Gilday, Alexander and Singh, Amit Kumar and Ganguly, Amlan and Merrett, Geoff V. and Wang, Xiaohang and Al-Hashimi, Bashir M.},
	date = {2020},
	note = {Publisher: {MDPI} {AG}
	Type: Journal article}
}

@article{mani_summac_2002,
	title = {{SUMMAC}: a text summarization evaluation},
	volume = {8},
	url = {http://dx.doi.org/10.1017/s1351324901002741},
	doi = {10.1017/s1351324901002741},
	abstract = {{\textless}jats:p{\textgreater}The {TIPSTER} Text Summarization Evaluation ({SUMMAC}) has developed several new extrinsic and intrinsic methods for evaluating summaries. It has established definitively that automatic text summarization is very effective in relevance assessment tasks on news articles. Summaries as short as 17\% of full text length sped up decision-making by almost a factor of 2 with no statistically significant degradation in accuracy. Analysis of feedback forms filled in after each decision indicated that the intelligibility of present-day machine-generated summaries is high. Systems that performed most accurately in the production of indicative and informative topic-related summaries used term frequency and co-occurrence statistics, and vocabulary overlap comparisons between text passages. However, in the absence of a topic, these statistical methods do not appear to provide any additional leverage: in the case of generic summaries, the systems were indistinguishable in accuracy. The paper discusses some of the tradeoffs and challenges faced by the evaluation, and also lists some of the lessons learned, impacts, and possible future directions. The evaluation methods used in the},
	pages = {43--68},
	number = {1},
	journaltitle = {Natural Language Engineering},
	author = {{MANI}, {INDERJEET} and {KLEIN}, {GARY} and {HOUSE}, {DAVID} and {HIRSCHMAN}, {LYNETTE} and {FIRMIN}, {THERESE} and {SUNDHEIM}, {BETH}},
	date = {2002},
	note = {Publisher: Cambridge University Press ({CUP})
	Type: Journal article},
	annotation = {Times cited: 48}
}

@book{manning_stanford_2014,
	title = {The Stanford {CoreNLP} natural language processing toolkit},
	volume = {Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations},
	abstract = {We describe the design and use of the Stanford {CoreNLP} toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research {NLP} community and also among commercial and government users of open source {NLP} technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.},
	pagetotal = {55-60},
	author = {Manning, Christopher D and Surdeanu, Mihai and Bauer, John and Finkel, Jenny Rose and Bethard, Steven and {McClosky}, David},
	date = {2014}
}

@article{manning_human_2020,
	title = {A Human Evaluation of {AMR}-to-English Generation Systems},
	url = {http://arxiv.org/abs/2004.06814v2 http://arxiv.org/pdf/2004.06814v2},
	abstract = {Most current state-of-the art systems for generating English text from Abstract Meaning Representation ({AMR}) have been evaluated only using automated metrics, such as {BLEU}, which are known to be problematic for natural language generation. In this work, we present the results of a new human evaluation which collects fluency and adequacy scores, as well as categorization of error types, for several recent {AMR} generation systems. We discuss the relative quality of these systems and how our results compare to those of automatic metrics, finding that while the metrics are mostly successful in ranking systems overall, collecting human judgments allows for more nuanced comparisons. We also analyze common errors made by these systems.},
	pages = {2004.06814v2},
	journaltitle = {{arXiv}},
	author = {Manning, Emma and Wein, Shira and Schneider, Nathan},
	date = {2020},
	note = {Type: Journal article},
	annotation = {{COLING} 2020}
}

@book{mao_resource_2016,
	location = {New York, {NY}, {USA} Atlanta, {GA}, {USA}},
	title = {Resource Management with Deep Reinforcement Learning Proceedings of the 15th {ACM} Workshop on Hot Topics in Networks},
	abstract = {Resource management problems in systems and networking often manifest as difficult online decision making tasks where appropriate solutions depend on understanding the workload and environment. Inspired by recent advances in deep reinforcement learning for {AI} problems, we consider building systems that learn to manage resources directly from experience. We present {DeepRM}, an example solution that translates the problem of packing tasks with multiple resource demands into a learning problem. Our initial results show that {DeepRM} performs comparably to state-of-the-art heuristics, adapts to different conditions, converges quickly, and learns strategies that are sensible in hindsight.},
	pagetotal = {50-56},
	publisher = {Association for Computing Machinery},
	author = {Mao, Hongzi and Alizadeh, Mohammad and Menache, Ishai and Kandula, Srikanth},
	date = {2016}
}

@article{zheng_tensorflow_2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {http://tensorflow.org/},
	author = {Zheng, Martin Abadi and Agarwal, Ashish and Barham, P. and Brevdo, E. and Chen, Z. and Citro, C. and Corrado, G. and Davis, Andy and Dean, J. and Devin, M. and Ghemawat, Sanjay and Goodfellow, I. and Harp, Andrew and Irving, Geoffrey and Isard, M. and Jia, Y. and Jozefowicz, R. and Kaiser, Lukasz and Kudlur, M. and Levenberg, Josh and Mane, Dandelion and Monga, Rajat and Moore, Sherry and Murray, D. and Olah, Christopher and Schuster, M. and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, P. and Vanhoucke, V. and Vasudevan, Vijay and Viegas, Fernanda B. and Vinyals, Oriol and Warden, Pete and Wattenberg, M. and Wicke, Martin and Yu, Y. and {X.}},
	date = {2015},
	note = {Type: Journal article},
	annotation = {Software available from tensorflow.org}
}

@book{matt_word_2015,
	title = {From Word Embeddings To Document Distances {ICML}},
	author = {Matt, J. Kusner and Yu, Sun and Nicholas, I. Kolkin and Kilian, Q. Weinberger},
	date = {2015}
}

@article{maynez_faithfulness_2020,
	title = {On Faithfulness and Factuality in Abstractive Summarization},
	url = {http://arxiv.org/abs/2005.00661v1},
	abstract = {It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., {ROUGE}, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.},
	pages = {2005.00661v1},
	journaltitle = {{arXiv}},
	author = {Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and {McDonald}, Ryan},
	date = {2020},
	note = {Type: Journal article},
	annotation = {{ACL} 2020, 14 pages}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	url = {https://link.springer.com/article/10.1007%2FBF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical …},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	author = {{McCulloch}, Warren S and Pitts, Walter},
	date = {1943},
	note = {{ISBN}: 1522-9602
	Publisher: Springer
	Type: Journal article},
	annotation = {Times cited: 20895}
}

@book{mieskes_data_2020,
	title = {A Data Set for the Analysis of Text Quality Dimensions in Summarization Evaluation},
	volume = {Proceedings of The 12th Language Resources and Evaluation Conference},
	pagetotal = {6690-6699},
	author = {Mieskes, Margot and Mencía, Eneldo Loza and Kronsbein, Tim},
	date = {2020}
}

@article{mihalcea_corpus-based_2006,
	title = {Corpus-based and knowledge-based measures of text semantic similarity},
	abstract = {This paper presents a method for measuring the semantic similarity of texts, using corpus-based and knowledge-based measures of similarity. Previous work on this problem has focused mainly on either large documents (eg text classification, information retrieval) or individual words (eg synonymy tests). Given that a large fraction of the information available today, on the Web and elsewhere, consists of short text snippets (eg abstracts of scientific documents, imagine captions, product descriptions), in this paper we focus on measuring …},
	journaltitle = {Aaai},
	author = {Mihalcea, R and Corley, C and Strapparava, C},
	date = {2006},
	note = {Type: Journal article},
	annotation = {Times cited: 1423}
}

@article{mikolov_statistical_2012,
	title = {Statistical language models based on neural networks},
	volume = {80},
	abstract = {Hidden layer s is orders of magnitude smaller (50-1000 neurons) U is the matrix of weights between input and hidden layer, V is the matrix of weights between hidden and output layer Without the recurrent weights W, this model would be a bigram neural network language model},
	pages = {26},
	journaltitle = {Presentation at Google, Mountain View, 2nd April},
	author = {Mikolov, Tomáš},
	date = {2012},
	note = {Type: Journal article},
	annotation = {Times cited: 694}
}

@article{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {http://arxiv.org/abs/1310.4546v1},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of “Canada” and “Air” cannot be easily combined to obtain “Air Canada”. Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	pages = {1310.4546v1},
	journaltitle = {{arXiv}},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	date = {2013},
	note = {Type: Journal article}
}

@article{miller_parlai_2017,
	title = {{ParlAI}: A Dialog Research Software Platform},
	url = {http://arxiv.org/abs/1705.06476v4},
	abstract = {We introduce {ParlAI} (pronounced “par-lay”), an open-source software platform for dialog research implemented in Python, available at http://parl.ai. Its goal is to provide a unified framework for sharing, training and testing of dialog models, integration of Amazon Mechanical Turk for data collection, human evaluation, and online/reinforcement learning; and a repository of machine learning models for comparing with others’ models, and improving upon existing architectures. Over 20 tasks are supported in the first release, including popular datasets such as {SQuAD}, {bAbI} tasks, {MCTest}, {WikiQA}, {QACNN}, {QADailyMail}, {CBT}, {bAbI} Dialog, Ubuntu, {OpenSubtitles} and {VQA}. Several models are integrated, including neural models such as memory networks, seq2seq and attentive {LSTMs}.},
	pages = {1705.06476v4},
	journaltitle = {{arXiv}},
	author = {Miller, Alexander H. and Feng, Will and Fisch, Adam and Lu, Jiasen and Batra, Dhruv and Bordes, Antoine and Parikh, Devi and Weston, Jason},
	date = {2017},
	note = {Type: Journal article}
}

@article{ming_understanding_2017,
	title = {Understanding Hidden Memories of Recurrent Neural Networks},
	volume = {cs.{CL}},
	url = {http://arxiv.org/abs/1710.10777v1},
	abstract = {Recurrent neural networks ({RNNs}) have been successfully applied to various natural language processing ({NLP}) tasks and achieved better results than conventional methods. However, the lack of understanding of the mechanisms behind their effectiveness limits further improvements on their architectures. In this paper, we present a visual analytics method for understanding and comparing {RNN} models for {NLP} tasks. We propose a technique to explain the function of individual hidden state units based on their expected response to input texts. We then co-cluster hidden state units and words based on the expected response and visualize co-clustering results as memory chips and word clouds to provide more structured knowledge on {RNNs}’ hidden states. We also propose a glyph-based sequence visualization based on aggregate information to analyze the behavior of an {RNN}’s hidden state at the sentence-level. The usability and effectiveness of our method are demonstrated through case studies and reviews from domain experts.},
	author = {Ming, Yao and Cao, Shaozu and Zhang, Ruixiang and Li, Zhen and Chen, Yuanzhe and Song, Yangqiu and Qu, Huamin},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Published at {IEEE} Conference on Visual Analytics Science and Technology ({IEEE} {VAST} 2017)}
}

@article{mnih_playing_2013,
	title = {Playing atari with deep reinforcement learning},
	url = {https://arxiv.org/pdf/1312.5602.pdf?source=post_page},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose …},
	journaltitle = {{arXiv} preprint {arXiv}:1312.5602},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	date = {2013},
	note = {Type: Journal article},
	annotation = {Times cited: 5934}
}

@article{mohebbi_texts_2016,
	title = {Texts semantic similarity detection based graph approach.},
	abstract = {Similarity of text documents is important to analyze and extract useful information from text documents and generation of the appropriate data. Several cases of lexical matching techniques offered to determine the similarity between documents that have been successful to a certain limit and these methods are failing to find the semantic similarity between two texts. Therefore, the semantic similarity approaches were suggested, such as corpus-based methods and knowledge based methods eg, {WordNet} based methods. This paper, offers a …},
	journaltitle = {Int. Arab J. Inf. Technol.},
	author = {Mohebbi, M and Talebpour, A},
	date = {2016},
	note = {Type: Journal article},
	annotation = {Times cited: 4}
}

@article{moratanch_survey_2016,
	title = {A survey on abstractive text summarization},
	url = {https://ieeexplore.ieee.org/abstract/document/7530193/},
	abstract = {Text Summarization is the task of extracting salient information from the original text document. In this process, the extracted information is generated as a condensed report and presented as a concise summary to the user. It is very difficult for humans to understand and interpret the content of the text. In this paper, an exhaustive survey on abstractive text summarization methods has been presented. The two broad abstractive summarization methods are structured based approach and semantic based approach. This paper …},
	pages = {1--7},
	journaltitle = {ieeexplore.ieee.org},
	author = {Moratanch, N and Chitrakala, S},
	date = {2016},
	note = {{ISBN}: 150901277X
	Publisher: {IEEE}
	Type: Journal article}
}

@incollection{mozer_induction_1992,
	title = {Induction of Multiscale Temporal Structure Advances in Neural Information Processing Systems 4},
	pages = {275--282},
	publisher = {Morgan-Kaufmann},
	author = {Mozer, Michael C},
	editor = {J., E. Moody and S., J. Hanson and R., P. Lippmann},
	date = {1992}
}

@article{nallapati_summarunner_2016,
	title = {{SummaRuNNer}: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents},
	url = {http://arxiv.org/abs/1611.04230v1 http://arxiv.org/pdf/1611.04230v1},
	abstract = {We present {SummaRuNNer}, a Recurrent Neural Network ({RNN}) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
	pages = {1611.04230v1},
	journaltitle = {{arXiv}},
	author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
	date = {2016},
	note = {Type: Journal article},
	annotation = {Published at {AAAI} 2017, The Thirty-First {AAAI} Conference on Artificial Intelligence ({AAAI}-2017)}
}

@article{nallapati_abstractive_2016,
	title = {Abstractive Text Summarization Using Sequence-to-Sequence {RNNs} and Beyond},
	url = {http://arxiv.org/abs/1602.06023v5},
	abstract = {In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
	pages = {1602.06023v5},
	journaltitle = {{arXivThe} {SIGNLL} Conference on Computational Natural Language Learning ({CoNLL}), 2016},
	author = {Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
	date = {2016},
	note = {Type: Journal article}
}

@article{narayan_ranking_2018,
	title = {Ranking sentences for extractive summarization with reinforcement learning},
	url = {https://arxiv.org/pdf/1802.08636},
	abstract = {Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the {ROUGE} evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the {CNN} and {DailyMail} datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and …},
	journaltitle = {{arXiv} preprint {arXiv}:1802.08636},
	author = {Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 268}
}

@article{narayan_dont_2018,
	title = {Don’t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
	url = {http://arxiv.org/abs/1808.08745v1},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation ({BBC}). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	pages = {1808.08745v1},
	journaltitle = {{arXiv}},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	date = {2018},
	note = {Type: Journal article},
	annotation = {11, 2018 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2018}
}

@article{nath_training_2019,
	title = {Training Recurrent Neural Networks Online by Learning Explicit State Variables},
	url = {https://openreview.net/pdf?id=SJgmR0NKPr},
	abstract = {Recurrent neural networks ({RNNs}) allow an agent to construct a state-representation from a stream of experience, which is essential in partially observable problems. However, there are two primary issues one must overcome when training an {RNN}: the sensitivity of the learning algorithm’s performance to truncation length and and long training times. There are variety of strategies to improve training in {RNNs}, the mostly notably Backprop Through Time ({BPTT}) and by Real-Time Recurrent Learning. These strategies, however, are typically …},
	journaltitle = {… Conference on Learning …},
	author = {Nath, S and Liu, V and Chan, A and Li, X and White…, A},
	date = {2019},
	note = {Type: Journal article}
}

@incollection{nenkova_survey_2012,
	location = {Boston, {MA}},
	title = {A Survey of Text Summarization Techniques},
	pages = {43--76},
	publisher = {Springer {US}},
	author = {Nenkova, Ani and {McKeown}, Kathleen},
	date = {2012}
}

@book{nenkova_evaluating_2004,
	title = {Evaluating content selection in summarization: The pyramid method},
	volume = {Proceedings of the human language technology conference of the north american chapter of the association for computational linguistics: Hlt-naacl 2004},
	abstract = {{\textless}span dir=ltr{\textgreater}We present an empirically grounded method for evaluating content selection in summarization. It incorporates the idea that no single best model summary for a collection of documents exists. Our method quantifies the relative importance of facts to be conveyed. We argue that it is reliable, predictive and diagnostic, thus improves considerably over the shortcomings of the human evaluation method currently used in the Document Understanding Conference.{\textless}/span{\textgreater}‏},
	pagetotal = {145-152},
	author = {Nenkova, Ani and Passonneau, Rebecca J},
	date = {2004}
}

@article{ng_better_2015,
	title = {Better summarization evaluation with word embeddings for rouge},
	journaltitle = {{arXiv} preprint {arXiv}:1508.06034},
	author = {Ng, Jun-Ping and Abrecht, Viktoria},
	date = {2015},
	note = {Type: Journal article}
}

@article{nova_strass_2019,
	title = {{STRASS}: A Light and Effective Method for Extractive Summarization Based on Sentence Embeddings},
	volume = {1},
	url = {https://www.aclweb.org/anthology/P19-2.pdf#page=263},
	abstract = {This paper introduces {STRASS}: Summarization by {TRAnsformation} Selection and Scoring. It is an extractive text summarization method which leverages the semantic information in existing sentence embedding spaces. Our method creates an extractive summary by selecting the sentences with the closest embeddings to the document embedding. The model learns a transformation of the document embedding to minimize the similarity between the extractive summary and the ground truth summary. As the transformation is only …},
	pages = {243},
	number = {2},
	journaltitle = {{ACL} 2019},
	author = {{NOVA}, {EURA}},
	date = {2019},
	note = {Type: Journal article}
}

@article{novikova_why_2017,
	title = {Why We Need New Evaluation Metrics for {NLG}},
	url = {http://arxiv.org/abs/1707.06875v1 http://arxiv.org/pdf/1707.06875v1},
	abstract = {The majority of {NLG} evaluation relies on automatic metrics, such as {BLEU}. In this paper, we motivate the need for novel, system- and data-independent automatic evaluation methods: We investigate a wide range of metrics, including state-of-the-art word-based and novel grammar-based ones, and demonstrate that they only weakly reflect human judgements of system outputs as generated by data-driven, end-to-end {NLG}. We also show that metric performance is data- and system-specific. Nevertheless, our results also suggest that automatic metrics perform reliably at system-level and can support system development by finding cases where a system performs poorly.},
	pages = {1707.06875v1},
	journaltitle = {{arXivProceedings} of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2231-2242, Copenhagen, Denmark, September 7-11, 2017},
	author = {Novikova, Jekaterina and Dušek, Ondřej and Curry, Amanda Cercas and Rieser, Verena},
	date = {2017},
	note = {Type: Journal article},
	annotation = {accepted to {EMNLP} 2017}
}

@article{opitz_popular_1999,
	title = {Popular Ensemble Methods: An Empirical Study},
	volume = {11},
	url = {http://dx.doi.org/10.1613/jair.614},
	doi = {10.1613/jair.614},
	abstract = {{\textless}jats:p{\textgreater}An ensemble consists of a set of individually trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman, 1996c) and Boosting (Freund \&amp; Shapire, 1996; Shapire, 1990) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm. Our results clearly indicate a number of conclusions. First, while Bagging is almost always more accurate than a single classifier, it is sometimes much less accurate than Boosting. On the other hand, Boosting can create ensembles that are less accurate than a single classifier – especially when using neural networks. Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined. In fact, further results show that Boosting ensembles may overfit noisy data sets, thus decreasing its performance. Finally, consistent with previous studies, our work suggests that most of the gain in an ensemble’s performance comes in the first few classifiers combined; however, relatively large gains can be seen up to 25 classifiers when Boosting decision trees.{\textless}/jats:p},
	pages = {169--198},
	journaltitle = {Journal of Artificial Intelligence Research},
	author = {Opitz, D. and Maclin, R.},
	date = {1999},
	note = {Publisher: {AI} Access Foundation
	Type: Journal article},
	annotation = {Times cited: 1202}
}

@article{opitz_amr_2020,
	title = {{AMR} Quality Rating with a Lightweight {CNN}},
	url = {http://arxiv.org/abs/2005.12187v2 http://arxiv.org/pdf/2005.12187v2},
	abstract = {Structured semantic sentence representations such as Abstract Meaning Representations ({AMRs}) are potentially useful in various {NLP} tasks. However, the quality of automatic parses can vary greatly and jeopardizes their usefulness. This can be mitigated by models that can accurately rate {AMR} quality in the absence of costly gold data, allowing us to inform downstream systems about an incorporated parse’s trustworthiness or select among different candidate parses. In this work, we propose to transfer the {AMR} graph to the domain of images. This allows us to create a simple convolutional neural network ({CNN}) that imitates a human judge tasked with rating graph quality. Our experiments show that the method can rate quality more accurately than strong baselines, in several quality dimensions. Moreover, the method proves to be efficient and reduces the incurred energy consumption.},
	pages = {2005.12187v2},
	journaltitle = {{arXiv}},
	author = {Opitz, Juri},
	date = {2020},
	note = {Type: Journal article},
	annotation = {{AACL}-{IJCNLP} 2020}
}

@article{opitz_automatic_2019,
	title = {Automatic summarisation: 25 years On},
	volume = {25},
	url = {https://wlv.openrepository.com/bitstream/handle/2436/622749/Summarisation25Years.pdf?sequence=3},
	abstract = {Automatic text summarisation is a topic that has been receiving attention from the research community from the early days of computational linguistics, but it really took off around 25 years ago. This article presents the main developments from the last 25 years. It starts by defining what a summary is and how its definition changed over time as a result of the interest in processing new types of documents. The article continues with a brief history of the field and highlights the main challenges posed by the evaluation of summaries. The …},
	pages = {735--751},
	number = {6},
	journaltitle = {Natural Language Engineering},
	author = {Opitz, Juri and Parcalabescu, Letitia and Frank, Anette and Orăsan, Constantin},
	date = {2019},
	note = {{ISBN}: 1351-3249
	Publisher: Cambridge University Press
	Type: Journal article},
	annotation = {Times cited: 1}
}

@article{ott_fairseq_2019,
	title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
	url = {http://arxiv.org/abs/1904.01038v1},
	abstract = {fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on {PyTorch} and supports distributed training across multiple {GPUs} and machines. We also support fast mixed-precision training and inference on modern {GPUs}. A demo video can be found at https://www.youtube.com/watch?v={OtgDdWtHvto}},
	pages = {1904.01038v1},
	journaltitle = {{arXiv}},
	author = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
	date = {2019},
	note = {Type: Journal article},
	annotation = {{NAACL} 2019 Demo paper}
}

@article{otter_survey_2018,
	title = {A Survey of the Usages of Deep Learning in Natural Language Processing},
	url = {http://arxiv.org/abs/1807.10854v3},
	abstract = {Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field.},
	pages = {1807.10854v3},
	journaltitle = {{arXiv}},
	author = {Otter, Daniel W. and Medina, Julian R. and Kalita, Jugal K.},
	date = {2018},
	note = {Type: Journal article}
}

@article{pang_opinion_2008,
	title = {Opinion Mining and Sentiment Analysis},
	volume = {2},
	url = {https://doi.org/10.1561/1500000011},
	doi = {10.1561/1500000011},
	abstract = {An important part of our information-gathering behavior has always been to find out what other people think. With the growing availability and popularity of opinion-rich resources such as online review sites and personal blogs, new opportunities and challenges arise as people now can, and do, actively use information technologies to seek out and understand the opinions of others. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has thus occurred at least in part as a direct response to the surge of interest in new systems that deal directly with opinions as a first-class object.This survey covers techniques and approaches that promise to directly enable opinion-oriented information-seeking systems. Our focus is on methods that seek to address the new challenges raised by sentiment-aware applications, as compared to those that are already present in more traditional fact-based analysis. We include material on summarization of evaluative text and on broader issues regarding privacy, manipulation, and economic impact that the development of opinion-oriented information-access services gives rise to. To facilitate future work, a discussion of available resources, benchmark datasets, and evaluation campaigns is also provided.},
	pages = {1--135},
	number = {1},
	journaltitle = {Found. Trends Inf. Retr.},
	author = {Pang, Bo and Lee, Lillian},
	date = {2008},
	note = {{ISBN}: 1554-0669
	Place: Hanover, {MA}, {USA}
	Publisher: Now Publishers Inc.
	Type: Journal article}
}

@book{papineni_neural_2001,
	location = {Morristown, {NJ}, {USA}},
	title = {Neural {AMR}: Sequence-to-Sequence Models for Parsing and Generation},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	date = {2001}
}

@book{papineni_bleu_2002,
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	volume = {Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1},
	pagetotal = {311-318},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	date = {2002}
}

@article{pascanu_how_2013,
	title = {How to Construct Deep Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1312.6026v5},
	abstract = {In this paper, we explore different ways to extend a recurrent neural network ({RNN}) to a \textit{deep} {RNN}. We start by arguing that the concept of depth in an {RNN} is not as clear as it is in feedforward neural networks. By carefully analyzing and understanding the architecture of an {RNN}, however, we find three points of an {RNN} which may be made deeper; (1) input-to-hidden function, (2) hidden-to-hidden transition and (3) hidden-to-output function. Based on this observation, we propose two novel architectures of a deep {RNN} which are orthogonal to an earlier attempt of stacking multiple recurrent layers to build a deep {RNN} (Schmidhuber, 1992; El Hihi and Bengio, 1996). We provide an alternative interpretation of these deep {RNNs} using a novel framework based on neural operators. The proposed deep {RNNs} are empirically evaluated on the tasks of polyphonic music prediction and language modeling. The experimental result supports our claim that the proposed deep {RNNs} benefit from the depth and outperform the conventional, shallow {RNNs}.},
	pages = {1312.6026v5},
	journaltitle = {{arXiv}},
	author = {Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
	date = {2013},
	note = {Type: Journal article},
	annotation = {Accepted at {ICLR} 2014 (Conference Track). 10-page text + 3-page references}
}

@article{pascanu_difficulty_2013,
	title = {On the difficulty of training recurrent neural networks},
	url = {http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf?source=post_page—————————},
	abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al.(1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients …},
	pages = {1310--1318},
	journaltitle = {jmlr.org},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	date = {2013},
	note = {Type: Journal article}
}

@article{pasunuru_multi-reward_2018,
	title = {Multi-reward reinforced summarization with saliency and entailment},
	url = {https://arxiv.org/pdf/1804.06451},
	abstract = {Abstractive text summarization is the task of compressing and rewriting a long document into a short summary while maintaining saliency, directed logical entailment, and non-redundancy. In this work, we address these three important aspects of a good summary via a reinforcement learning approach with two novel reward functions: {ROUGESal} and Entail, on top of a coverage-based baseline. The {ROUGESal} reward modifies the {ROUGE} metric by up-weighting the salient phrases/words detected via a keyphrase classifier. The Entail reward …},
	journaltitle = {{arXiv} preprint {arXiv}:1804.06451},
	author = {Pasunuru, Ramakanth and Bansal, Mohit},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 89}
}

@article{paulus_deep_2017,
	title = {A deep reinforced model for abstractive summarization},
	url = {https://arxiv.org/pdf/1705.04304},
	abstract = {Attentional, {RNN}-based encoder-decoder models for abstractive summarization have achieved good performance on short input and output sequences. For longer documents and summaries however these models often include repetitive and incoherent phrases. We …},
	journaltitle = {{arXiv} preprint {arXiv}:1705.04304},
	author = {Paulus, Romain and Xiong, Caiming and Socher, Richard},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Times cited: 723}
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: Machine Learning in Python},
	volume = {12},
	pages = {2825--2830},
	journaltitle = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	date = {2011},
	note = {Type: Journal article}
}

@article{peters_deep_2018,
	title = {Deep contextualized word representations},
	url = {http://arxiv.org/abs/1802.05365v2 http://arxiv.org/pdf/1802.05365v2},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model ({biLM}), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging {NLP} problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	pages = {1802.05365v2},
	journaltitle = {{arXiv}},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{NAACL} 2018. Originally posted to openreview 27 Oct 2017. v2 updated for {NAACL} camera ready}
}

@book{peyrard_studying_2019,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Studying Summarization Evaluation Metrics in the Appropriate Scoring Range},
	publisher = {Association for Computational Linguistics},
	author = {Peyrard, Maxime},
	date = {2019}
}

@book{peyrard_learning_2017,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Learning to Score System Summaries for Better Content Selection Evaluation.},
	publisher = {Association for Computational Linguistics},
	author = {Peyrard, Maxime and Botschen, Teresa and Gurevych, Iryna},
	date = {2017}
}

@book{popovic_chrf_2015,
	title = {{chrF}: character n-gram F-score for automatic {MT} evaluation},
	volume = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
	abstract = {We propose the use of character n-gram F-score for automatic evaluation of machine translation output. Character ngrams have already been used as a part of more complex metrics, but their individual potential has not been investigated yet. We report system-level correlations with human rankings for 6-gram F1-score ({CHRF}) on the {WMT}12, {WMT}13 and {WMT}14 data as well as segment-level correlation for 6-gram F1 ({CHRF}) and F3-scores ({CHRF}3) on {WMT}14 data for all available target languages. The results are very promising …},
	pagetotal = {392-395},
	author = {Popović, Maja},
	date = {2015}
}

@article{qazvinian_scientific_2008,
	title = {Scientific Paper Summarization Using Citation Summary Networks},
	url = {http://arxiv.org/abs/0807.1560v1 http://arxiv.org/pdf/0807.1560v1},
	abstract = {Quickly moving to a new area of research is painful for researchers due to the vast amount of scientific literature in each field of study. One possible way to overcome this problem is to summarize a scientific topic. In this paper, we propose a model of summarizing a single article, which can be further used to summarize an entire topic. Our model is based on analyzing others’ viewpoint of the target article’s contributions and the study of its citation summary network using a clustering approach.},
	pages = {0807.1560v1},
	journaltitle = {{arXiv}},
	author = {Qazvinian, Vahed and Radev, Dragomir R.},
	date = {2008},
	note = {Type: Journal article}
}

@article{radev_introduction_2002,
	title = {Introduction to the special issue on summarization},
	volume = {28},
	url = {https://www.mitpressjournals.org/doi/pdf/10.1162/089120102762671927},
	abstract = {As the amount of on-line information increases, systems that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [{NIST}’s] Text Retrieval Conference [{TREC}]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia …},
	pages = {399--408},
	number = {4},
	journaltitle = {Computational linguistics},
	author = {Radev, Dragomir R and Hovy, Eduard and {McKeown}, Kathleen},
	date = {2002},
	note = {{ISBN}: 0891-2017
	Publisher: {MIT} Press
	Type: Journal article},
	annotation = {Times cited: 603}
}

@article{raffel_exploring_2019,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	url = {http://arxiv.org/abs/1910.10683v3},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new “Colossal Clean Crawled Corpus’’, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	pages = {1910.10683v3},
	journaltitle = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Final version as published in {JMLR}}
}

@book{raganato_analysis_2018,
	title = {An analysis of encoder representations in transformer-based machine translation},
	volume = {Proceedings of the 2018 {EMNLP} Workshop {BlackboxNLP}: Analyzing and Interpreting Neural Networks for {NLP}},
	abstract = {{\textless}span dir=ltr{\textgreater}The attention mechanism is a successful technique in modern {NLP}, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the …{\textless}/span{\textgreater}‏},
	publisher = {The Association for Computational Linguistics},
	author = {Raganato, Alessandro and Tiedemann, Jörg},
	date = {2018}
}

@article{rankel_better_2012,
	title = {Better Metrics to Automatically Predict the Quality of a Text Summary},
	volume = {5},
	url = {http://dx.doi.org/10.3390/a5040398},
	doi = {10.3390/a5040398},
	abstract = {{\textless}jats:p{\textgreater}In this paper we demonstrate a family of metrics for estimating the quality of a text summary relative to one or more human-generated summaries. The improved metrics are based on features automatically computed from the summaries to measure content and linguistic quality. The features are combined using one of three methods—robust regression, non-negative least squares, or canonical correlation, an eigenvalue method. The new metrics significantly outperform the previous standard for automatic text summarization evaluation, {ROUGE}.{\textless}/jats:p},
	pages = {398--420},
	number = {4},
	journaltitle = {Algorithms},
	author = {Rankel, Peter A. and Conroy, John M. and Schlesinger, Judith D.},
	date = {2012},
	note = {Publisher: {MDPI} {AG}
	Type: Journal article},
	annotation = {Times cited: 4}
}

@book{ray_opportunistic_2018,
	location = {New York, {NY}, {USA}},
	title = {Opportunistic Power Savings with Coordinated Control in Data Center Networks},
	publisher = {{ACM}},
	author = {Ray, Madhurima and Sondur, Sanjeev and Biswas, Joyanta and Pal, Amitangshu and Kant, Krishna},
	date = {2018}
}

@article{reiter_structured_2018,
	title = {A structured review of the validity of {BLEU}},
	volume = {44},
	abstract = {{\textless}span dir=ltr{\textgreater}The {BLEU} metric has been widely used in {NLP} for over 15 years to evaluate {NLP} systems, especially in machine translation and natural language generation. I present a structured review of the evidence on whether {BLEU} is a valid evaluation technique—in other words, whether {BLEU} scores correlate with real-world utility and user-satisfaction of {NLP} systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using {BLEU} for diagnostic evaluation of {MT} systems (which is what it was originally …{\textless}/span{\textgreater}‏},
	pages = {393--401},
	number = {3},
	journaltitle = {Computational Linguistics},
	author = {Reiter, Ehud},
	date = {2018},
	note = {{ISBN}: 1530-9312
	Publisher: {MIT} Press
	Type: Journal article}
}

@article{ribeiro_enhancing_2019,
	title = {Enhancing {AMR}-to-Text Generation with Dual Graph Representations},
	url = {http://arxiv.org/abs/1909.00352v1 http://arxiv.org/pdf/1909.00352v1},
	abstract = {Generating text from graph-based data, such as Abstract Meaning Representation ({AMR}), is a challenging task due to the inherent difficulty in how to properly encode the structure of a graph with labeled edges. To address this difficulty, we propose a novel graph-to-sequence model that encodes different but complementary perspectives of the structural information contained in the {AMR} graph. The model learns parallel top-down and bottom-up representations of nodes capturing contrasting views of the graph. We also investigate the use of different node message passing strategies, employing different state-of-the-art graph encoders to compute node representations based on incoming and outgoing perspectives. In our experiments, we demonstrate that the dual graph representation leads to improvements in {AMR}-to-text generation, achieving state-of-the-art results on two {AMR} datasets.},
	pages = {1909.00352v1},
	journaltitle = {{arXiv}},
	author = {Ribeiro, Leonardo F. R. and Gardent, Claire and Gurevych, Iryna},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted as a long conference paper to {EMNLP} 2019}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	url = {https://www.nature.com/articles/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create …},
	pages = {533--536},
	number = {6088},
	journaltitle = {nature},
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	date = {1986},
	note = {{ISBN}: 1476-4687
	Publisher: Nature Publishing Group
	Type: Journal article},
	annotation = {Times cited: 22676}
}

@article{rush_neural_2015,
	title = {A Neural Attention Model for Abstractive Sentence Summarization},
	url = {http://arxiv.org/abs/1509.00685v2},
	abstract = {Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the {DUC}-2004 shared task compared with several strong baselines.},
	pages = {1509.00685v2},
	journaltitle = {{arXiv}},
	author = {Rush, Alexander M. and Chopra, Sumit and Weston, Jason},
	date = {2015},
	note = {Type: Journal article},
	annotation = {Proceedings of {EMNLP} 2015}
}

@incollection{saggion_automatic_2013,
	title = {Automatic text summarization: Past, present and future},
	abstract = {Automatic text summarization, the computer-based production of condensed versions of documents, is an important technology for the information society. Without summaries it would be practically impossible for human beings to get access to the ever growing mass of information available online. Although research in text summarization is over 50 years old, some efforts are still needed given the insufficient quality of automatic summaries and the number of interesting summarization topics being proposed in different contexts by end …},
	pages = {3--21},
	publisher = {Springer},
	author = {Saggion, Horacio and Poibeau, Thierry},
	date = {2013}
}

@book{samira_evaluation_2013,
	title = {An Evaluation Summary Method Based on a Combination of Content and Linguistic Metrics {RANLP}},
	author = {Samira, Ellouze and M., Jaoua and L., Belguith},
	date = {2013}
}

@article{samira_merging_2017,
	title = {Merging Multiple Features to Evaluate the Content of Text Summary},
	volume = {58},
	pages = {69--76},
	journaltitle = {Proces. del Leng. Natural},
	author = {Samira, Ellouze and M., Jaoua and L., Belguith},
	date = {2017},
	note = {Type: Journal article}
}

@article{santhanam_survey_2019,
	title = {A Survey of Natural Language Generation Techniques with a Focus on Dialogue Systems - Past, Present and Future Directions},
	url = {http://arxiv.org/abs/1906.00500v1},
	abstract = {One of the hardest problems in the area of Natural Language Processing and Artificial Intelligence is automatically generating language that is coherent and understandable to humans. Teaching machines how to converse as humans do falls under the broad umbrella of Natural Language Generation. Recent years have seen unprecedented growth in the number of research articles published on this subject in conferences and journals both by academic and industry researchers. There have also been several workshops organized alongside top-tier {NLP} conferences dedicated specifically to this problem. All this activity makes it hard to clearly define the state of the field and reason about its future directions. In this work, we provide an overview of this important and thriving area, covering traditional approaches, statistical approaches and also approaches that use deep neural networks. We provide a comprehensive review towards building open domain dialogue systems, an important application of natural language generation. We find that, predominantly, the approaches for building dialogue systems use seq2seq or language models architecture. Notably, we identify three important areas of further research towards building more effective dialogue systems: 1) incorporating larger context, including conversation context and world knowledge; 2) adding personae or personality in the {NLG} system; and 3) overcoming dull and generic responses that affect the quality of system-produced responses. We provide pointers on how to tackle these open problems through the use of cognitive architectures that mimic human language understanding and generation capabilities.},
	pages = {1906.00500v1},
	journaltitle = {{arXiv}},
	author = {Santhanam, Sashank and Shaikh, Samira},
	date = {2019},
	note = {Type: Journal article}
}

@article{schmidhuber_deep_2014,
	title = {Deep Learning in Neural Networks: An Overview},
	url = {http://arxiv.org/abs/1404.7828v4},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
	pages = {1404.7828v4},
	journaltitle = {{arXivNeural} Networks, Vol 61, pp 85-117, Jan 2015},
	author = {Schmidhuber, Juergen},
	date = {2014},
	note = {Type: Journal article},
	annotation = {88 pages, 888 references}
}

@article{sebastian_mlxtend_2018,
	title = {{MLxtend}: Providing machine learning and data science utilities and extensions to Python’s scientific computing stack},
	volume = {3},
	url = {http://joss.theoj.org/papers/10.21105/joss.00638},
	doi = {10.21105/joss.00638},
	number = {24},
	journaltitle = {The Journal of Open Source Software},
	author = {Sebastian, Raschka},
	date = {2018},
	note = {Publisher: The Open Journal
	Type: Journal article}
}

@article{sedgwick_spearmans_2014,
	title = {Spearman’s rank correlation coefficient},
	volume = {349},
	url = {https://www.bmj.com/content/349/bmj.g7327},
	doi = {10.1136/bmj.g7327},
	journaltitle = {{BMJ}},
	author = {Sedgwick, Philip},
	date = {2014},
	note = {{ISBN}: 0959-8138
	Publisher: {BMJ} Publishing Group Ltd
	Type: Journal article}
}

@article{see_get_2017,
	title = {Get To The Point: Summarization with Pointer-Generator Networks},
	url = {http://arxiv.org/abs/1704.04368v2},
	abstract = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the {CNN} / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 {ROUGE} points.},
	pages = {1704.04368v2},
	journaltitle = {{arXiv}},
	author = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
	date = {2017},
	note = {Type: Journal article},
	annotation = {Add {METEOR} evaluation results, add some citations, fix some equations (what are now equations 1, 8 and 11 were missing a bias term), fix url to pyrouge package, add acknowledgments}
}

@article{sellam_bleurt_2020,
	title = {{BLEURT}: Learning Robust Metrics for Text Generation},
	url = {http://arxiv.org/abs/2004.04696v5 http://arxiv.org/pdf/2004.04696v5},
	abstract = {Text generation has made significant advances in the last few years. Yet, evaluation metrics have lagged behind, as the most popular choices (e.g., {BLEU} and {ROUGE}) may correlate poorly with human judgments. We propose {BLEURT}, a learned evaluation metric based on {BERT} that can model human judgments with a few thousand possibly biased training examples. A key aspect of our approach is a novel pre-training scheme that uses millions of synthetic examples to help the model generalize. {BLEURT} provides state-of-the-art results on the last three years of the {WMT} Metrics shared task and the {WebNLG} Competition dataset. In contrast to a vanilla {BERT}-based approach, it yields superior results even when the training data is scarce and out-of-distribution.},
	pages = {2004.04696v5},
	journaltitle = {{arXiv}},
	author = {Sellam, Thibault and Das, Dipanjan and Parikh, Ankur P.},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted at {ACL} 2020}
}

@book{sevastjanova_going_2018,
	title = {Going beyond visualization: Verbalization as complementary medium to explain machine learning models},
	volume = {Workshop on Visualization for {AI} Explainability at {IEEE} {VIS}},
	abstract = {In this position paper, we argue that a combination of visualization and verbalization techniques is beneficial for creating broad and versatile insights into the structure and decision-making processes of machine learning models. Explainability of machine learning models is emerging as an important area of research. Hence, insights into the inner workings of a trained model allow users and analysts, alike, to understand the models, develop justifications, and gain trust in the systems they inform. Explanations can be …},
	author = {Sevastjanova, Rita and Beck, Fabian and Ell, Basil and Turkay, Cagatay and Henkin, Rafael and Butt, Miriam and Keim, Daniel A and El-Assady, Mennatallah},
	date = {2018}
}

@article{sharma_entity-driven_2019,
	title = {An entity-driven framework for abstractive summarization},
	url = {https://arxiv.org/pdf/1909.02059},
	abstract = {Abstractive summarization systems aim to produce more coherent and concise summaries than their extractive counterparts. Popular neural models have achieved impressive results for single-document summarization, yet their outputs are often incoherent and unfaithful to the input. In this paper, we introduce {SENECA}, a novel System for {ENtity}-{drivEn} Coherent Abstractive summarization framework that leverages entity information to generate informative and coherent abstracts. Our framework takes a two-step approach:(1) an entity …},
	journaltitle = {{arXiv} preprint {arXiv}:1909.02059},
	author = {Sharma, Eva and Huang, Luyang and Hu, Zhe and Wang, Lu},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Times cited: 17}
}

@article{shi_neural_2018,
	title = {Neural Abstractive Text Summarization with Sequence-to-Sequence Models},
	url = {http://arxiv.org/abs/1812.02303v4},
	abstract = {In the past few years, neural abstractive text summarization with sequence-to-sequence (seq2seq) models have gained a lot of popularity. Many interesting techniques have been proposed to improve seq2seq models, making them capable of handling different challenges, such as saliency, fluency and human readability, and generate high-quality summaries. Generally speaking, most of these techniques differ in one of these three categories: network structure, parameter inference, and decoding/generation. There are also other concerns, such as efficiency and parallelism for training a model. In this paper, we provide a comprehensive literature survey on different seq2seq models for abstractive text summarization from the viewpoint of network structures, training strategies, and summary generation algorithms. Several models were first proposed for language modeling and generation tasks, such as machine translation, and later applied to abstractive text summarization. Hence, we also provide a brief review of these models. As part of this survey, we also develop an open source library, namely, Neural Abstractive Text Summarizer ({NATS}) toolkit, for the abstractive text summarization. An extensive set of experiments have been conducted on the widely used {CNN}/Daily Mail dataset to examine the effectiveness of several different neural network components. Finally, we benchmark two models implemented in {NATS} on the two recently released datasets, namely, Newsroom and Bytecup.},
	pages = {1812.02303v4},
	journaltitle = {{arXiv}},
	author = {Shi, Tian and Keneshloo, Yaser and Ramakrishnan, Naren and Reddy, Chandan K.},
	date = {2018},
	note = {Type: Journal article}
}

@article{shi_leafnats_2019,
	title = {{LeafNATS}: An Open-Source Toolkit and Live Demo System for Neural Abstractive Text Summarization},
	url = {http://arxiv.org/abs/1906.01512v1},
	abstract = {Neural abstractive text summarization ({NATS}) has received a lot of attention in the past few years from both industry and academia. In this paper, we introduce an open-source toolkit, namely {LeafNATS}, for training and evaluation of different sequence-to-sequence based models for the {NATS} task, and for deploying the pre-trained models to real-world applications. The toolkit is modularized and extensible in addition to maintaining competitive performance in the {NATS} task. A live news blogging system has also been implemented to demonstrate how these models can aid blog/news editors by providing them suggestions of headlines and summaries of their articles.},
	pages = {1906.01512v1},
	journaltitle = {{arXiv}},
	author = {Shi, Tian and Wang, Ping and Reddy, Chandan K.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted by {NAACL}-{HLT} 2019 demo track}
}

@article{silver_mastering_2016,
	title = {Mastering the game of Go with deep neural networks and tree search},
	volume = {529},
	url = {https://www.nature.com/articles/nature16961?MRK_CMPG_SOURCE=sm_tw_pp},
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value …},
	pages = {484--489},
	number = {7587},
	journaltitle = {nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc},
	date = {2016},
	note = {{ISBN}: 1476-4687
	Publisher: Nature Publishing Group
	Type: Journal article},
	annotation = {Times cited: 9947}
}

@book{singh_value_2016,
	title = {Value and Energy Aware Adaptive Resource Allocation of Soft Real-Time Jobs on Many-Core {HPC} Data Centers},
	publisher = {{IEEE}},
	author = {Singh, Amit Kumar and Dziurzanski, Piotr and Indrusiak, Leandro Soares},
	date = {2016}
}

@book{sipos_generating_2013,
	location = {New York, New York, {USA}},
	title = {Generating comparative summaries from reviews},
	publisher = {{ACM} Press},
	author = {Sipos, Ruben and Joachims, Thorsten},
	date = {2013}
}

@thesis{sloman_knowing_1962,
	title = {Knowing and Understanding: Relations between meaning and truth, meaning and necessary truth, meaning and synthetic necessary truth},
	abstract = {The avowed aim of the thesis is to show that there are some synthetic necessary truths, or that synthetic apriori knowledge is possible. This is really a pretext for an investigation into the general connection between meaning and truth, or between understanding and …},
	institution = {University of Oxford},
	type = {phdthesis},
	author = {Sloman, Aaron},
	date = {1962}
}

@article{song_controlling_2019,
	title = {Controlling the Amount of Verbatim Copying in Abstractive Summarization},
	url = {http://arxiv.org/abs/1911.10390v1},
	abstract = {An abstract must not change the meaning of the original text. A single most effective way to achieve that is to increase the amount of copying while still allowing for text abstraction. Human editors can usually exercise control over copying, resulting in summaries that are more extractive than abstractive, or vice versa. However, it remains poorly understood whether modern neural abstractive summarizers can provide the same flexibility, i.e., learning from single reference summaries to generate multiple summary hypotheses with varying degrees of copying. In this paper, we present a neural summarization model that, by learning from single human abstracts, can produce a broad spectrum of summaries ranging from purely extractive to highly generative ones. We frame the task of summarization as language modeling and exploit alternative mechanisms to generate summary hypotheses. Our method allows for control over copying during both training and decoding stages of a neural summarization model. Through extensive experiments we illustrate the significance of our proposed method on controlling the amount of verbatim copying and achieve competitive results over strong baselines. Our analysis further reveals interesting and unobvious facts.},
	pages = {1911.10390v1},
	journaltitle = {{arXiv}},
	author = {Song, Kaiqiang and Wang, Bingqing and Feng, Zhe and Ren, Liu and Liu, Fei},
	date = {2019},
	note = {Type: Journal article},
	annotation = {{AAAI} 2020 (Main Technical Track)}
}

@article{song_graph--sequence_2018,
	title = {A Graph-to-Sequence Model for {AMR}-to-Text Generation},
	url = {http://arxiv.org/abs/1805.02473v3 http://arxiv.org/pdf/1805.02473v3},
	abstract = {The problem of {AMR}-to-text generation is to recover a text representing the same meaning as an input {AMR} graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging {LSTM} for encoding a linearized {AMR} structure. Although being able to model non-local semantic information, a sequence {LSTM} can lose information from the {AMR} graph structure, and thus faces challenges with large graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel {LSTM} structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.},
	pages = {1805.02473v3},
	journaltitle = {{arXiv}},
	author = {Song, Linfeng and Zhang, Yue and Wang, Zhiguo and Gildea, Daniel},
	date = {2018},
	note = {Type: Journal article},
	annotation = {{ACL} 2018 camera-ready, Proceedings of {ACL} 2018 with updated performance}
}

@article{staudemeyer_understanding_2019,
	title = {Understanding {LSTM} - a tutorial into Long Short-Term Memory Recurrent Neural Networks},
	url = {http://arxiv.org/abs/1909.09586v1},
	abstract = {Long Short-Term Memory Recurrent Neural Networks ({LSTM}-{RNN}) are one of the most powerful dynamic classifiers publicly known. The network itself and the related learning algorithms are reasonably well documented to get an idea how it works. This paper will shed more light into understanding how {LSTM}-{RNNs} evolved and why they work impressively well, focusing on the early, ground-breaking publications. We significantly improved documentation and fixed a number of errors and inconsistencies that accumulated in previous publications. To support understanding we as well revised and unified the notation used.},
	pages = {1909.09586v1},
	journaltitle = {{arXiv}},
	author = {Staudemeyer, Ralf C. and Morris, Eric Rothstein},
	date = {2019},
	note = {Type: Journal article},
	annotation = {42 pages, 11 figures, tutorial}
}

@article{subramanian_extractive_2019,
	title = {On Extractive and Abstractive Neural Document Summarization with Transformer Language Models},
	url = {http://arxiv.org/abs/1909.03186v2},
	abstract = {We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization. We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary. We show that this extractive step significantly improves summarization results. We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher rouge scores. Note: The abstract above was not written by the authors, it was generated by one of the models presented in this paper.},
	pages = {1909.03186v2},
	journaltitle = {{arXiv}},
	author = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher},
	date = {2019},
	note = {Type: Journal article}
}

@article{suleiman_deep_2020,
	title = {Deep Learning Based Abstractive Text Summarization: Approaches, Datasets, Evaluation Measures, and Challenges},
	volume = {2020},
	url = {http://downloads.hindawi.com/journals/mpe/2020/9365340.pdf},
	doi = {10.1155/2020/9365340},
	abstract = {In recent years, the volume of textual data has rapidly increased, which has generated a valuable resource for extracting and analysing information. To retrieve useful knowledge within a reasonable time period, this information must be summarised. This paper reviews recent approaches for abstractive text summarisation using deep learning models. In addition, existing datasets for training and validating these approaches are reviewed, and their features and limitations are presented. The Gigaword dataset is commonly employed for single-sentence summary approaches, while the Cable News Network ({CNN})/Daily Mail dataset is commonly employed for multisentence summary approaches. Furthermore, the measures that are utilised to evaluate the quality of summarisation are investigated, and Recall-Oriented Understudy for Gisting Evaluation 1 ({ROUGE}1), {ROUGE}2, and {ROUGE}-L are determined to be the most commonly applied metrics. The challenges that are encountered during the summarisation process and the solutions proposed in each approach are analysed. The analysis of the several approaches shows that recurrent neural networks with an attention mechanism and long short-term memory ({LSTM}) are the most prevalent techniques for abstractive text summarisation. The experimental results show that text summarisation with a pretrained encoder model achieved the highest values for {ROUGE}1, {ROUGE}2, and {ROUGE}-L (43.85, 20.34, and 39.9, respectively). Furthermore, it was determined that most abstractive text summarisation models faced challenges such as the unavailability of a golden token at testing time, out-of-vocabulary ({OOV}) words, summary sentence repetition, inaccurate sentences, and fake facts.},
	pages = {1--29},
	journaltitle = {Mathematical Problems in Engineering},
	author = {Suleiman, Dima and Awajan, Arafat},
	date = {2020},
	note = {Publisher: Hindawi Limited
	Type: Journal article},
	keywords = {General Engineering, General Mathematics}
}

@book{sutskever_generating_2011,
	title = {Generating text with recurrent neural networks},
	volume = {{ICML}},
	abstract = {Recurrent Neural Networks ({RNNs}) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training {RNNs}, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of {RNNs} trained with the new Hessian-Free optimizer ({HF}) by applying them to character-level language modeling tasks …},
	author = {Sutskever, Ilya and Martens, James and Hinton, Geoffrey E},
	date = {2011}
}

@book{sutskever_sequence_2014,
	title = {Sequence to sequence learning with neural networks},
	volume = {Advances in neural information processing systems},
	abstract = {Abstract Deep Neural Networks ({DNNs}) are powerful models that have achieved excellent performance on difficult learning tasks. Although {DNNs} work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory ({LSTM}) to map the input sequence to a vector of a fixed dimensionality, and then …},
	pagetotal = {3104-3112},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	date = {2014}
}

@book{sutton_reinforcement_2018,
	title = {Reinforcement learning: An introduction},
	abstract = {The significantly expanded and updated new edition of a widely used text on reinforcement learning, one of the most active research areas in artificial intelligence. Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational …},
	publisher = {{MIT} press},
	author = {Sutton, Richard S and Barto, Andrew G},
	date = {2018}
}

@book{takase_neural_2016,
	title = {Neural headline generation on abstract meaning representation},
	volume = {Proceedings of the 2016 conference on empirical methods in natural language processing},
	abstract = {Neural network-based encoder-decoder models are among recent attractive methodologies for tackling natural language generation tasks. This paper investigates the usefulness of structural syntactic and semantic information additionally incorporated in a baseline neural attention-based model. We encode results obtained from an abstract meaning representation ({AMR}) parser using a modified version of Tree-{LSTM}. Our proposed attention-based {AMR} encoder-decoder model improves headline generation benchmarks compared …},
	pagetotal = {1054-1059},
	author = {Takase, Sho and Suzuki, Jun and Okazaki, Naoaki and Hirao, Tsutomu and Nagata, Masaaki},
	date = {2016}
}

@article{tallec_can_2018,
	title = {Can recurrent neural networks warp time},
	url = {http://arxiv.org/abs/1804.11188v1 http://arxiv.org/pdf/1804.11188v1},
	abstract = {Successful recurrent models such as long short-term memories ({LSTMs}) and gated recurrent units ({GRUs}) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data. We recover part of the {LSTM} architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in {LSTMs} and {GRUs}. Ex- perimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.},
	pages = {1804.11188v1},
	journaltitle = {{arXiv}},
	author = {Tallec, Corentin and Ollivier, Yann},
	date = {2018},
	note = {Type: Journal article}
}

@book{tan_abstractive_2017,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Abstractive Document Summarization with a Graph-Based Attentional Neural Model},
	publisher = {Association for Computational Linguistics},
	author = {Tan, Jiwei and Wan, Xiaojun and Xiao, Jianguo},
	date = {2017}
}

@book{tan_energy-aware_2017,
	location = {New York, New York, {USA}},
	title = {An Energy-aware Virtual Machine Placement Algorithm in Cloud Data Center},
	publisher = {{ACM} Press},
	author = {Tan, Mingzhe and Chi, Ce and Zhang, Jiahao and Zhao, Shichang and Li, Guangli and Lü, Shuai},
	date = {2017}
}

@book{tan_introduction_2019,
	title = {Introduction to Data Mining},
	publisher = {Pearson},
	author = {Tan, P.N. and Steinbach, M. and Karpatne, A. and Kumar, V.},
	date = {2019}
}

@article{tas_survey_2017,
	title = {A survey automatic text summarization},
	volume = {5},
	url = {http://dx.doi.org/10.17261/pressacademia.2017.591},
	doi = {10.17261/pressacademia.2017.591},
	pages = {205--213},
	number = {1},
	journaltitle = {Pressacademia},
	author = {Tas, Oguzhan and Kiyani, Farzad},
	date = {2017},
	note = {Publisher: Pressacademia
	Type: Journal article},
	annotation = {Times cited: 3}
}

@article{tian_um-corpus_2014,
	title = {{UM}-Corpus: A Large English-Chinese Parallel Corpus for Statistical Machine Translation.},
	url = {https://www.researchgate.net/profile/Derek_Wong5/publication/291824181_UM-Corpus_a_large_English-Chinese_parallel_corpus_for_statistical_machine_translation/links/5c1f65fb458515a4c7f30171/UM-Corpus-a-large-English-Chinese-parallel-corpus-for-statistical-machine-translation.pdf},
	abstract = {Parallel corpus is a valuable resource for cross-language information retrieval and data-driven natural language processing systems, especially for Statistical Machine Translation ({SMT}). However, most existing parallel corpora to Chinese are subject to in-house use, while others are domain specific and limited in size. To a certain degree, this limits the {SMT} research. This paper describes the acquisition of a large scale and high quality parallel corpora for English and Chinese. The corpora constructed in this paper contain about 15 …},
	pages = {1837--1842},
	journaltitle = {researchgate.net},
	author = {Tian, Liang and Wong, Derek F and Chao, Lidia S and Quaresma, Paulo and Oliveira, Francisco and Yi, Lu},
	date = {2014},
	note = {Type: Journal article}
}

@book{toutanova_feature-rich_2003,
	title = {Feature-rich part-of-speech tagging with a cyclic dependency network},
	volume = {Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics},
	abstract = {We present a new part-of-speech tagger that demonstrates the following ideas:(i) explicit use of both preceding and following tag contexts via a dependency network representation,(ii) broad use of lexical features, including jointly conditioning on multiple …},
	pagetotal = {252-259},
	author = {Toutanova, Kristina and Klein, Dan and Manning, Christopher D and Singer, Yoram},
	date = {2003}
}

@article{vasilyev_is_2020,
	title = {Is human scoring the best criteria for summary evaluation},
	url = {http://arxiv.org/abs/2012.14602v1 http://arxiv.org/pdf/2012.14602v1},
	abstract = {Normally, summary quality measures are compared with quality scores produced by human annotators. A higher correlation with human scores is considered to be a fair indicator of a better measure. We discuss observations that cast doubt on this view. We attempt to show a possibility of an alternative indicator. Given a family of measures, we explore a criterion of selecting the best measure not relying on correlations with human scores. Our observations for the {BLANC} family of measures suggest that the criterion is universal across very different styles of summaries.},
	pages = {2012.14602v1},
	journaltitle = {{arXiv}},
	author = {Vasilyev, Oleg and Bohannon, John},
	date = {2020},
	note = {Type: Journal article},
	annotation = {7 pages, 5 figures, 1 table}
}

@book{vaswani_attention_2017,
	title = {Attention is all you need},
	volume = {Advances in neural information processing systems},
	abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm …},
	pagetotal = {5998-6008},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	date = {2017}
}

@book{vedantam_cider_2015,
	title = {{CIDEr}: Consensus-based image description evaluation},
	publisher = {{IEEE}},
	author = {Vedantam, Ramakrishna and Zitnick, C. Lawrence and Parikh, Devi},
	date = {2015}
}

@article{vig_multiscale_2019,
	title = {A Multiscale Visualization of Attention in the Transformer Model},
	url = {http://arxiv.org/abs/1906.05714v1 http://arxiv.org/pdf/1906.05714v1},
	abstract = {The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on {BERT} and {OpenAI} {GPT}-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.},
	pages = {1906.05714v1},
	journaltitle = {{arXiv}},
	author = {Vig, Jesse},
	date = {2019},
	note = {Type: Journal article},
	annotation = {To appear in {ACL} 2019 (System Demonstrations). {arXiv} admin note: substantial text overlap with {arXiv}:1904.02679}
}

@article{vig_analyzing_2019,
	title = {Analyzing the Structure of Attention in a Transformer Language Model},
	url = {http://arxiv.org/abs/1906.04284v2 http://arxiv.org/pdf/1906.04284v2},
	abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of {NLP} tasks. In this paper, we analyze the structure of attention in a Transformer language model, the {GPT}-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
	pages = {1906.04284v2},
	journaltitle = {{arXiv}},
	author = {Vig, Jesse and Belinkov, Yonatan},
	date = {2019},
	note = {Type: Journal article},
	annotation = {To appear in {ACL} {BlackboxNLP} workshop}
}

@book{vinyals_pointer_2015,
	title = {Pointer networks},
	volume = {Advances in neural information processing systems},
	abstract = {We introduce a new neural architecture to learn the conditional probability of an output sequence with elements that arediscrete tokens corresponding to positions in an input sequence. Such problems cannot be trivially addressed by existent approaches such as sequence-to-sequence and Neural Turing Machines, because the number of target classes in eachstep of the output depends on the length of the input, which is variable. Problems such as sorting variable sized sequences, and various combinatorialoptimization problems …},
	pagetotal = {2692-2700},
	author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
	date = {2015}
}

@article{virtanen_scipy_2020,
	title = {{SciPy} 1.0: Fundamental Algorithms for Scientific Computing in Python},
	volume = {17},
	doi = {10.1038/s41592-019-0686-2},
	pages = {261--272},
	journaltitle = {Nature Methods},
	author = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and and Haberland, Matt and Reddy, Tyler and Cournapeau, David and and Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and and Bright, Jonathan and van der Walt, Stefan J. and and Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and and Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and and Kern, Robert and Larson, Eric and Carey, C J and and Polat, Ilhan and Feng, Yu and Moore, Eric W. and and {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and and Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and and Harris, Charles R. and Archibald, Anne M. and and Ribeiro, Antonio H. and Pedregosa, Fabian and and van Mulbregt, Paul},
	date = {2020},
	note = {Type: Journal article}
}

@article{vu_towards_2010,
	title = {Towards automated related work summarization},
	url = {https://scholarbank.nus.edu.sg/handle/10635/20953},
	abstract = {This thesis introduces and describes the novel problem of automated related work summarization. Given multiple articles (eg, conference or journal papers) as input, and a set of keywords that describes a target paper? s topics of interest in a hierarchical fashion, a related work summarization system creates a topic-biased summary of related work specific to the target paper. This thesis has two main contributions. First, I conducted a deep manual analysis on various aspects of related work sections to identify their important characteristics …},
	journaltitle = {scholarbank.nus.edu.sg},
	author = {Vu, {HCD}},
	date = {2010},
	note = {Type: Journal article},
	annotation = {Times cited: 3}
}

@article{w_ordinal_1958,
	title = {Ordinal Measures of Association},
	volume = {53},
	pages = {814--861},
	journaltitle = {Journal of the American Statistical Association},
	author = {W., Kruskal},
	date = {1958},
	note = {Type: Journal article}
}

@article{wang_asking_2020,
	title = {Asking and answering questions to evaluate the factual consistency of summaries},
	url = {https://arxiv.org/pdf/2004.04228},
	abstract = {Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose an automatic evaluation protocol called {QAGS} (pronounced” kags”) that is designed to identify factual inconsistencies in a generated summary. {QAGS} is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is …},
	journaltitle = {{arXiv} preprint {arXiv}:2004.04228},
	author = {Wang, Alex and Cho, Kyunghyun and Lewis, Mike},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Times cited: 27}
}

@book{wanzheng_gruen_2020,
	title = {{GRUEN} for Evaluating Linguistic Quality of Generated Text {EMNLP}},
	author = {Wanzheng, Zhu and S., Bhat},
	date = {2020}
}

@article{weber_controlling_2018,
	title = {Controlling Decoding for More Abstractive Summaries with Copy-Based Networks},
	url = {http://arxiv.org/abs/1803.07038v2},
	abstract = {Attention-based neural abstractive summarization systems equipped with copy mechanisms have shown promising results. Despite this success, it has been noticed that such a system generates a summary by mostly, if not entirely, copying over phrases, sentences, and sometimes multiple consecutive sentences from an input paragraph, effectively performing extractive summarization. In this paper, we verify this behavior using the latest neural abstractive summarization system - a pointer-generator network. We propose a simple baseline method that allows us to control the amount of copying without retraining. Experiments indicate that the method provides a strong baseline for abstractive systems looking to obtain high {ROUGE} scores while minimizing overlap with the source article, substantially reducing the n-gram overlap with the original article while keeping within 2 points of the original model’s {ROUGE} score.},
	pages = {1803.07038v2},
	journaltitle = {{arXiv}},
	author = {Weber, Noah and Shekhar, Leena and Balasubramanian, Niranjan and Cho, Kyunghyun},
	date = {2018},
	note = {Type: Journal article}
}

@article{weissenborn_dynamic_2017,
	title = {Dynamic Integration of Background Knowledge in Neural {NLU} Systems},
	url = {http://arxiv.org/abs/1706.02596v3 http://arxiv.org/pdf/1706.02596v3},
	abstract = {Common-sense and background knowledge is required to understand natural language, but in most neural natural language understanding ({NLU}) systems, this knowledge must be acquired from training corpora during learning, and then it is static at test time. We introduce a new architecture for the dynamic integration of explicit background knowledge in {NLU} models. A general-purpose reading module reads background knowledge in the form of free-text statements (together with task-specific text inputs) and yields refined word representations to a task-specific {NLU} architecture that reprocesses the task inputs with these representations. Experiments on document question answering ({DQA}) and recognizing textual entailment ({RTE}) demonstrate the effectiveness and flexibility of the approach. Analysis shows that our model learns to exploit knowledge in a semantically appropriate way.},
	pages = {1706.02596v3},
	journaltitle = {{arXiv}},
	author = {Weissenborn, Dirk and Kočiský, Tomáš and Dyer, Chris},
	date = {2017},
	note = {Type: Journal article}
}

@article{wenbo_concept_2019,
	title = {Concept Pointer Network for Abstractive Summarization},
	url = {http://arxiv.org/abs/1910.08486v1},
	abstract = {A quality abstractive summary should not only copy salient source texts as summaries but should also tend to generate new conceptual words to express concrete details. Inspired by the popular pointer generator sequence-to-sequence model, this paper presents a concept pointer network for improving these aspects of abstractive summarization. The network leverages knowledge-based, context-aware conceptualizations to derive an extended set of candidate concepts. The model then points to the most appropriate choice using both the concept set and original source text. This joint approach generates abstractive summaries with higher-level semantic concepts. The training model is also optimized in a way that adapts to different data, which is based on a novel method of distantly-supervised learning guided by reference summaries and testing set. Overall, the proposed approach provides statistically significant improvements over several state-of-the-art models on both the {DUC}-2004 and Gigaword datasets. A human evaluation of the model’s abstractive abilities also supports the quality of the summaries produced within this framework.},
	pages = {1910.08486v1},
	journaltitle = {{arXiv}},
	author = {Wenbo, Wang and Yang, Gao and Heyan, Huang and Yuxiang, Zhou},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Accepted by {EMNLP}’2019}
}

@article{werbos_backpropagation_1990,
	title = {Backpropagation through time: what it does and how to do it},
	volume = {78},
	url = {https://ieeexplore.ieee.org/abstract/document/58337},
	doi = {10.1109/5.58337},
	abstract = {Basic backpropagation, which is a simple method now being widely used in areas like pattern recognition and fault diagnosis, is reviewed. The basic equations for backpropagation through time, and applications to areas like pattern recognition involving dynamic systems, systems identification, and control are discussed. Further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations, or true recurrent networks, and other practical issues arising with …},
	pages = {1550--1560},
	number = {10},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Werbos, Paul J},
	date = {1990},
	note = {{ISBN}: 0018-9219
	Publisher: {IEEE}
	Type: Journal article},
	annotation = {Times cited: 4392}
}

@book{wes_data_2010,
	title = {Data Structures for Statistical Computing in Python Proceedings of the 9th Python in Science Conference},
	pagetotal = {56 - 61},
	author = {Wes, {McKinney}},
	editor = {St\{{\textbackslash}textbackslash\}’efan, van der Walt and Jarrod, Millman},
	date = {2010}
}

@article{wolf_meta-learning_2018,
	title = {Meta-Learning a Dynamical Language Model},
	url = {http://arxiv.org/abs/1803.10631v1},
	abstract = {We consider the task of word-level language modeling and study the possibility of combining hidden-states-based short-term representations with medium-term representations encoded in dynamical weights of a language model. Our work extends recent experiments on language models with dynamically evolving weights by casting the language modeling problem into an online learning-to-learn framework in which a meta-learner is trained by gradient-descent to continuously update a language model weights.},
	pages = {1803.10631v1},
	journaltitle = {{arXiv}},
	author = {Wolf, Thomas and Chaumond, Julien and Delangue, Clement},
	date = {2018},
	note = {Type: Journal article},
	annotation = {5 pages, 2 figures, accepted at {ICLR} 2018 workshop track}
}

@article{wolf_huggingfaces_2019,
	title = {{HuggingFace}’s Transformers: State-of-the-art Natural Language Processing},
	url = {http://arxiv.org/abs/1910.03771v5},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified {API}. Backing this library is a curated collection of pretrained models made by and available for the community. \textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	pages = {1910.03771v5},
	journaltitle = {{arXiv}},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {8 pages, 4 figures, more details at https://github.com/huggingface/transformers}
}

@article{wu_top_2008,
	title = {Top 10 algorithms in data mining},
	volume = {14},
	url = {http://dx.doi.org/10.1007/s10115-007-0114-2},
	doi = {10.1007/s10115-007-0114-2},
	pages = {1--37},
	number = {1},
	journaltitle = {Knowledge and Information Systems},
	author = {Wu, Xindong and Kumar, Vipin and Ross Quinlan, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and {McLachlan}, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
	date = {2008},
	note = {Publisher: Springer Science and Business Media {LLC}
	Type: Journal article},
	annotation = {Times cited: 2440}
}

@article{wu_learning_2018,
	title = {Learning to Extract Coherent Summary via Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1804.07036v1 http://arxiv.org/pdf/1804.07036v1},
	abstract = {Coherence plays a critical role in producing a high-quality summary from a document. In recent years, neural extractive summarization is becoming increasingly attractive. However, most of them ignore the coherence of summaries when extracting sentences. As an effort towards extracting coherent summaries, we propose a neural coherence model to capture the cross-sentence semantic and syntactic coherence patterns. The proposed neural coherence model obviates the need for feature engineering and can be trained in an end-to-end fashion using unlabeled data. Empirical results show that the proposed neural coherence model can efficiently capture the cross-sentence coherence patterns. Using the combined output of the neural coherence model and {ROUGE} package as the reward, we design a reinforcement learning method to train a proposed neural extractive summarizer which is named Reinforced Neural Extractive Summarization ({RNES}) model. The {RNES} model learns to optimize coherence and informative importance of the summary simultaneously. Experimental results show that the proposed {RNES} outperforms existing baselines and achieves state-of-the-art performance in term of {ROUGE} on {CNN}/Daily Mail dataset. The qualitative evaluation indicates that summaries produced by {RNES} are more coherent and readable.},
	pages = {1804.07036v1},
	journaltitle = {{arXiv}},
	author = {Wu, Yuxiang and Hu, Baotian},
	date = {2018},
	note = {Type: Journal article},
	annotation = {8 pages, 1 figure, presented at {AAAI}-2018}
}

@article{xu_improving_2020,
	title = {Improving {AMR} Parsing with Sequence-to-Sequence Pre-training},
	url = {http://arxiv.org/abs/2010.01771v1 http://arxiv.org/pdf/2010.01771v1},
	abstract = {In the literature, the research on abstract meaning representation ({AMR}) parsing is much restricted by the size of human-curated dataset which is critical to build an {AMR} parser with good performance. To alleviate such data size restriction, pre-trained models have been drawing more and more attention in {AMR} parsing. However, previous pre-trained models, like {BERT}, are implemented for general purpose which may not work as expected for the specific task of {AMR} parsing. In this paper, we focus on sequence-to-sequence (seq2seq) {AMR} parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and {AMR} parsing itself. Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of {AMR} parsing while endeavors to preserve the response of pre-trained models. Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on {AMR} 2.0), which reaches the state of the art. The result is very encouraging since we achieve this with seq2seq models rather than complex models. We make our code and model available at https://github.com/xdqkid/S2S-{AMR}-Parser.},
	pages = {2010.01771v1},
	journaltitle = {{arXiv}},
	author = {Xu, Dongqin and Li, Junhui and Zhu, Muhua and Zhang, Min and Zhou, Guodong},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted by {EMNLP} 2020}
}

@article{xu_neural_2019,
	title = {Neural Extractive Text Summarization with Syntactic Compression},
	url = {http://arxiv.org/abs/1902.00863v2 http://arxiv.org/pdf/1902.00863v2},
	abstract = {Recent neural network approaches to summarization are largely either selection-based extraction or generation-based abstraction. In this work, we present a neural model for single-document summarization based on joint extraction and syntactic compression. Our model chooses sentences from the document, identifies possible compressions based on constituency parses, and scores those compressions with a neural model to produce the final summary. For learning, we construct oracle extractive-compressive summaries, then learn both of our components jointly with this supervision. Experimental results on the {CNN}/Daily Mail and New York Times datasets show that our model achieves strong performance (comparable to state-of-the-art systems) as evaluated by {ROUGE}. Moreover, our approach outperforms an off-the-shelf compression module, and human and manual evaluation shows that our model’s output generally remains grammatical.},
	pages = {1902.00863v2},
	journaltitle = {{arXiv}},
	author = {Xu, Jiacheng and Durrett, Greg},
	date = {2019},
	note = {Type: Journal article},
	annotation = {14 pages, {EMNLP} 2019}
}

@book{xu_fact-based_2020,
	location = {Stroudsburg, {PA}, {USA}},
	title = {Fact-based Content Weighting for Evaluating Abstractive Summarisation},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Xinnuo and Dušek, Ondřej and Li, Jingyi and Rieser, Verena and Konstas, Ioannis},
	date = {2020}
}

@book{y_efficient_2013,
	title = {An Efficient Power-Aware Resource Scheduling Strategy in Virtualized Datacenters},
	abstract = {In the era of cloud computing, data centers are well-known to be bounded by the power wall issue. This issue lowers the profit of service providers and obstructs the expansions of data center’s scale. As virtual machine’s behavior was not explored sufficiently in classic data center’s power-saving strategies, in this paper we address the power consumption issue in the setting of a virtualized data center. We propose an efficient power-aware resource scheduling strategy that reduces data center’s power consumption effectively based on {VM} live migration which is a key technical feature of cloud computing. Our scheduling algorithm leverages the Xen platform and consolidates {VM} workloads periodically to reduce the number of running servers. To satisfy each {VM}’s service level agreements, our strategy keeps adjusting {VM} placements between scheduling rounds. We developed a power-aware data center simulator to test our algorithm. The simulator runs in time domain and includes server’s segmented linear power model. We validated our simulator using measured server power trace. Our simulation shows that compared with event-driven schedulers, our strategy improves data center power budget by 35\% for random workloads resembling web-requests, and improve data center power budget by 22.7\% for workloads exhibiting stable resource requirements like {ScaLAPACK}.},
	pagetotal = {110-117},
	author = {Y., Zu and T., Huang and Y., Zhu},
	date = {2013-12},
	keywords = {cloud computing, Cloud computing, Computational modeling, computer centres, contracts, datacenter power consumption, datacenter simulator, event-driven schedulers, power aware computing, power consumption, power consumption issue, Power demand, power wall issue, power-aware data center simulator, power-aware resource scheduling strategy, resource allocation, resource provisioning, {ScaLAPACK}, scheduling, Scheduling algorithms, server power model, server power trace, server segmented linear power model, Servers, Time-domain analysis, virtual machine, virtual machines, virtualisation, virtualized datacenters, {VM} live migration, {VM} placements, {VM} service level agreements, {VM} workloads, Xen platform}
}

@book{yang_peak_2016,
	title = {Peak: Pyramid evaluation via automated knowledge extraction},
	volume = {Proceedings of the {AAAI} Conference on Artificial Intelligence 30(1)},
	abstract = {Evaluating the selection of content in a summary is important both for human-written summaries, which can be a useful pedagogical tool for reading and writing skills, and machine-generated summaries, which are increasingly being deployed in information …},
	author = {Yang, Qian and Passonneau, Rebecca and De Melo, Gerard},
	date = {2016}
}

@article{yang_breaking_2017,
	title = {Breaking the softmax bottleneck: A high-rank {RNN} language model},
	url = {https://arxiv.org/abs/1711.03953},
	abstract = {We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn …},
	journaltitle = {{arXiv} preprint {arXiv}:1711.03953},
	author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W},
	date = {2017},
	note = {Type: Journal article}
}

@article{yoon_learning_2020,
	title = {Learning by Semantic Similarity Makes Abstractive Summarization Better},
	url = {http://arxiv.org/abs/2002.07767v1 http://arxiv.org/pdf/2002.07767v1},
	abstract = {One of the obstacles of abstractive summarization is the presence of various potentially correct predictions. Widely used objective functions for supervised learning, such as cross-entropy loss, cannot handle alternative answers effectively. Rather, they act as a training noise. In this paper, we propose Semantic Similarity strategy that can consider semantic meanings of generated summaries while training. Our training objective includes maximizing semantic similarity score which is calculated by an additional layer that estimates semantic similarity between generated summary and reference summary. By leveraging pre-trained language models, our model achieves a new state-of-the-art performance, {ROUGE}-L score of 41.5 on {CNN}/{DM} dataset. To support automatic evaluation, we also conducted human evaluation and received higher scores relative to both baseline and reference summaries.},
	pages = {2002.07767v1},
	journaltitle = {{arXiv}},
	author = {Yoon, Wonjin and Yeo, Yoon Sun and Jeong, Minbyul and Yi, Bong-Jun and Kang, Jaewoo},
	date = {2020},
	note = {Type: Journal article}
}

@book{yu_casict-dcu_2015,
	title = {{CASICT}-{DCU} participation in {WMT}2015 metrics task},
	volume = {Proceedings of the Tenth Workshop on Statistical Machine Translation},
	abstract = {{\textless}span dir=ltr{\textgreater}Human-designed sub-structures are required by most of the syntax-based machine translation evaluation metrics. In this paper, we propose a novel evaluation metric based on dependency parsing model, which does not need this human involvement. Experimental results show that the new single metric gets better correlation than {METEOR} on system level and is comparable with it on sentence level. To introduce more information, we combine the new metric with many other metrics. The combined metric obtains state-of-theart …{\textless}/span{\textgreater}‏},
	pagetotal = {417-421},
	author = {Yu, Hui and Ma, Qingsong and Wu, Xiaofeng and Liu, Qun},
	date = {2015}
}

@article{yu_survey_2020,
	title = {A Survey of Knowledge-Enhanced Text Generation},
	url = {http://arxiv.org/abs/2010.04389v1 http://arxiv.org/pdf/2010.04389v1},
	abstract = {The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing ({NLP}). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
	pages = {2010.04389v1},
	journaltitle = {{arXiv}},
	author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
	date = {2020},
	note = {Type: Journal article},
	annotation = {44 pages; Preprint; A paper and code collection is available at https://github.com/wyu97/{KENLG}-Reading}
}

@article{yu_review_2019,
	title = {A Review of Recurrent Neural Networks: {LSTM} Cells and Network Architectures.},
	volume = {31},
	url = {https://pubmed.ncbi.nlm.nih.gov/31113301},
	doi = {10.1162/neco_a_01199},
	abstract = {Recurrent neural networks ({RNNs}) have been widely adopted in research areas concerned with sequential data, such as text, audio, and video. However, {RNNs} consisting of sigma cells or tanh cells are unable to learn the relevant information of input data when the input gap is large. By introducing gate functions into the cell structure, the long short-term memory ({LSTM}) could handle the problem of long-term dependencies well. Since its introduction, almost all the exciting results based on {RNNs} have been achieved by the {LSTM}. The {LSTM} has become the focus of deep learning. We review the {LSTM} cell and its variants to explore the learning capacity of the {LSTM} cell. Furthermore, the {LSTM} networks are divided into two broad categories: {LSTM}-dominated networks and integrated {LSTM} networks. In addition, their various applications are discussed. Finally, future research directions are presented for {LSTM} networks.},
	pages = {1235--1270},
	number = {7},
	journaltitle = {Neural Comput},
	author = {Yu, Y and Si, X and Hu, C and Zhang, J},
	date = {2019},
	note = {Type: Journal article}
}

@article{zacharias_survey_2018,
	title = {A Survey on Deep Learning Toolkits and Libraries for Intelligent User Interfaces},
	url = {http://arxiv.org/abs/1803.04818v2},
	abstract = {This paper provides an overview of prominent deep learning toolkits and, in particular, reports on recent publications that contributed open source software for implementing tasks that are common in intelligent user interfaces ({IUI}). We provide a scientific reference for researchers and software engineers who plan to utilise deep learning techniques within their {IUI} research and development projects.},
	pages = {1803.04818v2},
	journaltitle = {{arXiv}},
	author = {Zacharias, Jan and Barz, Michael and Sonntag, Daniel},
	date = {2018},
	note = {Type: Journal article}
}

@article{zaheer_big_2020,
	title = {Big Bird: Transformers for Longer Sequences},
	url = {http://arxiv.org/abs/2007.14062v1},
	abstract = {Transformers-based models, such as {BERT}, have been one of the most successful deep learning models for {NLP}. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, {BigBird}, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that {BigBird} is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having \$O(1)\$ global tokens (such as {CLS}), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, {BigBird} drastically improves performance on various {NLP} tasks such as question answering and summarization. We also propose novel applications to genomics data.},
	pages = {2007.14062v1},
	journaltitle = {{arXiv}},
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	date = {2020},
	note = {Type: Journal article}
}

@article{zaremba_learning_2014,
	title = {Learning to Execute},
	volume = {cs.{NE}},
	url = {http://arxiv.org/abs/1410.4615v3},
	abstract = {Recurrent Neural Networks ({RNNs}) with Long Short-Term Memory units ({LSTM}) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of {LSTMs} in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that {LSTMs} can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks’ performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an {LSTM} to add two 9-digit numbers with 99\% accuracy.},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	date = {2014},
	note = {Type: Journal article}
}

@article{zeng_efficient_2016,
	title = {Efficient summarization with read-again and copy mechanism},
	url = {https://arxiv.org/pdf/1611.03382},
	abstract = {Encoder-decoder models have been widely used to solve sequence to sequence prediction tasks. However current approaches suffer from two shortcomings. First, the encoders compute a representation of each word taking into account only the history of the words it has read so far, yielding suboptimal representations. Second, current decoders utilize large vocabularies in order to minimize the problem of unknown words, resulting in slow decoding times. In this paper we address both shortcomings. Towards this goal, we first introduce a …},
	journaltitle = {{arXiv} preprint {arXiv}:1611.03382},
	author = {Zeng, Wenyuan and Luo, Wenjie and Fidler, Sanja and Urtasun, Raquel},
	date = {2016},
	note = {Type: Journal article},
	annotation = {Times cited: 65}
}

@article{zhang_pegasus_2019,
	title = {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
	url = {http://arxiv.org/abs/1912.08777v3},
	abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream {NLP} tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In {PEGASUS}, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best {PEGASUS} model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by {ROUGE} scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
	pages = {1912.08777v3},
	journaltitle = {{arXiv}},
	author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J.},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Added results from mixed+stochastic model, test-set overlapping analysis; Code link added; Accepted for {ICML} 2020. {arXiv} admin note: text overlap with {arXiv}:1605.06560, {arXiv}:1205.2395, {arXiv}:0902.4351, {arXiv}:1610.09932, {arXiv}:nucl-ex/0512029 by other authors}
}

@book{zhang_knowledge_2019,
	title = {Knowledge Adaptive Neural Network for Natural Language Inference},
	publisher = {{IEEE}},
	author = {Zhang, Qi and Yang, Yan and Chen, Chengcai and He, Liang and Yu, Zhou},
	date = {2019}
}

@article{zhang_bertscore_2019,
	title = {{BERTScore}: Evaluating Text Generation with {BERT}},
	url = {http://arxiv.org/abs/1904.09675v3},
	abstract = {We propose {BERTScore}, an automatic evaluation metric for text generation. Analogously to common metrics, {BERTScore} computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. {BERTScore} correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that {BERTScore} is more robust to challenging examples when compared to existing metrics.},
	pages = {1904.09675v3},
	journaltitle = {{arXiv}},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Code available at https://github.com/Tiiiger/bert\_score; To appear in {ICLR}2020}
}

@article{zhang_sentence_2017,
	title = {Sentence Simplification with Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1703.10931v2},
	abstract = {Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call {\textbackslash}sc Dress (as shorthand for {\textbackslash}bf Deep {\textbackslash}bf {REinforcement} {\textbackslash}bf Sentence {\textbackslash}bf Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.},
	pages = {1703.10931v2},
	journaltitle = {{arXiv}},
	author = {Zhang, Xingxing and Lapata, Mirella},
	date = {2017},
	note = {Type: Journal article},
	annotation = {to appear in {EMNLP} 2017}
}

@article{zhang_neural_2018,
	title = {Neural latent extractive document summarization},
	url = {https://arxiv.org/pdf/1808.07187},
	abstract = {Extractive summarization models require sentence-level labels, which are usually created heuristically (eg, with rule-based methods) given that most summarization datasets only have document-summary pairs. Since these labels might be suboptimal, we propose a latent variable extractive model where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training the loss comes{\textbackslash}emph directly from gold summaries. Experiments on the {CNN}/Dailymail dataset …},
	journaltitle = {{arXiv} preprint {arXiv}:1808.07187},
	author = {Zhang, Xingxing and Lapata, Mirella and Wei, Furu and Zhou, Ming},
	date = {2018},
	note = {Type: Journal article},
	annotation = {Times cited: 75}
}

@article{zhang_hibert_2019,
	title = {{HIBERT}: Document level pre-training of hierarchical bidirectional transformers for document summarization},
	url = {https://arxiv.org/pdf/1905.06566},
	abstract = {Neural extractive summarization models usually employ a hierarchical encoder for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods. Training the hierarchical encoder with these{\textbackslash}emph inaccurate labels is challenging. Inspired by the recent work on pre-training transformer sentence encoders{\textbackslash}cite devlin: 2018: arxiv, we propose {\textbackslash}sc Hibert(as shorthand for {\textbackslash}bf {HI} erachical {\textbackslash}bf B idirectional {\textbackslash}bf E ncoder {\textbackslash}bf R epresentations from {\textbackslash}bf T ransformers) for …},
	journaltitle = {{arXiv} preprint {arXiv}:1905.06566},
	author = {Zhang, Xingxing and Wei, Furu and Zhou, Ming},
	date = {2019},
	note = {Type: Journal article},
	annotation = {Times cited: 101}
}

@article{zhang_lightweight_2020,
	title = {Lightweight, Dynamic Graph Convolutional Networks for {AMR}-to-Text Generation},
	url = {http://arxiv.org/abs/2010.04383v1 http://arxiv.org/pdf/2010.04383v1},
	abstract = {{AMR}-to-text generation is used to transduce Abstract Meaning Representation structures ({AMR}) into text. A key challenge in this task is to efficiently learn effective graph representations. Previously, Graph Convolution Networks ({GCNs}) were used to encode input {AMRs}, however, vanilla {GCNs} are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme. To account for these issues, larger and deeper {GCN} models are required to capture more complex interactions. In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks ({LDGCNs}) that capture richer non-local interactions by synthesizing higher order information from the input graphs. We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity. With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity. Experiments demonstrate that {LDGCNs} outperform state-of-the-art models on two benchmark datasets for {AMR}-to-text generation with significantly fewer parameters.},
	pages = {2010.04383v1},
	journaltitle = {{arXiv}},
	author = {Zhang, Yan and Guo, Zhijiang and Teng, Zhiyang and Lu, Wei and Cohen, Shay B. and Liu, Zuozhu and Bing, Lidong},
	date = {2020},
	note = {Type: Journal article},
	annotation = {Accepted to {EMNLP} 2020, long paper}
}

@book{zhao_moverscore_2019,
	location = {Stroudsburg, {PA}, {USA}},
	title = {{MoverScore}: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Wei and Peyrard, Maxime and Liu, Fei and Gao, Yang and Meyer, Christian M. and Eger, Steffen},
	date = {2019}
}

@book{zhou_paraeval_2006,
	title = {Paraeval: Using paraphrases to evaluate summaries automatically},
	volume = {Proceedings of the human language technology conference of the {NAACL}, main conference},
	abstract = {{ParaEval} is an automated evaluation method for comparing reference and peer summaries. It facilitates a tieredcomparison strategy where recall-oriented global optimal and local greedy searches for paraphrase matching are enabled in the top tiers. We utilize a domainindependent paraphrase table extracted from a large bilingual parallel corpus using methods from Machine Translation ({MT}). We show that the quality of {ParaE}-val’s evaluations, measured by correlating with human judgments, closely resembles that of …},
	pagetotal = {447-454},
	author = {Zhou, Liang and Lin, Chin-Yew and Munteanu, Dragos Stefan and Hovy, Eduard},
	date = {2006}
}

@article{zhou_neural_2018,
	title = {Neural Document Summarization by Jointly Learning to Score and Select Sentences},
	url = {http://arxiv.org/abs/1807.02305v1},
	abstract = {Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the {CNN}/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.},
	pages = {1807.02305v1},
	journaltitle = {{arXiv}},
	author = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Huang, Shaohan and Zhou, Ming and Zhao, Tiejun},
	date = {2018},
	note = {Type: Journal article},
	annotation = {In {ACL} 2018}
}

@article{zhou_selective_2017,
	title = {Selective Encoding for Abstractive Sentence Summarization},
	url = {http://arxiv.org/abs/1704.07073v1},
	abstract = {We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, {DUC} 2004 and {MSR} abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.},
	pages = {1704.07073v1},
	journaltitle = {{arXiv}},
	author = {Zhou, Qingyu and Yang, Nan and Wei, Furu and Zhou, Ming},
	date = {2017},
	note = {Type: Journal article},
	annotation = {10 pages; To appear in {ACL} 2017}
}

@article{zhu_boosting_2020,
	title = {Boosting Factual Correctness of Abstractive Summarization with Knowledge Graph},
	url = {http://arxiv.org/abs/2003.08612v5},
	abstract = {A commonly observed problem with abstractive summarization is the distortion or fabrication of factual information in the article. This inconsistency between summary and original text has led to various concerns over its applicability. In this paper, we propose a Fact-Aware Summarization model, {FASum}, which extracts factual relations from the article to build a knowledge graph and integrates it into the neural decoding process. Then, we propose a Factual Corrector model, {FC}, that can modify abstractive summaries generated by any summarization model to improve factual correctness. Empirical results show that {FASum} can generate summaries with higher factual correctness compared with state-of-the-art abstractive summarization systems. And {FC} improves the factual correctness of summaries generated by various models via only modifying several entity tokens.},
	pages = {2003.08612v5},
	journaltitle = {{arXiv}},
	author = {Zhu, Chenguang and Hinthorn, William and Xu, Ruochen and Zeng, Qingkai and Zeng, Michael and Huang, Xuedong and Jiang, Meng},
	date = {2020},
	note = {Type: Journal article},
	annotation = {15 pages, 3 figures}
}

@article{ziegler_fine-tuning_2019,
	title = {Fine-Tuning Language Models from Human Preferences},
	url = {http://arxiv.org/abs/1909.08593v2 http://arxiv.org/pdf/1909.08593v2},
	abstract = {Reward learning enables the application of reinforcement learning ({RL}) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making {RL} practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the {TL};{DR} and {CNN}/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable {ROUGE} scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
	pages = {1909.08593v2},
	journaltitle = {{arXiv}},
	author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
	date = {2019},
	note = {Type: Journal article}
}